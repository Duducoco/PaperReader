[
  {
    "date": "2026-01-22",
    "title": "Magnon equilibrium spin current in collinear antiferromagnets",
    "authors": "Vladimir A. Zyuzin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16184v1",
    "source": "arXiv",
    "abstract": "We theoretically predict that Dzyaloshinskii-Moriya interaction can induce magnon equilibrium spin current in collinear antiferromagnets. Such a current, being a response to the effective magnon vector potential, can be considered as magnon analog of the superconducting supercurrent or the persistent current. Large amplitude of the predicted effect may compensate for the smallness of the Dzyaloshinskii-Moriya interaction, making the equilibrium spin currents to be experimentally observed. We suggest that external electric field can play the role of effective flux magnons interact with and propose an experiment based on the interference of magnons in the ring geometry as a verification of the concept."
  },
  {
    "date": "2026-01-22",
    "title": "Practical applications of Set Shaping Theory to Non-Uniform Sequences",
    "authors": "A. Schmidt, A. Vdberg, A. Petit",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15853v1",
    "source": "arXiv",
    "abstract": "Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work"
  },
  {
    "date": "2026-01-22",
    "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "authors": "Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang, Haitao Mi, Dong Yu, Michael R. Lyu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15808v1",
    "source": "arXiv",
    "abstract": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities."
  },
  {
    "date": "2026-01-22",
    "title": "U3-xi: Pushing the Boundaries of Speaker Recognition via Incorporating Uncertainty",
    "authors": "Junjie Li, Kong Aik Lee",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15719v1",
    "source": "arXiv",
    "abstract": "An utterance-level speaker embedding is typically obtained by aggregating a sequence of frame-level representations. However, in real-world scenarios, individual frames encode not only speaker-relevant information but also various nuisance factors. As a result, different frames contribute unequally to the final utterance-level speaker representation for Automatic Speaker Verification systems. To address this issue, we propose to estimate the inherent uncertainty of each frame and assign adaptive weights accordingly, where frames with higher uncertainty receive lower attention. Based on this idea, we present U3-xi, a comprehensive framework designed to produce more reliable and interpretable uncertainty estimates for speaker embeddings. Specifically, we introduce several strategies for uncertainty supervision. First, we propose speaker-level uncertainty supervision via a Stochastic Variance Loss, where the distance between an utterance embedding and its corresponding speaker centroid serves as a pseudo ground truth for uncertainty learning. Second, we incorporate global-level uncertainty supervision by injecting the predicted uncertainty into the sof tmax scale during training. This adaptive scaling mechanism adjusts the sharpness of the decision boundary according to sample difficulty, providing global guidance. Third, we redesign the uncertainty estimation module by integrating a Transformer encoder with multi-view self-attention, enabling the model to capture rich local and long-range temporal dependencies. Comprehensive experiments demonstrate that U3-xi is model-agnostic and can be seamlessly applied to various speaker encoders. In particular, when applied to ECAPA-TDNN, it achieves 21.1% and 15.57% relative improvements on the VoxCeleb1 test sets in terms of EER and minDCF, respectively."
  },
  {
    "date": "2026-01-22",
    "title": "zkFinGPT: Zero-Knowledge Proofs for Financial Generative Pre-trained Transformers",
    "authors": "Xiao-Yang Liu, Ningjie Li, Keyi Wang, Xiaoli Zhi, Weiqin Tong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15716v1",
    "source": "arXiv",
    "abstract": "Financial Generative Pre-trained Transformers (FinGPT) with multimodal capabilities are now being increasingly adopted in various financial applications. However, due to the intellectual property of model weights and the copyright of training corpus and benchmarking questions, verifying the legitimacy of GPT's model weights and the credibility of model outputs is a pressing challenge. In this paper, we introduce a novel zkFinGPT scheme that applies zero-knowledge proofs (ZKPs) to high-value financial use cases, enabling verification while protecting data privacy. We describe how zkFinGPT will be applied to three financial use cases. Our experiments on two existing packages reveal that zkFinGPT introduces substantial computational overhead that hinders its real-world adoption. E.g., for LLama3-8B model, it generates a commitment file of $7.97$MB using $531$ seconds, and takes $620$ seconds to prove and $2.36$ seconds to verify."
  },
  {
    "date": "2026-01-22",
    "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation",
    "authors": "Khusrav Badalov, Young Yoon",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15687v1",
    "source": "arXiv",
    "abstract": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations."
  },
  {
    "date": "2026-01-22",
    "title": "Tensor-based phase difference estimation on time series analysis",
    "authors": "Shu Kanno, Kenji Sugisaki, Rei Sakuma, Jumpei Kato, Hajime Nakamura, Naoki Yamamoto",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15616v1",
    "source": "arXiv",
    "abstract": "We propose a phase-difference estimation algorithm based on the tensor-network circuit compression, leveraging time-evolution data to pursue scalability and higher accuracy on a quantum phase estimation (QPE)-type algorithm. Using tensor networks, we construct circuits composed solely of nearest-neighbor gates and extract time-evolution data by four-type circuit measurements. In addition, to enhance the accuracy of time-evolution and state-preparation circuits, we propose techniques based on algorithmic error mitigation and on iterative circuit optimization combined with merging into matrix product states, respectively. Verifications using a noiseless simulator for the 8-qubit one-dimensional Hubbard model using an ancilla qubit show that the proposed algorithm achieves accuracies with 0.4--4.7\\% error from a true energy gap on an appropriate time-step size, and that accuracy improvements due to the algorithmic error mitigation are observed. We also confirm the enhancement of the overlap with matrix product states through iterative optimization. Finally, the proposed algorithm is demonstrated on IBM Heron devices with Q-CTRL error suppression for 8-, 36-, and 52-qubit models using more than 5,000 2-qubit gates. These largest-scale demonstrations for the QPE-type algorithm represent significant progress not only toward practical applications of near-term quantum computing but also toward preparation for the era of error-corrected quantum devices."
  },
  {
    "date": "2026-01-22",
    "title": "Swelling-Induced Stress-Assisted Transfer of Nanodiamond Arrays with a PVA Carrier Tape for Conformal Bio-Integrated Sensing and Labelling",
    "authors": "Luyao Zhang, Lingzhi Wang, Xinhao Hu, Yip Tai Nam, Mingzhe Sun, Jixiang Jing, Lizhi Xu, Yuan Lin, Yong Hou, Zhiqin Chu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15587v1",
    "source": "arXiv",
    "abstract": "The conformal integration of nitrogen-vacancy (NV) center nanodiamond arrays onto soft, hydrated, curvilinear biological interfaces remain a fundamental challenge for in vivo quantum sensing and imaging. Conventional transfer techniques often fail due to reliance on high temperature, corrosive chemicals, or mechanical peeling, leading to pattern damage, low fidelity, or poor biocompatibility. Here, we report a transfer strategy utilizing polyvinyl alcohol (PVA) carrier soluble tape, enabling rapid, residue-free, high-fidelity transfer of nanodiamond patterns onto diverse biointerfaces. The success of this method is rooted in a unique \"hydrate-soften-expand-self-peel\" mechanism of the soluble tape with PVA backing. In situ mechanical tracking reveals non-uniform PVA swelling upon hydration generates transient local normal and shear stresses at the interface. These stresses delaminate the tape within 3 minutes at room temperature while promoting adhesion of the nanodiamond array to the substrate. In contrast, conventional water-soluble tapes with composite structures undergo passive dissolution and collapse, causing residue contamination and reduced efficiency. Leveraging this mechanism, we achieve conformal patterning on ultra-soft hydrogels (~0.6 kPa) and highly curved bio-surfaces (hair, 100 μm^-1). Additionally, we demonstrate a dual-identity verification system integrating data storage and physical unclonable functions on a hydrogel contact lens. This work provides a versatile tool for bio-interface engineering and a general framework for gentle, efficient transfer of functional nanomaterials."
  },
  {
    "date": "2026-01-21",
    "title": "MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification",
    "authors": "Jingwei Song, Xinyu Wang, Hanbin Wang, Xiaoxuan Lei, Bill Shi, Shixin Han, Eric Yang, Xiao-Wen Chang, Lynn Ai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15498v1",
    "source": "arXiv",
    "abstract": "Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks."
  },
  {
    "date": "2026-01-21",
    "title": "Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases",
    "authors": "Alex Dantart",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15476v1",
    "source": "arXiv",
    "abstract": "This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models (\"creative oracle\"), (2) basic retrieval-augmented systems (\"expert archivist\"), and (3) an advanced, end-to-end optimized RAG system (\"rigorous archivist\"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains."
  },
  {
    "date": "2026-01-21",
    "title": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs",
    "authors": "Christoph Bartmann, Johannes Schimunek, Mykyta Ielanskyi, Philipp Seidl, Günter Klambauer, Sohvi Luukkonen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15279v1",
    "source": "arXiv",
    "abstract": "A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure."
  },
  {
    "date": "2026-01-21",
    "title": "Automating Idealness Proofs for Binary Programs with Application to Rectangle Packing",
    "authors": "Jamie Fravel, Robert Hildebrand",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15252v1",
    "source": "arXiv",
    "abstract": "An integer program is called ideal if its continuous relaxation coincides with its convex hull allowing the problem to be solved as a continuous program and offering substantial computational advantages. Proving idealness analytically can be extraordinarily tedious -- even for small formulations -- such proofs often span many pages of intricate case analysis which motivates the development of automated verification methods. We develop a general-purpose framework for certifying idealness in Mixed Binary Linear Programs (MBLPs), formulating the verification problem as a linear program when the data is fixed and as a nonconvex quadratic program when the data is parametric. We apply this framework to study several formulations of the rectangle packing problem that are conjectured to be pairwise-ideal, obtaining computational proofs where analytic proofs were previously unknown or impractical. As our second contribution, we introduce and model a novel generalization of the rectangle packing problem that enforces edge clearances between selected rectangles. We present both existing and novel MBLP formulations which arise from different encodings of the underlying disjunctive constraints. We perform some computational experiments on these formulations under a strip-packing objective to determine the importance of pairwise-idealness in practice."
  },
  {
    "date": "2026-01-21",
    "title": "How to Verify a Turing Machine with Dafny",
    "authors": "Edgar F. A. Lederer",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15230v1",
    "source": "arXiv",
    "abstract": "This paper describes the formal verification of two Turing machines using the program verifier Dafny. Both machines are deciders, so we prove total correctness. They are typical first examples of Turing machines used in any course of Theoretical Computer Science; in fact, the second machine is literally taken from a relevant textbook. Usually, the correctness of such machines is made plausible by some informal explanations of their basic ideas, augmented with a few sample executions, but neither by rigorous mathematical nor mechanized formal proof. No wonder: The invariants (and variants) required for such proofs are big artifacts, peppered with overpowering technical details. Finding and checking these artifacts without mechanical support is practically impossible, and such support is only available since recent times. But nowadays, just because of these technicalities, with such subjects under proof a program verifier can really show off and demonstrate its capabilities."
  },
  {
    "date": "2026-01-21",
    "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks",
    "authors": "Yaru Liu, Ao-bo Wang, Nanyang Ye",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15164v1",
    "source": "arXiv",
    "abstract": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines."
  },
  {
    "date": "2026-01-21",
    "title": "The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks",
    "authors": "Ivan Carrera, Daniel Maldonado-Ruiz",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15130v1",
    "source": "arXiv",
    "abstract": "The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the \"Plausibility Trap\": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the \"efficiency tax\"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it."
  },
  {
    "date": "2026-01-21",
    "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
    "authors": "Oleg Romanchuk, Roman Bondar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15059v1",
    "source": "arXiv",
    "abstract": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis. We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity. We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime. We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum. We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments."
  },
  {
    "date": "2026-01-22",
    "title": "LogicScore: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering",
    "authors": "Zhichao Yan, Yunxiao Zhao, Jiapu Wang, Jiaoyan Chen, Shaoru Guo, Xiaoli Li, Ru Li, Jeff Z. Pan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15050v2",
    "source": "arXiv",
    "abstract": "Current evaluation methods for Attributed Question Answering (AQA) suffer from \\textit{attribution myopia}: they emphasize verification of isolated statements and their attributions but overlook the global logical integrity of long-form answers. Consequently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present \\textsc{LogicScore}, a unified evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: \\textit{Completeness} (logically sound deduction), \\textit{Conciseness} (non-redundancy), and \\textit{Determinateness} (consistent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical capability gap: leading models often achieve high attribution scores (e.g., 92.85\\% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11\\% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical evaluation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM development. Codes are available at: https://github.com/zhichaoyan11/LogicScore."
  },
  {
    "date": "2026-01-21",
    "title": "Interoperable Architecture for Digital Identity Delegation for AI Agents with Blockchain Integration",
    "authors": "David Ricardo Saavedra",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.14982v1",
    "source": "arXiv",
    "abstract": "Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures."
  },
  {
    "date": "2026-01-21",
    "title": "$H$ dibaryon and its cousins from SU(6)-constrained baryon-baryon interaction",
    "authors": "Tao-Ran Hu, Feng-Kun Guo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.14922v1",
    "source": "arXiv",
    "abstract": "We constrain the $S$-wave baryon-baryon interaction using SU(6) symmetry within a nonrelativistic effective field theory. The most general leading-order Lagrangian contains two independent parameters, which we determine using physical $NN$ and lattice QCD $ΩΩ$ scattering lengths. This framework allows for parameter-free predictions in the strangeness $S=-2$ sector relevant to the $H$ dibaryon. Solving the coupled-channel scattering problem, we identify two bound states below the $ΛΛ$ threshold, one deeply bound and one shallow, along with resonances near the $ΣΣ$ and $Σ^*Σ^*$ thresholds. We demonstrate that these poles result in distinct enhancements in $ΛΛ$ invariant mass distributions, suggesting that the $H$ dibaryon exists as a multichannel bound state and providing clear signatures for experimental verification."
  },
  {
    "date": "2026-01-21",
    "title": "A Category-Theoretic Framework for Dependent Effect Systems",
    "authors": "Satoshi Kura, Marco Gaboardi, Taro Sekiyama, Hiroshi Unno",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.14846v1",
    "source": "arXiv",
    "abstract": "Graded monads refine traditional monads using effect annotations in order to describe quantitatively the computational effects that a program can generate. They have been successfully applied to a variety of formal systems for reasoning about effectful computations. However, existing categorical frameworks for graded monads do not support effects that may depend on program values, which we call dependent effects, thereby limiting their expressiveness. We address this limitation by introducing indexed graded monads, a categorical generalization of graded monads inspired by the fibrational \"indexed\" view and by classical categorical semantics of dependent type theories. We show how indexed graded monads provide semantics for a refinement type system with dependent effects. We also show how this type system can be instantiated with specific choices of parameters to obtain several formal systems for reasoning about specific program properties. These instances include, in particular, cost analysis, probability-bound reasoning, expectation-bound reasoning, and temporal safety verification."
  },
  {
    "date": "2026-1-23",
    "title": "LLM Evaluations for Emotional Expressiveness and Factual Consistency in Medical Dialogue Systems Using LLM-as-a-Judge",
    "authors": "Rahul Mahadik, Akhilesh Mandal, Sanya Tripathi, Shruti Jaiswal",
    "publish": "2025 International Conference on Responsible, Generative and Explainable AI (ResGenXAI)",
    "url": "https://doi.org/10.1109/resgenxai64788.2025.11344029",
    "source": "IEEE",
    "abstract": "Evaluation of Large Language Models (LLMs) has primarily focused on factual correctness, with limited consideration for factors such as emotional tone, clarity, and general response quality. This paper investigates the performance of smaller, open-source transformer models (2B–4B parameters) on a domain-specific task using a structured evaluation framework. A curated Medical QnA dataset is used to assess model responses across multiple dimensions, including factual alignment (via G-Eval), sentiment polarity, emotional tone, and readability. The study employs existing tools such as DeepEval to support metric computation and model comparison. Four candidate models—variants of Qwen and Gemma—are evaluated using standardized scoring methods, enabling consistent cross-model analysis. The evaluation reveals differences in model outputs across multiple dimensions, including factual consistency, tonal characteristics, and linguistic clarity, when applied to a domain-specific question answering task. These results underscore the relevance of multi-faceted evaluation approaches for specialized use cases. The observations may inform future research on language model assessment in domain-adapted applications."
  },
  {
    "date": "2026-1-23",
    "title": "LLM-Driven Automated Penetration Testing: Architectures, Benchmarks, and Safety Considerations",
    "authors": "Akshitha Segireddy",
    "publish": "2025 10th International Conference on Smart Structures and Systems (ICSSS)",
    "url": "https://doi.org/10.1109/icsss66939.2025.11346427",
    "source": "IEEE",
    "abstract": "In recent years, there has been a growing trend toward using large language models (LLMs) to automate penetration testing tasks, including reconnaissance, vulnerability discovery, exploit identification, and reporting. Previous works, including PentestGPT, Curriculum PT, RapidPen, White Rabbit Neo- PentestGPT, and HackSynth, have shown that LLM guided workflows can be superior to naive prompting and traditional scripting-based approaches on both capture the flag (CTF) challenges and realistic lab environments, and have attracted significant community interest and public benchmarking. Nevertheless, the complete automation of penetration testing remains an unsolved problem: current tools continue to fail to address issues related to managing context, orchestrating tools, handling environmental variations, and enforcing safety constraints. In this paper, we introduce a unified LLM-driven automated Penetration Testing Framework that divides the workflow of penetration testing into formally defined tasks, states, and actions, utilizing LLM-based agents to develop, execute, and modify multi-step attack chains across multiple layers of networks, web, cloud, and application environments. We detail how to integrate external tools (scanners, exploit frameworks, and knowledge bases) into the framework, design a memory and reasoning layer to prevent the loss of context, and define safety and governance controls to limit the misuse of the system. We conduct experiments using synthetic labs, standardized CTFstyle benchmarks, and real-world-inspired scenarios, and compare our results against those obtained from human testers, scripted pipelines, and previous LLM-based systems. Our results indicate significant improvements in task completion, coverage, and time-to-compromise, while also identifying failure modes and informing future research directions. The purpose of this paper is to provide a reference architecture and experimental guide for next-generation, LLM-driven penetration testing."
  },
  {
    "date": "2026-1-23",
    "title": "Evaluating Toxicity Understanding of LLM Agents",
    "authors": "Ramaravind Kommiya Mothilal, Syed Ishtiaque Ahmed, Shion Guha",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00121",
    "source": "IEEE",
    "abstract": "Research on toxicity in LLMs has largely focused on detection tasks, such as identifying hate speech or stereotyping in texts. Recently, these tasks have increasingly been embedded in agentic workflows, where LLMs autonomously query external APIs and reason over results before responding. This shift promotes the perception that LLMs exhibit an “understanding” of toxicity, yet how such understanding can be meaningfully interpreted by humans remains unclear. In this position paper, we first unpack this oversight by highlighting the fundamental gaps in current literature and then propose a framework for evaluating toxicity understanding of agentic LLMs. Overall, this short paper aims to shift the discourse from improving toxicity detection in LLMs to evaluating how LLMs understand toxicity in order to enhance their trustworthiness in downstream tasks"
  },
  {
    "date": "2026-1-23",
    "title": "LLM-Powered Code Quality Bot: Automating Refactoring in CI/CD",
    "authors": "Akram Adem, Glaucia Melo",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00101",
    "source": "IEEE",
    "abstract": "Code quality issues, such as code smells and minor bugs, consume valuable developer time and can slow down software delivery pipelines. This poster presents SonaRefactor, an AI-assisted CI/CD pipeline that merges SonarCloud static analysis with Gemini LLM to automatically summarize flagged issues and propose safe, minimal code fixes. When a pull request is opened, the bot automatically detects issues, explains them in context, and generates a minimal safe autofix that is committed back as a new pull request. This work demonstrates how large language models can act as hands-on teammates in software development pipelines, offloading repetitive refactoring tasks so developers can focus on more complex and creative aspects of software engineering. Our tool minimizes developer interruption by enforcing conservative, auditable changes that reduce friction in the review process, without compromising safety or autonomy."
  },
  {
    "date": "2026-1-23",
    "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation",
    "authors": "Huixiang Zhang, Mahzabeen Emu",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00108",
    "source": "IEEE",
    "abstract": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation."
  },
  {
    "date": "2026-1-23",
    "title": "ChatAcadien: A RAG-LLM-Based Chatbot for Exploring Acadian Genealogy",
    "authors": "Rayen Ghali, Sid Ahmed Selouani",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00026",
    "source": "IEEE",
    "abstract": "This paper introduces ChatAcadien, a RetrievalAugmented Generation (RAG) chatbot making Acadian genealogical archives more accessible. ChatAcadien connects users to the Centre d'Études Acadiennes Anselme-Chiasson (CEAAC) knowledge base through a Large Language Model (LLM), enabling natural language queries while mitigating hallucinations through document-grounded generation. To effectively process genealogical data, ChatAcadien's architecture manages context length through tailored segmentation techniques that preserve family relationships. Advanced segmentation and retrieval mechanisms, incorporating hypothetical document embeddings (HyDE), and rerankers to enhance search precision. Evaluations using key metrics, including Faithfulness, Context Precision and Recall, demonstrate improved accuracy and contextual relevance. ChatAcadien represents a practical solution for democratizing access to Acadian heritage while maintaining factual accuracy in genealogical research assistance."
  },
  {
    "date": "2026-1-23",
    "title": "Task-Aware Reduction for Scalable LLM-Database Systems",
    "authors": "Marcus Emmanuel Barnes, Taher A. Ghaleb, Safwat Hassan",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00114",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) are increasingly applied to data-intensive workflows, from database querying to developer observability. Yet the effectiveness of these systems is constrained by the volume, verbosity, and noise of real-world text-rich data such as logs, telemetry, and monitoring streams. Feeding such data directly into LLMs is costly, environmentally unsustainable, and often misaligned with task objectives. Parallel efforts in LLM efficiency have focused on model- or architecturelevel optimizations, but the challenge of reducing upstream input verbosity remains underexplored. In this paper, we argue for treating the token budget of an LLM as an attention budget and elevating task-aware text reduction as a first-class design principle for language-data systems. We position input-side reduction not as compression, but as attention allocation: prioritizing information most relevant to downstream tasks. We outline open research challenges for building benchmarks, designing adaptive reduction pipelines, and integrating token-budget-aware preprocessing into database and retrieval systems. Our vision is to channel scarce attention resources toward meaningful signals in noisy, data-intensive workflows, enabling scalable, accurate, and sustainable LLM-data integration."
  },
  {
    "date": "2026-1-23",
    "title": "Integrating Cross-Domain Data Analytics through LLM-Derived Embedding and Transfer Learning Techniques",
    "authors": "Sukanth Korkanti",
    "publish": "2025 IEEE 2nd International Conference on Information Technology, Electronics and Intelligent Communication Systems (ICITEICS)",
    "url": "https://doi.org/10.1109/iciteics64870.2025.11341555",
    "source": "IEEE",
    "abstract": "Nowadays, data are being generated from so many diversified domains, characterized by their own traits and intricacies. Data integration from such diversified sources poses significant challenges in heterogeneity, scalability, and meaningful feature representation. The paper introduces a new paradigm for cross-domain analytics using the power of LLM-derived embeddings combined with transfer learning techniques. This will also enable us to show how LLMs can effectively capture complex patterns and semantic relationships within and across domains. We take real-world datasets across diverse sectors such as healthcare, finance, and social media, and show that it is possible to leverage state-of-the-art LLMs for creating high-dimensional embeddings as a unified representation framework across diverse data sources. These are then followed by transfer learning to fine-tune such embeddings on a specific analytical task, hence improving the generalization of the model and its accuracy across diverse applications. We show via an extensive set of experiments that our integrative approach substantially improves both the predictive performance and interpretability of traditional cross-domain analytics methods. Moreover, since transfer learning is applied here, much fewer labeled data are needed, making the approach quite time-effective and scalable. The findings point to the possibility of extracting the embeddings from LLMs, which further uses transfer learning to bridge the gap between disparate data domains and allows for much more robust and deep analytics. This advancement not only strengthens the capability for data-driven decision-making but also opens up new avenues for interdisciplinary research and innovation. This will enable organizations to unearth deeper insights, to drive more informed strategies in an increasingly data-rich environment, by embedding cross-domain data analytics using sophisticated techniques of embedding and transfer learning."
  },
  {
    "date": "2026-1-23",
    "title": "Securing LLM-Generated Embedded Firmware Through AI Agent-Driven Validation and Patching",
    "authors": "Seyed Moein Abtahi, Akramul Azim",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00044",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) show promise in generating firmware for embedded systems but often introduce security flaws and fail to meet real-time performance constraints. This paper proposes a three-phase methodology that combines LLM-based firmware generation with automated security validation and iterative refinement in a virtualized environment. Using structured prompts, models like GPT-4 generate firmware for networking and control tasks, deployed on FreeRTOS via QEMU. These implementations are tested using fuzzing, static analysis, and runtime monitoring to detect vulnerabilities such as buffer overflows (CWE-120), race conditions (CWE-362), and denial-of-service threats (CWE-400). Specialized AI agents for Threat Detection, Performance Optimization, and Compliance Verification collaborate to improve detection and remediation. Identified issues are categorized using CWE, then used to prompt targeted LLM-generated patches in an iterative loop. Experiments show a 92.4% Vulnerability Remediation Rate (37.3% improvement), <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\text{95.8 \\%}$</tex> Threat Model Compliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6 ms worst-case execution time and <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$195 \\mu$</tex> jitter. This process enhances firmware security and performance while contributing an open-source dataset for future research."
  },
  {
    "date": "2026-1-23",
    "title": "Towards Scenario-Driven Reference Architecture for Integrating Microservices and LLM-Based Multi-Agent Systems",
    "authors": "Peyman Yazdanian, Yan Liu",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00107",
    "source": "IEEE",
    "abstract": "Microservice Systems (MS) and Large Language Model-based Multi-Agent Systems (LLM-MAS) are two major paradigms shaping modern software design. While MS emphasizes modularity and scalability, LLM-MAS introduces adaptive reasoning and autonomy. However, deriving systematic insights into their similarities, differences, and integration potential remains challenging due to their complex heterogeneous technology stacks. This poster proposes a scenario-driven methodology as the core analytical approach. By grounding the comparison of MS and LLM-MAS in concrete, domain-specific scenarios within the retail supply chain management, we expose architectural contrasts and connections across both paradigms. Using this case study, we demonstrate how scenario-driven analysis captures functional workflows, architectural constraints, and quality attributes, enabling the derivation of a layered reference architecture. This provides a structured base for hybrid integration and the development of scenario-based evaluation benchmarks, including failure injection aligned with the SOTA taxonomy."
  },
  {
    "date": "2026-1-23",
    "title": "PubMed API and LLM-Driven Hybrid Retrieval System for Biomedical Question Answering",
    "authors": "Anderson Morillo, Carlos Agamez, Edwin Puertas, Juan Carlos Martinez-Santos, Jairo Serrano",
    "publish": "2025 IEEE Colombian Caribbean Conference (C3)",
    "url": "https://doi.org/10.1109/c366505.2025.11340582",
    "source": "IEEE",
    "abstract": "This paper presents a biomedical questionanswering system developed. While existing hybrid systems often prioritize high computational power for document retrieval, our approach focuses on resource-efficient methods, combining key term extraction via large language models (lLMs) and statistical Weirdness scoring with PubMed API-based retrieval. For answer generation, we employ prompt engineering techniques, including few-shot learning, instructional prompts, and Retrieval Augmented Generation (RAG), leveraging the Qwen3-30B-A3B model. Evaluation on the BioASQ benchmark demonstrates competitive performance in Factoid and Summary question types. Our results underscore the viability of lightweight architectures for biomedical question answering (QA), balancing accuracy and computational feasibility for real-world deployment."
  },
  {
    "date": "2026-1-23",
    "title": "ZeroByX: A Cyber Security Specialized Large Language Model (LLM)",
    "authors": "B. Kalaiselvi, S. Anbarasan, R. Athesh Pargau, G. Raja",
    "publish": "2025 IEEE 2nd International Conference on Information Technology, Electronics and Intelligent Communication Systems (ICITEICS)",
    "url": "https://doi.org/10.1109/iciteics64870.2025.11341519",
    "source": "IEEE",
    "abstract": "ZeroByX is an enterprise-grade Large Language Model (LLM) dedicated specifically towards cybersecurity, aimed at streamlining the efficiency and efficacy of security professionals across the attack and defensive cycle. In contradistinction with typical language models, ZeroByX is constructed on top of an optimized Mistral base model trained on a broad spectrum of domains in the field of cybersecurity, ranging from Advanced Persistent Threat (APT) monitoring, network security, cryptography, incident response, malware analysis, penetration testing, cloud misconfigurations, as well as risk governance. In addition, a Qwen-based model is incorporated, performing exceptionally well in threat hunting, reverse engineering, code analysis, as well as exploit development. Innovation occurs at response assembling, where responses from both models are combined to offer deep, contextual, and accurate responses for uses in the realm of cybersecurity. ZeroByX features a modular slash-command interface, allowing users to trigger specific commands such as /cve for the latest vulnerabilities, /ir for incident response playbooks, /malware for analysis tasks, or /cloudattack to simulate threats in AWS, Azure, or GCP environments."
  },
  {
    "date": "2026-1-23",
    "title": "OpenEYEs: Leveraging LLM and Graph Neural Networks for Deep Ransomware Detection",
    "authors": "Yufan Zhang, Yating Gao, Wei Hu, Gaolei Li, Jianhua Li",
    "publish": "2025 IEEE Conference on Cloud and Big Data Computing (CBDCom)",
    "url": "https://doi.org/10.1109/cbdcom68404.2025.00032",
    "source": "IEEE",
    "abstract": "Ransomware has become an escalating cybersecurity threat, causing significant financial damage. However, existing detection methods often rely on limited information and lack semantic understanding of program behaviour, limiting their ability to represent essential ransomware characteristics and generalize to unseen variants. To address these challenges, we propose OpenEYEs, a novel ransomware detection frame-work that leverages Graph Neural Networks (GNNs) and Large Language Models (LLMs) to gain deeper and wider insight into ransomware behaviours by representing them in a knowledge-enhanced manner. OpenEYEs constructs application programming interface (API) call graphs and extracts frequent API call subsequences to capture both global and local behavioural patterns. GNNs perform representation learning over the API call graph to model ransomware attack behaviours, while LLMs enhance the semantic encoding of graph nodes, edges, and API call subsequences by interpreting API descriptions and textual parameters. Experiments on real-world ransomware show that OpenEYEs achieves 99.27% detection accuracy on 691 samples and 99.53% family classification accuracy on 895 samples. Ablation studies confirm that LLMs boost performance through semantic embeddings, and local calling patterns improve generalization to novel families."
  },
  {
    "date": "2026-1-23",
    "title": "A Preliminary Systematic Review on LLM-Based System Assurance: Is Generative AI All You Need?",
    "authors": "Alvine Boaye Belle, Gerhard Yu",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00098",
    "source": "IEEE",
    "abstract": "Assurance cases allow verifying that the nonfunctional requirements (e.g., safety, security, reliability) of mission-critical systems are correctly implemented. This helps avoid system malfunction, which could have catastrophic consequences (e.g., deaths, environmental damage, financial losses). However, assurance cases are usually very large documents spanning several hundred pages. Their creation and maintenance may therefore be time-consuming and tedious. Relying on (semi)automated techniques such as those supported by generative AI through LLMs could alleviate the task of assurance case developers by facilitating the execution of all activities related to the assurance case lifecycle. In this paper, we report a critical analysis of the peer-reviewed literature on LLM-based system assurance to inform future research on this topic."
  },
  {
    "date": "2026-1-23",
    "title": "LLM-Driven Event Log Generation from Forensic Cases: A Comparative Study of ChatGPT, Claude, and Gemini",
    "authors": "Mirai Gendi, Periklis Andritsos",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00115",
    "source": "IEEE",
    "abstract": "Event logs underpin both process mining and digital forensic analysis. Yet turning unstructured case narratives into wellformed logs remains a non-trivial task. Much of the work to date has relied on bespoke Natural Language Processing (NLP) pipelines or transformer-based stacks, which are solutions that can be effective but are also engineering-intensive. This study asks a straightforward question: can large language models (LLMs) (ChatGPT, Claude, and Gemini, produce structured, XES-compatible event logs directly from forensic narratives? We examine eight case files and assess the outputs along four dimensions that matter in practice: temporal consistency, redundancy, schema compliance, and classification performance. The results are instructive. ChatGPT generated timelines that were fully consistent and adhered to the schema in all cases. Claude performed well overall but showed slightly lower consistency and compliance. Gemini achieved near-perfect temporal alignment, though its outputs varied more across cases. Pairwise comparisons indicate small but meaningful gaps in consistency. Taken together, these findings suggest that LLMs are a potential alternative to custom pipelines for event log generation. Even so, the forensic context warrants caution: outputs should be validated carefully before use, with attention to case details and downstream impact."
  },
  {
    "date": "2026-1-23",
    "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms",
    "authors": "Samah Kansab, Francis Bordeleau, Ali Tizghadam",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00092",
    "source": "IEEE",
    "abstract": "Empirical research on code review processes is increasingly central to understanding software quality and collaboration. However, collecting and analyzing review data remains a time-consuming and technically intensive task. Most researchers follow similar workflows-writing ad hoc scripts to extract, filter, and analyze review data from platforms like GitHub and GitLab. This paper introduces RevMine, a conceptual tool that streamlines the entire code review mining pipeline using large language models (LLMs). RevMine guides users through authentication, endpoint discovery, and natural language-driven data collection, significantly reducing the need for manual scripting. After retrieving review data, it supports both quantitative and qualitative analysis based on user-defined filters or LLM-inferred patterns. This poster outlines the tool's architecture, use cases, and research potential. By lowering the barrier to entry, RevMine aims to democratize code review mining and enable a broader range of empirical software engineering studies."
  },
  {
    "date": "2026-1-23",
    "title": "Resilient LLM-DBMS Pipelines via Event-Driven Fallback Orchestration",
    "authors": "Erfan Shahab, Sharareh Taghipour",
    "publish": "2025 IEEE International Conference on Collaborative Advances in Software and COmputiNg (CASCON)",
    "url": "https://doi.org/10.1109/cascon66301.2025.00113",
    "source": "IEEE",
    "abstract": "LLM-augmented database assistants face bursty demand, provider volatility, and data drift. These conditions hinder tail-latency and availability SLOs within cost and safety limits. This paper presents an event-driven self-healing pipeline with graded fallbacks-Degrade, Substitute, Bypass. Health monitors drive a simple policy with hysteresis. A provenanceaware cache keyed by schema fingerprints enables safe reuse, and a transaction guard blocks unsafe DDL/DML. A fault-injection prototype evaluates availability, latency, cost, cache hit rate, and safety under provider outages, quota exhaustion, latency jitter, schema drift, and traffic surges. Versus a naïve pipeline, it cuts p95 latency to 1156.8 ms (from 1403.4) and p99 to 1335.9 (from 1632.5), nearly halves cost, sustains near 100% availability (vs 96.5%), and blocks 13 unsafe actions. The trade-off is a controlled 17% drop in a quality proxy during fallbacks. Overall, the results provide a practical blueprint for resilient, policy-controlled LLM-DB deployments with tunable accuracy-latency-cost trade-offs."
  },
  {
    "date": "2026-1-23",
    "title": "Evolutionary AI Architectures: Automated Design for Robust AI Systems and LLM Enhancement",
    "authors": "Anshjyot Singh Wadhwa, Mohit Chaudhary, Nitin Kumar Goyal",
    "publish": "2025 IEEE 2nd International Conference on Information Technology, Electronics and Intelligent Communication Systems (ICITEICS)",
    "url": "https://doi.org/10.1109/iciteics64870.2025.11340844",
    "source": "IEEE",
    "abstract": "This paper introduces a method for autonomously engineering powerful neural network ensembles, bypassing the laborious manual tuning processes typically associated with complex AI architectures. Through the strategic application of genetic algorithms, this work demonstrates the simultaneous generation and optimization of network configurations, culminating in highly robust and consistent performance outcomes. The presented technique offers a scalable paradigm for developing advanced artificial intelligence capabilities, providing a significant avenue for enhancing the foundational design and efficacy of large language models and other sophisticated AI systems."
  },
  {
    "date": "2026-1-23",
    "title": "Comparative Evaluation of Fragmentation and Encoding Strategies in a Web Agent Integrating RAG and Local LLM",
    "authors": "Yeison Javier Muñoz Gomez, Fabian Armando Hoyos Cerón, Juan José Caiza Narvaez",
    "publish": "2025 IEEE Colombian Caribbean Conference (C3)",
    "url": "https://doi.org/10.1109/c366505.2025.11340287",
    "source": "IEEE",
    "abstract": "This study presents a comparative evaluation of an autonomous web browsing agent based on Retrieval-Augmented Generation (RAG) architecture with local LLM. A modular system integrating ethical web content extraction, semantic vector processing and inference orchestration was implemented using Mistral-7B. Four experimental configurations were evaluated on three institutional domains (.edu, .gov, .org) using quantitative metrics of accuracy, relevance, coherence, completeness, and evidence precision. Results reveal that simpler configurations outperform theoretically more sophisticated approaches, evidencing a “performance paradox” where hyperfragmentation degrades semantic context. The technical integrity of web sources sets fundamental performance limits, with server errors and broken links creating insurmountable blind spots. The study concludes that preserving semantic context is more critical than maximizing granularity, advocating domain-specific adaptive strategies rather than universal configurations."
  },
  {
    "date": "2026-1-23",
    "title": "Detecting the Invisible: Adversarial Strategies for AI-Generated Text in the LLM Era",
    "authors": "Kent Alber Fredson, Yithro Paulus Tjendra, Leander Farrell Suryadi, Puti Andam Suri",
    "publish": "2025 8th International Conference on Information and Communications Technology (ICOIACT)",
    "url": "https://doi.org/10.1109/icoiact67584.2025.11345089",
    "source": "IEEE",
    "abstract": "As LLM-generated text becomes increasingly human-like, detecting it, especially when paraphrased, becomes more challenging. This paper enhances the AI-Catcher model by introducing adversarial training using the DAIGT v4 dataset, with a key focus on adding perturbed samples during training and new linguistic and statistical features. The goal is to make the model more aware of paraphrastic variations which often help LLM-generated content evade detection. This paper approach improves the model's robustness by exposing it to both human- and LLM-generated paraphrases, enabling better generalization and higher accuracy, especially in adversarial settings. Our experiments further validate the effectiveness of this enhancement, with the enhanced model outperforming the baseline. Specifically, the inclusion of new features led to a 0.6% increase in F1-score compared to the previous study, followed by an additional 0.8% gain after applying adversarial training."
  }
]