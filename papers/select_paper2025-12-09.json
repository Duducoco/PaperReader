[
  {
    "date": "2025-12-09",
    "title": "An Explainable AI Model for the Detecting Malicious Smart Contracts Based on EVM Opcode Based Features",
    "authors": "Roopak Surendran",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08782v1",
    "source": "arXiv",
    "abstract": "Hackers may create malicious solidity programs and deploy it in the Ethereum block chain. These malicious smart contracts try to attack legitimate programs by exploiting its vulnerabilities such as reentrancy, tx.origin attack, bad randomness, deligatecall and so on. This may lead to drain of the funds, denial of service and so on . Hence, it is necessary to identify and prevent the malicious smart contract before deploying it into the blockchain. In this paper, we propose an ML based malicious smart contract detection mechanism by analyzing the EVM opcodes. After balancing the opcode frequency dataset with SMOTE algorithm, we transformed opcode frequencies to the binary values (0,1) using an entropy based supervised binning method. Then, an explainable AI model is trained with the proposed binary opcode based features. From the implementations, we found that the proposed mechanism can detect 99% of malicious smart contracts with a false positive rate of only 0.01. Finally, we incorporated LIME algorithm in our classifier to justify its predictions. We found that, LIME algorithm can explain why a particular smart contract app is declared as malicious by our ML classifier based on the binary value of EVM opcodes.",
    "title_zh": "基于EVM操作码特征的可解释AI模型在恶意智能合约检测中的应用",
    "abstract_zh": "黑客可能创建恶意的Solidity程序并将其部署到以太坊区块链上。这些恶意智能合约试图通过利用诸如重入攻击、tx.origin攻击、不良随机性、delegatecall等漏洞来攻击合法程序，可能导致资金被窃取、服务拒绝等问题。因此，在将智能合约部署到区块链之前，识别并防止恶意智能合约至关重要。本文提出了一种基于机器学习的恶意智能合约检测机制，通过分析EVM操作码来实现。在使用SMOTE算法对操作码频率数据集进行平衡后，我们采用基于熵的监督分箱方法将操作码频率转换为二值（0,1）特征。随后，基于所提出的二值操作码特征训练了一个可解释的人工智能模型。实验结果表明，该机制能够检测出99%的恶意智能合约，且误报率仅为0.01%。最后，我们在分类器中引入了LIME算法，以验证和解释模型的预测结果。研究发现，LIME算法能够根据EVM操作码的二值特征，解释为何某个特定的智能合约被我们的机器学习分类器判定为恶意。"
  },
  {
    "date": "2025-12-09",
    "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
    "authors": "Eranga Bandara, Ross Gore, Peter Foytik, Sachin Shetty, Ravi Mukkamala, Abdul Rahman, Xueping Liang, Safdar H. Bouk, Amin Hass, Sachini Rajapakse, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08769v1",
    "source": "arXiv",
    "abstract": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
    "title_zh": "设计、开发和部署生产级智能代理AI工作流的实用指南",
    "abstract_zh": "智能代理型AI标志着自主系统在推理、规划和执行多步骤任务方面的一次重大转变。与传统的单一模型提示方式不同，智能代理工作流通过整合多个具备特定功能的智能体（每个智能体配备不同的大语言模型LLM）、工具增强能力、编排逻辑以及与外部系统的交互，构建出能够实现自主决策与行动的动态处理管道。随着其在产业界和学术研究领域的快速普及，组织面临一个核心挑战：如何设计、工程化并运营可投入生产环境的智能代理型AI工作流，使其具备可靠性、可观测性、可维护性，并符合安全与治理要求。\n\n本文提供了一份实用且端到端的指南，涵盖智能代理型AI系统的设计、开发与部署全过程。我们提出了一套结构化的工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议（Model Context Protocol, MCP）、工具集成、确定性编排、负责任AI（Responsible-AI）考量，以及面向环境感知的部署策略。随后，我们总结出九项核心最佳实践，以指导生产级智能代理型AI工作流的工程实现：\n\n1. 优先采用“工具先行”设计，而非依赖MCP；\n2. 使用纯函数调用方式；\n3. 设计单工具、单职责的智能体；\n4. 将提示词管理外部化；\n5. 构建与负责任AI对齐的模型联盟（model-consortium）架构；\n6. 清晰分离工作流逻辑与MCP服务器；\n7. 采用容器化部署以支持可扩展的运维；\n8. 坚持“保持简单，愚蠢”（Keep it Simple, Stupid, KISS）原则，以维持系统的简洁性与鲁棒性。\n\n为验证上述原则的实际应用效果，本文呈现了一个完整的案例研究：一个多模态新闻分析与媒体生成工作流。通过融合架构指导、操作模式与实际实施经验，本文为构建稳健、可扩展且可投入生产的智能代理型AI工作流提供了基础性参考。"
  },
  {
    "date": "2025-12-09",
    "title": "Towards Foundation Models with Native Multi-Agent Intelligence",
    "authors": "Shuyue Hu, Haoyang Yan, Yiqun Zhang, Yang Chen, Dongzhan Zhou, Lei Bai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08743v1",
    "source": "arXiv",
    "abstract": "Foundation models (FMs) are increasingly assuming the role of the \"brain\" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.",
    "title_zh": "具有原生多智能体智能的基础模型",
    "abstract_zh": "基础模型（FMs）正日益扮演着人工智能代理“大脑”的角色。尽管近期的研究已开始赋予FMs原生的单智能体能力——例如图形界面交互或集成工具使用——但我们认为，下一个前沿方向是赋予FMs原生的多智能体智能。我们识别出FMs在多智能体情境下的四项核心能力：理解、规划、高效沟通以及适应性。与某些关于这些能力会自发涌现的假设相反，我们在41个大型语言模型上提供了广泛的实证证据，表明强大的单智能体表现并不能自动转化为稳健的多智能体智能。为弥合这一差距，我们提出了若干关键研究方向——涵盖数据集构建、评估方法、训练范式以及安全考量——以推动具备原生多智能体智能的基础模型的发展。"
  },
  {
    "date": "2025-12-09",
    "title": "Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs",
    "authors": "Md Nazmul Haque, Elizabeth Lin, Lawrence Arkoh, Biruk Tadesse, Bowen Xu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08213v1",
    "source": "arXiv",
    "abstract": "Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation. In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.",
    "title_zh": "安全还是可疑？探究原始与量化大语言模型中 Shell 命令的包装幻觉",
    "abstract_zh": "用于代码的大语言模型（LLMs4Code）在生成软件制品方面正变得越来越普遍，包括在Go等编程语言中推荐库和包。然而，近期研究表明，大语言模型经常虚构包名称，或生成包含已知安全漏洞的依赖项，这对开发者以及下游软件供应链构成了重大风险。与此同时，量化技术已成为降低推理成本、并使大语言模型能够在资源受限环境中部署的广泛采用手段。尽管该技术广受欢迎，但目前对其如何影响大语言模型生成软件依赖项的正确性与安全性——尤其是在生成包安装Shell命令时——仍缺乏深入理解。在本研究中，我们首次系统地开展了关于量化对大语言模型生成Go包时的包虚构现象（Package Hallucination Rate, PHR）及漏洞风险影响的实证研究。我们在三个数据集（SO、MBPP 和 paraphrase）上，评估了五种不同规模的Qwen模型在全精度、8位量化和4位量化下的表现。结果表明，量化显著提高了包虚构率，其中4位量化模型的退化最为严重。此外，我们还发现，即使在生成正确的包中，随着精度下降，漏洞存在率（Vulnerability Presence Rate, VPR）也呈上升趋势，表明低精度模型的安全风险更高。最后，对虚构输出的分析显示，大多数被伪造的包都呈现出类似真实URL结构的Go模块路径，尤其是最常见的GitHub或golang.org仓库的格式错误或不存在的地址，揭示出大语言模型在虚构依赖项时具有系统性的模式。总体而言，我们的研究为量化大语言模型在代码生成与依赖推荐中的可靠性与安全性提供了切实可行的洞见。"
  },
  {
    "date": "2025-12-09",
    "title": "Information-Dense Reasoning for Efficient and Auditable Security Alert Triage",
    "authors": "Guangze Zhao, Yongzheng Zhang, Changbo Tian, Dan Xie, Hongri Liu, Bailing Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08169v1",
    "source": "arXiv",
    "abstract": "Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.",
    "title_zh": "信息密集型推理在高效且可审计的安全告警分类中的应用",
    "abstract_zh": "安全运营中心（SOC）在分钟级的服务窗口内面临海量且异构的告警流，由此产生了“告警优先级判定延迟悖论”：详尽的推理链虽能保障准确性和合规性，却带来难以承受的延迟和Token成本；而过于简略的推理链则牺牲了透明度与可审计性。现有解决方案均存在缺陷：基于规则的系统脆弱易损，异常检测方法缺乏可操作性，而完全依赖云端托管的大语言模型（LLM）又引发延迟、成本与隐私方面的担忧。\n\n为此，我们提出AIDR——一种混合云边协同框架，通过受约束的信息密度优化来破解这一权衡难题。其核心创新在于基于梯度的推理链压缩技术，仅保留对决策至关重要的步骤，即在满足Token与延迟预算的前提下，提取足以支撑预测的最小必要证据。实验证明，该方法在最大限度保留决策相关信息的同时，显著降低复杂度。\n\n我们通过知识蒸馏将原始告警提炼为3至5个高信息量要点（实现68%的Token减少），利用LoRA微调领域专用专家模型，并部署云边协同架构：云端大模型负责告警路由，本地化专家系统生成符合SOAR标准的JSON输出。实验表明，AIDR相较传统思维链（Chain-of-Thought）方法，在保持更高准确率的同时，实现了40.6%的延迟降低，且具备对数据损坏的鲁棒性以及跨分布场景的泛化能力，真正实现可审计、高效、符合全数据本地化合规要求的安全运营中心告警优先级处理。"
  },
  {
    "date": "2025-12-09",
    "title": "Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance",
    "authors": "Aliaksei Kaliutau",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08492v1",
    "source": "arXiv",
    "abstract": "Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the \"Semantic Trap\" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.",
    "title_zh": "自主问题解决者：迈向零接触代码维护",
    "abstract_zh": "大型语言模型的最新进展已彻底改变了函数级别的代码生成；然而，仓库级自动化程序修复（APR）仍是重大挑战。当前的方法通常采用以控制为中心的范式，迫使智能体在复杂的目录结构和无关的控制逻辑中艰难导航。本文提出了一种范式转变：从传统的代码属性图（CPGs）转向数据转换图（DTG）的概念，该方法通过将数据状态建模为节点、函数建模为边，反转了图的拓扑结构，使智能体能够通过数据血缘追踪逻辑缺陷，而非依赖于控制流。我们提出了一种多智能体框架，实现了数据完整性导航与控制流逻辑的协调统一。理论分析与案例研究均表明，该方法有效解决了现代代码智能体中标准RAG系统固有的“语义陷阱”问题。我们构建了一个完整的实现——自主问题求解器（AIR），这是一个无需人工干预的自进化代码维护系统，采用神经符号推理，并利用DTG结构实现可扩展的逻辑修复。我们的方法在多个SWE基准测试中表现出色，在SWE-Verified基准上达到了87.1%的问题解决率。该方法直接应对了现有AI代码助手工具的核心局限，满足了日益依赖软件的世界对更强大基础架构的迫切需求。"
  },
  {
    "date": "2025-12-09",
    "title": "DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals",
    "authors": "Kaiwei Liu, Yuting He, Bufang Yang, Mu Yuan, Chun Man Victor Wong, Ho Pong Andrew Sze, Zhenyu Yan, Hongkai Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08379v1",
    "source": "arXiv",
    "abstract": "Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.",
    "title_zh": "DeepFeature：用于可穿戴生物信号的迭代上下文感知特征生成",
    "abstract_zh": "从可穿戴设备收集的生物信号在医疗健康应用中得到了广泛应用。这些应用中使用的机器学习模型通常依赖于从生物信号中提取的特征，因为这类特征具有高效性、更低的数据维度以及对各种模型架构的良好兼容性。然而，现有的特征提取方法往往缺乏针对特定任务的上下文知识，在高维特征空间中难以确定最优的特征提取设置，并且容易出现代码生成和自动化过程中的错误。本文提出 DeepFeature，这是首个基于大语言模型（LLM）赋能、具备上下文感知能力的可穿戴生物信号特征生成框架。DeepFeature 引入了一种多源特征生成机制，将专家知识与具体任务设置相结合；同时采用基于特征评估反馈的迭代特征优化流程，实现特征的重新选择与精炼。此外，DeepFeature 还设计了一种稳健的多层过滤与验证机制，以确保特征到代码的转换过程稳定可靠，避免提取函数运行崩溃。实验评估结果表明，与基线方法相比，DeepFeature 在八个不同任务上实现了平均 AUROC 提升 4.21% 至 9.67%；在五个任务上优于当前最先进的方法，而在其余任务上也保持了相当的性能水平。"
  },
  {
    "date": "2025-12-09",
    "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA",
    "authors": "Jing Zhang, Lianghong Guo, Yanlin Wang, Mingwei Liu, Jiachi Chen, Yuchi Ma, Ensheng Shi, Terry Yue Zhuo, Hongyu Zhang, Zibin Zheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08867v1",
    "source": "arXiv",
    "abstract": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.",
    "title_zh": "SimpleDevQA：大型语言模型在开发知识问答任务上的基准测试",
    "abstract_zh": "开发知识问答（Dev Knowledge QA）任务旨在为软件开发过程中的知识探索型问题提供自然语言答案。为了探究该任务的重要性及其研究深度，我们分析了来自WildChat的真实用户-大模型对话数据，发现：（1）Dev Knowledge QA任务占所有交互的39.6%，是各类任务中占比最高的（高于代码生成任务的32.3%），表明开发者在实际开发中存在广泛的知识需求；（2）仅有27.5%的真实Dev Knowledge QA对话聚焦于代码理解，大多数对话涉及更广泛的开发知识需求；（3）仅17.1%的真实世界Dev Knowledge QA对话可用于构建基准测试。现有基准在评估大模型的Dev Knowledge QA能力方面存在两个主要局限：第一，现有基准涵盖的开发知识范围有限，主要集中在代码理解，忽视了开发过程中更广泛的知识需求；第二，部分基准并非基于真实用户查询构建。为弥合这一差距，我们设计了一个三阶段流程，将真实对话转化为简洁的开发知识问答对。通过该流程，我们提出了SimpleDevQA——一个源自真实用户对话的多语言基准。该基准包含三种语言（英语、中文和俄语）共2,740个问答对，专注于具有唯一性、简短且可验证的答案，以实现准确而简便的评估。实验结果表明：代码类大模型在性能上普遍优于同规模的一般大模型；采用检索增强生成（RAG）策略进行知识注入后，大模型的平均准确率提升11.3%；大模型在Dev Knowledge QA任务中表现出系统性过度自信，其回答准确率与所声明的信心水平呈正相关；总体而言，代码生成能力越强的大模型，在Dev Knowledge QA任务中表现也越出色。"
  },
  {
    "date": "2025-12-09",
    "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems",
    "authors": "Po-An Shih, Shao-Hua Wang, Yung-Che Li, Chia-Heng Tu, Chih-Han Chang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08476v1",
    "source": "arXiv",
    "abstract": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.",
    "title_zh": "面向自动驾驶系统设计空间探索的多智能体大模型框架",
    "abstract_zh": "设计自动驾驶系统需要在多种环境条件下，对庞大的软硬件配置空间进行高效探索，例如在交通状况、天气条件和道路布局各异的情况下。传统的设计空间探索（DSE）方法难以应对多模态的执行输出以及复杂的性能权衡问题，通常还需要人工介入来根据执行结果评估系统的正确性。本文提出了一种基于多智能体与大语言模型（LLM）的DSE框架，该框架将多模态推理与三维仿真及性能分析工具相结合，实现了对执行结果的自动化解读，并引导系统设计方案的探索。通过部署专门的LLM智能体，分别负责用户输入理解、设计点生成、执行任务调度以及对视觉和文本形式的执行输出进行分析，从而能够在无需人工干预的情况下识别潜在的性能瓶颈。我们开发了一个原型系统，并在一辆无人驾驶出租车案例研究（SAE四级自动驾驶应用）上进行了评估。实验结果表明，相较于遗传算法基线方法，所提出的框架在相同的探索预算下，能够发现更多帕累托最优且成本效益更高的解决方案，并显著降低导航时间。实验还验证了采用基于LLM的方法进行DSE的高效性。我们认为，这一框架为自动驾驶系统的自动化设计开辟了新的路径。"
  },
  {
    "date": "2025-12-09",
    "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models",
    "authors": "Hui Wang, Yang Liu, Xiaoyu Zhang, Chaoxu Mu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08609v1",
    "source": "arXiv",
    "abstract": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.",
    "title_zh": "CogMCTS：一种用于大语言模型迭代启发式演化的新型认知引导蒙特卡洛树搜索框架",
    "abstract_zh": "自动启发式设计（AHD）是一种解决复杂优化问题的有效框架。大型语言模型（LLMs）的发展使得启发式规则的自动生成成为可能。现有的基于LLM的进化方法依赖于种群策略，容易陷入局部最优。将LLM与蒙特卡洛树搜索（MCTS）相结合，能够在探索与利用之间取得更好的平衡，但多轮认知融合能力仍然有限，且搜索多样性受到制约。为克服这些局限性，本文提出一种新型的认知引导MCTS框架（CogMCTS）。CogMCTS将LLM的认知引导机制与MCTS紧密结合，实现高效的自动化启发式优化。该框架通过多轮认知反馈，融合历史经验、节点信息以及负面结果，动态优化启发式生成过程。结合双轨节点扩展与精英启发式管理机制，有效平衡了多样化启发式的探索与高质量经验的利用。此外，通过策略性变异对启发式形式和参数进行调整，进一步提升了解空间的多样性及整体优化性能。实验结果表明，CogMCTS在稳定性、效率和解的质量方面均优于现有的基于LLM的AHD方法。"
  },
  {
    "date": "2025-12-09",
    "title": "rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection",
    "authors": "Sijia Chen, Baochun Li, Di Niu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08300v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.",
    "title_zh": "rSIM：通过强化策略注入激励大语言模型的推理能力",
    "abstract_zh": "大型语言模型（LLMs）通过强化学习（RL）进行后训练，进化为推理语言模型（RLMs），其核心特征在于“顿悟”时刻——即在思维链（CoTs）中开始采用自省、深度思考等策略。受此启发，本文提出一种新颖的强化策略注入机制（rSIM），使任意LLM均可通过一个小型规划器引导其思维链，自适应地注入推理策略，从而转变为RLM。为实现这一目标，规划器（领导者代理）与LLM（跟随者代理）基于领导者-跟随者框架，利用多智能体强化学习（MARL）进行联合训练，并采用简单规则化的奖励机制。实验结果表明，rSIM可使Qwen2.5-0.5B模型具备RLM能力，并显著超越Qwen2.5-14B模型。此外，该规划器具有良好的泛化性：仅需一次训练即可作为即插即用的模块，显著提升现有LLM的推理能力。同时，规划器支持跨任务的持续学习，使其规划能力能够逐步提升，并推广至更广泛的问题场景。"
  },
  {
    "date": "2025-12-09",
    "title": "Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand",
    "authors": "Zhensu Sun, Chengran Yang, Xiaoning Du, Zhou Yang, Li Li, David Lo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08266v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level. In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.",
    "title_zh": "Token Sugar：通过高效的缩写方式让源代码对大语言模型更友好",
    "abstract_zh": "大型语言模型（LLMs）在代码生成与理解任务中表现出色，但其高昂的计算成本限制了更广泛的应用。一个重要原因在于编程语言固有的冗余性，例如不必要的格式化元素和冗长的样板代码。这导致输入和生成输出中的标记数量显著增加，从而提高了推理成本并减慢了生成速度。以往的研究通过简化编程语言的语法结构来改善这一问题，减少了代码理解和生成任务中的标记使用量。然而，这些方法仅限于语法层面的转换，未能充分挖掘语义层面的标记压缩潜力。\n\n在本研究中，我们提出了“Token Sugar”这一概念：将频繁出现且冗长的代码模式替换为可逆、高效的简写形式，以降低源代码中的标记数量。为实现该理念，我们设计了一套系统性方案，从代码语料库中挖掘高频且高耗标记的代码模式，为每个模式映射唯一的简写，并通过代码转换的方式将其融入LLM的预训练过程。基于此方案，我们共获得799对（代码模式, 简写）组合，可在源代码中最多减少15.1%的标记数量，且与现有专注于语法优化的方法具有互补性。\n\n此外，我们使用Token Sugar增强的数据对三种广泛应用的LLM进行了再训练。实验结果表明，这些模型在生成过程中不仅实现了显著的标记节省（最高达11.2%），同时在Pass@1指标上与基于原始代码训练的基线模型保持近乎一致的性能表现。"
  },
  {
    "date": "2025-12-09",
    "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models",
    "authors": "Haoran Wang, Zhuohang Chen, Guang Li, Bo Ma, Chuanghuang Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08145v1",
    "source": "arXiv",
    "abstract": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.",
    "title_zh": "与无人机对话——基于大语言模型的人机交互",
    "abstract_zh": "无人机交互系统的发展正从工程师主导转向用户主导，旨在取代传统的预设人机交互设计。这一转变致力于实现更个性化的任务规划与设计，从而提升交互体验的质量并增强灵活性，可广泛应用于农业、航拍、物流及环境监测等多个领域。然而，由于用户与无人机之间缺乏通用语言，此类交互往往难以实现。大型语言模型（LLM）的发展使得理解自然语言和机器人（无人机）行为成为可能，为个性化人机交互提供了技术基础。近年来，一些基于大语言模型的人机交互框架已被提出，但普遍存在混合任务规划与执行困难的问题，导致在复杂场景中适应性较差。本文提出一种新型双智能体人机交互框架。该框架构建了两个独立的LLM智能体——任务规划代理与执行代理，并通过不同的提示工程（Prompt Engineering）分别处理任务的理解、规划与执行。为验证该框架的有效性与性能，我们建立了一个涵盖无人机四大典型应用场景的任务数据库，并采用三个独立指标对人机交互框架的性能进行量化评估。同时，选取不同LLM模型控制无人机，对比其表现。用户实验结果表明，该框架显著提升了人机交互的流畅性以及任务执行的灵活性，有效满足了用户在设定场景下的个性化需求。"
  },
  {
    "date": "2025-12-09",
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "authors": "Xiang Chen, Yuling Shi, Qizhen Lan, Yuchao Qiu, Xiaodong Gu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08870v1",
    "source": "arXiv",
    "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
    "title_zh": "Fed-SE：面向隐私约束多环境大模型智能体的联邦自进化方法",
    "abstract_zh": "大型语言模型（LLM）代理已被广泛应用于复杂的交互任务中，然而隐私约束常常阻碍在动态环境中的集中式优化与协同进化。尽管联邦学习（FL）在静态数据集上已证明有效，但其向开放式的代理自演化扩展仍鲜有研究。直接应用传统联邦学习面临挑战：任务异构性以及稀疏的、基于轨迹的奖励信号会引发严重的梯度冲突，从而破坏全局优化过程的稳定性。为弥合这一差距，我们提出了 Fed-SE——一种面向 LLM 代理的联邦自演化框架。Fed-SE 构建了一种“本地演化—全局聚合”的范式：在本地，代理对筛选出的高回报轨迹进行参数高效微调，以实现稳定的梯度更新；在全局层面，Fed-SE 在低秩子空间内聚合更新，解耦了环境特异性动态，有效降低了不同客户端之间的负迁移。在五个异构环境中的实验表明，与联邦基线方法相比，Fed-SE 将平均任务成功率提升了约18%，验证了其在隐私受限部署下实现鲁棒跨环境知识迁移的有效性。"
  },
  {
    "date": "2025-12-09",
    "title": "Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships",
    "authors": "Bin Wang, Hui Li, Liyang Zhang, Qijia Zhuang, Ao Yang, Dong Zhang, Xijun Luo, Bing Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08326v1",
    "source": "arXiv",
    "abstract": "Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.",
    "title_zh": "阿耳古斯：一种基于分层参考关系的多智能体敏感信息泄露检测框架",
    "abstract_zh": "代码仓库中的敏感信息泄露已成为一项关键的安全挑战。传统的检测方法依赖正则表达式、指纹特征和高熵计算，往往存在较高的误报率，不仅降低了检测效率，还显著增加了开发人员的手动筛查负担。近年来，大语言模型（LLMs）以及多智能体协同架构的进展，在处理复杂任务方面展现出巨大潜力，为敏感信息检测提供了全新的技术视角。针对上述挑战，我们提出了Argus——一种用于检测敏感信息的多智能体协同框架。Argus采用三层检测机制，融合关键内容、文件上下文及项目引用关系，有效降低误报率，提升整体检测准确率。为在真实仓库环境中全面评估Argus，我们构建了两个新基准：一个用于评估真实泄漏检测能力，另一个用于衡量误报过滤性能。实验结果表明，Argus在泄漏检测中最高可达94.86%的准确率，精确率为96.36%，召回率为94.64%，F1得分为0.955。此外，对97个真实仓库的分析总成本仅为2.2美元。所有代码实现及相关数据集均已公开，欢迎进一步研究与应用，地址为：https://github.com/TheBinKing/Argus-Guard。"
  },
  {
    "date": "2025-12-09",
    "title": "Towards a Science of Scaling Agent Systems",
    "authors": "Yubin Kim, Ken Gu, Chanwoo Park, Chunjong Park, Samuel Schmidgall, A. Ali Heydari, Yao Yan, Zhihan Zhang, Yuchen Zhuang, Mark Malhotra, Paul Pu Liang, Hae Won Park, Yuzhe Yang, Xuhai Xu, Yilun Du, Shwetak Patel, Tim Althoff, Daniel McDuff, Xin Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08296v1",
    "source": "arXiv",
    "abstract": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",
    "title_zh": "迈向代理系统扩展性的科学",
    "abstract_zh": "代理系统，即基于语言模型（LM）能够进行推理、规划与行动的系统，正逐渐成为现实世界人工智能应用的主导范式。尽管这一趋势广泛存在，决定其性能的基本原理仍缺乏深入研究，导致从业者不得不依赖启发式方法而非有原则的设计决策。为填补这一空白，我们推导出代理系统定量的扩展规律。我们在四个不同的基准测试中进行了评估：Finance-Agent、BrowseComp-Plus、PlanCraft 和 Workbench。通过在三个主流大语言模型家族中实现五种典型架构（单一型、独立型、集中型、去中心化型、混合型），我们对180种配置进行了受控实验，采用标准化工具和固定的token预算。我们基于实证的协调度量指标（包括效率、开销、错误放大和冗余）构建了一个预测模型，实现了交叉验证下的R²=0.513。\n\n我们识别出三个主导效应：（1）工具-协调权衡：在固定计算预算下，工具密集型任务会因多代理协作开销而遭受不成比例的性能损失；（2）能力饱和现象：当单代理基线性能超过约45%后，进一步的协调反而带来递减甚至负收益（β = -0.408，p < 0.001）；（3）拓扑结构相关的错误放大：独立代理会导致错误传播 unchecked，使错误放大达17.2倍，而集中式协调可将此控制在4.4倍以内。\n\n集中式协调在可并行的任务（如金融推理）上使性能提升80.9%，而去中心化协调在动态网页导航任务中表现更优（+9.2% vs. +0.2%）。然而，在顺序推理任务中，所有多代理变体均导致性能下降39%-70%。该框架能够预测87%未见配置中的最优协调策略，提供了一套基于可测量任务特性的智能体扩展预测原则。"
  },
  {
    "date": "2025-12-09",
    "title": "Exposing and Defending Membership Leakage in Vulnerability Prediction Models",
    "authors": "Yihan Liao, Jacky Keung, Xiaoxue Ma, Jingyu Zhang, Yicheng Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08291v1",
    "source": "arXiv",
    "abstract": "Neural models for vulnerability prediction (VP) have achieved impressive performance by learning from large-scale code repositories. However, their susceptibility to Membership Inference Attacks (MIAs), where adversaries aim to infer whether a particular code sample was used during training, poses serious privacy concerns. While MIA has been widely investigated in NLP and vision domains, its effects on security-critical code analysis tasks remain underexplored. In this work, we conduct the first comprehensive analysis of MIA on VP models, evaluating the attack success across various architectures (LSTM, BiGRU, and CodeBERT) and feature combinations, including embeddings, logits, loss, and confidence. Our threat model aligns with black-box and gray-box settings where prediction outputs are observable, allowing adversaries to infer membership by analyzing output discrepancies between training and non-training samples. The empirical findings reveal that logits and loss are the most informative and vulnerable outputs for membership leakage. Motivated by these observations, we propose a Noise-based Membership Inference Defense (NMID), which is a lightweight defense module that applies output masking and Gaussian noise injection to disrupt adversarial inference. Extensive experiments demonstrate that NMID significantly reduces MIA effectiveness, lowering the attack AUC from nearly 1.0 to below 0.65, while preserving the predictive utility of VP models. Our study highlights critical privacy risks in code analysis and offers actionable defense strategies for securing AI-powered software systems.",
    "title_zh": "暴露与防御漏洞预测模型中的成员泄露问题",
    "abstract_zh": "用于漏洞预测（VP）的神经模型通过从大规模代码仓库中学习，取得了令人瞩目的性能。然而，这些模型容易受到成员推断攻击（Membership Inference Attacks, MIAs）的影响——攻击者试图推断特定代码样本是否曾被用于训练过程——这带来了严重的隐私隐患。尽管MIAs在自然语言处理和计算机视觉领域已得到广泛研究，但其对安全关键型代码分析任务的影响仍鲜有探讨。在本研究中，我们首次对VP模型中的MIAs进行了全面分析，评估了多种模型架构（LSTM、BiGRU和CodeBERT）以及不同特征组合（包括嵌入表示、logits、损失值和置信度）下的攻击成功率。我们的威胁模型符合黑盒和灰盒场景，即攻击者可观察到模型的预测输出，从而通过分析训练样本与非训练样本之间的输出差异来推断成员身份。实验结果表明，logits和损失值是泄露成员信息最显著且最易受攻击的输出。基于上述发现，我们提出一种基于噪声的成员推断防御方法（Noise-based Membership Inference Defense, NMID），该方法是一种轻量级防御模块，通过输出掩码和高斯噪声注入来干扰攻击者的推断能力。大量实验表明，NMID能显著降低MIAs的有效性，将攻击的AUC从接近1.0降至0.65以下，同时保持VP模型的预测性能。本研究揭示了代码分析中的关键隐私风险，并为保护人工智能驱动的软件系统提供了切实可行的防御策略。"
  },
  {
    "date": "2025-12-09",
    "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
    "authors": "Shiva Gaire, Srijan Gyawali, Saroj Mishra, Suman Niroula, Dilip Thakur, Umesh Yadav",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08290v1",
    "source": "arXiv",
    "abstract": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",
    "title_zh": "知识体系化：模型上下文协议生态系统中的安全与保障",
    "abstract_zh": "模型上下文协议（MCP）已成为连接大型语言模型（LLMs）与外部数据及工具的事实标准，其作用如同“智能代理AI的USB-C接口”。尽管这种将上下文与执行解耦的设计有效解决了关键的互操作性问题，却也引入了一个深刻的新威胁格局——在这一格局中，认知错误（幻觉）与安全漏洞（未经授权的操作）之间的界限变得模糊。本文作为一项知识体系化研究（SoK），旨在为MCP生态系统中的风险提供全面的分类框架，明确区分对抗性安全威胁（如间接提示注入、工具污染）与认知安全性隐患（如分布式工具委派中的对齐失败）。我们分析了MCP基本构件（资源、提示、工具）的结构性脆弱点，并揭示“上下文”如何被武器化，在多智能体环境中触发未经授权的操作。此外，本文还综述了前沿防御技术，涵盖从密码学溯源（ETDI）到运行时意图验证等多种手段，并最终提出从对话式聊天机器人向自主智能体操作系统演进过程中的安全保障路线图。"
  },
  {
    "date": "2025-12-09",
    "title": "RESTifAI: LLM-Based Workflow for Reusable REST API Testing",
    "authors": "Leon Kogler, Maximilian Ehrhart, Benedikt Dornauer, Eduard Paul Enoiu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08706v1",
    "source": "arXiv",
    "abstract": "With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.",
    "title_zh": "RESTifAI：基于大语言模型的可复用REST API测试工作流",
    "abstract_zh": "本文介绍了RESTifAI，一种基于大语言模型（LLM）的可复用、支持CI/CD流程的REST API测试生成方法，采用“正常路径”（happy-path）设计思路。与现有工具多聚焦于内部服务器错误不同，RESTifAI系统性地构建有效的测试场景（即正常路径），并推导出负向案例，以验证功能预期（2xx响应）以及对无效输入或业务规则违规的鲁棒性（4xx响应）。实验结果表明，RESTifAI在性能上可与最新的LLM工具（如AutoRestTest和LogiAgent）相媲美，同时解决了在可复用性、测试断言复杂度及集成方面存在的局限性。为支持该结论，我们提供了通用的对比实验结果，并展示了该工具在工业级服务中的实际适用性。工具演示视频请参见：https://www.youtube.com/watch?v=2vtQo0T0Lo4。RESTifAI已公开发布，获取地址为：https://github.com/casablancahotelsoftware/RESTifAI。"
  },
  {
    "date": "2025-12-09",
    "title": "MIRAGE: Misleading Retrieval-Augmented Generation via Black-box and Query-agnostic Poisoning Attacks",
    "authors": "Tailun Chen, Yu He, Yan Wang, Shuo Shao, Haolun Zheng, Zhihao Liu, Jinfeng Li, Yuefeng Chen, Zhixuan Chu, Zhan Qin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08289v1",
    "source": "arXiv",
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance LLMs with external knowledge but introduce a critical attack surface: corpus poisoning. While recent studies have demonstrated the potential of such attacks, they typically rely on impractical assumptions, such as white-box access or known user queries, thereby underestimating the difficulty of real-world exploitation. In this paper, we bridge this gap by proposing MIRAGE, a novel multi-stage poisoning pipeline designed for strict black-box and query-agnostic environments. Operating on surrogate model feedback, MIRAGE functions as an automated optimization framework that integrates three key mechanisms: it utilizes persona-driven query synthesis to approximate latent user search distributions, employs semantic anchoring to imperceptibly embed these intents for high retrieval visibility, and leverages an adversarial variant of Test-Time Preference Optimization (TPO) to maximize persuasion. To rigorously evaluate this threat, we construct a new benchmark derived from three long-form, domain-specific datasets. Extensive experiments demonstrate that MIRAGE significantly outperforms existing baselines in both attack efficacy and stealthiness, exhibiting remarkable transferability across diverse retriever-LLM configurations and highlighting the urgent need for robust defense strategies.",
    "title_zh": "幻象：通过黑盒且查询无关的投毒攻击实现误导性检索增强生成",
    "abstract_zh": "检索增强生成（RAG）系统通过引入外部知识来增强大语言模型（LLM）的能力，但同时也带来了关键的攻击面：语料库投毒。尽管近期研究已展示了此类攻击的潜力，但这些研究通常依赖于不切实际的假设，例如白盒访问权限或已知用户查询，从而低估了真实世界中实施攻击的难度。本文通过提出一种名为MIRAGE的新颖多阶段投毒框架，填补了这一研究空白。该框架专为严格的黑盒环境和查询无关场景设计，基于代理模型的反馈运行，作为一个自动优化框架，集成了三个核心机制：利用角色驱动的查询生成来近似潜在的用户搜索分布；采用语义锚定技术，以难以察觉的方式嵌入这些意图，从而实现高检索可见性；并借助一种对抗性测试时偏好优化（TPO）变体，最大化说服力。为了严格评估这一威胁，我们基于三个长篇、领域特定的数据集构建了一个新的基准测试。大量实验表明，MIRAGE在攻击效果和隐蔽性方面均显著优于现有基线方法，在多种检索器-大模型配置间展现出卓越的可迁移性，凸显了亟需建立强大防御策略的紧迫性。"
  },
  {
    "date": "2025-12-09",
    "title": "Multicalibration for LLM-based Code Generation",
    "authors": "Viola Campos, Robin Kuschnereit, Adrian Ulges",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08810v1",
    "source": "arXiv",
    "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",
    "title_zh": "基于大语言模型的代码生成多校准方法",
    "abstract_zh": "随着基于人工智能的代码生成技术日益普及，研究人员正致力于代码大语言模型（LLM）的校准问题——即确保其置信度分数能真实反映代码正确性的实际概率。为此，我们研究了多校准（multicalibration）方法，该方法能够捕捉编码问题的额外特征，如复杂度、代码长度或所用编程语言等。我们在三个函数合成基准测试上，对四种多校准方法进行了评估，使用了最新一代的代码大语言模型（Qwen3 Coder、GPT-OSS、DeepSeek-R1-Distill）。实验结果表明，相较于未经校准的token似然值（技能得分提升+1.03），以及基线校准方法（技能得分提升+0.37），多校准方法可带来显著性能提升。我们通过消融实验分析了上述因素的影响，并公开了我们的数据集（包含代码生成结果、似然值及正确性标签），以支持未来在代码LLM校准领域的研究。"
  },
  {
    "date": "2025-12-09",
    "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?",
    "authors": "Dyna Soumhane Ouchebara, Stéphane Dupont",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08493v1",
    "source": "arXiv",
    "abstract": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.",
    "title_zh": "基于大语言模型的漏洞代码增强：生成还是重构？",
    "abstract_zh": "漏洞代码库通常存在严重的数据不平衡问题，这限制了基于深度学习的漏洞分类器的有效性。数据增强技术可以通过缓解代表性不足的CWE（通用弱点枚举）样本稀缺问题来解决这一挑战。在此背景下，我们研究了基于大语言模型（LLM）的漏洞函数增强方法，比较了两种策略：一是通过受控生成新漏洞样本，二是对现有样本进行语义保持的重构。我们采用Qwen2.5-Coder生成增强数据，并使用CodeBERT作为漏洞分类器，在SVEN数据集上进行实验，结果表明，我们的方法能够通过简单且高效的过程，以合理质量丰富漏洞代码库，同时发现混合策略能最有效地提升漏洞分类器的性能。"
  },
  {
    "date": "2025-12-09",
    "title": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring",
    "authors": "Jiin Park, Hyuna Jeon, Yoonseo Lee, Jisu Hong, Misuk Kim",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08398v1",
    "source": "arXiv",
    "abstract": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.",
    "title_zh": "基于本体的工业标准文档知识图谱框架：通过分层与命题结构实现",
    "abstract_zh": "基于本体的知识图谱（KG）构建是一项核心技术，能够实现对领域知识的多维度理解与高级推理。工业标准尤其包含大量技术信息和复杂的规则，以高度结构化的形式呈现，结合了表格、适用范围、约束条件、例外情况以及数值计算等内容，使得知识图谱的构建尤为困难。在本研究中，我们提出一种方法，将此类文档组织为层次化的语义结构，将句子和表格分解为由条件规则和数值规则推导出的原子命题，并通过基于大语言模型（LLM）的三元组抽取技术将其整合进本体-知识图谱中。该方法有效捕捉了文档的层次结构与逻辑关系，充分表达了传统方法难以体现的领域特定语义。为验证其有效性，我们从工业标准中构建了规则数据集、表格数据集、多跳问答数据集以及有毒条款检测数据集，并实现了面向本体的KG-RAG框架进行对比评估。实验结果表明，与现有的KG-RAG方法相比，我们的方法在各类问答任务中均取得了显著的性能提升。本研究证明，即使面对条件、约束与适用范围交织复杂的工业文档，实现可靠且可扩展的知识表示也是可行的，为未来领域特定RAG的发展及智能文档管理提供了重要支持。"
  },
  {
    "date": "2025-12-09",
    "title": "USCSA: Evolution-Aware Security Analysis for Proxy-Based Upgradeable Smart Contracts",
    "authors": "Xiaoqi Li, Lei Xie, Wenkai Li, Zongwei Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08372v1",
    "source": "arXiv",
    "abstract": "In the case of upgrading smart contracts on blockchain systems, it is essential to consider the continuity of upgrade and subsequent maintenance. In practice, upgrade operations often introduce new vulnerabilities. To address this, we propose an Upgradable Smart Contract Security Analyzer, USCSA, which evaluates the risks associated with the upgrade process using the Abstract Syntax Tree (AST) differential analysis. We collected and analyzed 3,546 cases of vulnerabilities in upgradable contracts,covering common vulnerability categories such as reentrancy, access control flaws, and integer overflow. Experimental results show that USCSA achieves an accuracy of 92.3%, recall of 89.7%, and F1-score of 91.0% in detecting upgrade-induced vulnerabilities. In addition, the efficiency of mapping high-risk changes has achieved a 30% improvement over the conventional approach. As a result, USCSA provides a significant advantage to improve the security and integrity of upgradable smart contracts, providing a novel and efficient solution to secure audits on blockchain applications.",
    "title_zh": "美国计算机学会：面向代理升级型智能合约的进化感知安全分析",
    "abstract_zh": "在区块链系统中升级智能合约时，必须充分考虑升级过程的连续性以及后续维护的可行性。实践中，升级操作常常引入新的安全漏洞。为应对这一问题，我们提出了一种可升级智能合约安全分析工具——USCSA（Upgradable Smart Contract Security Analyzer），该工具通过抽象语法树（AST）差异分析技术，评估升级过程中潜在的安全风险。我们收集并分析了3,546个可升级合约中的漏洞案例，涵盖重入攻击、权限控制缺陷、整数溢出等常见漏洞类型。实验结果表明，USCSA在检测升级引发的漏洞方面，准确率达到了92.3%，召回率为89.7%，F1分数为91.0%。此外，针对高风险变更的映射效率相比传统方法提升了30%。综上所述，USCSA显著提升了可升级智能合约的安全性与完整性，为区块链应用的安全审计提供了一种新颖且高效的解决方案。"
  },
  {
    "date": "2025-12-09",
    "title": "Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging",
    "authors": "Yi Pan, Wenbo Qian, Dedong Xie, Ruiyan Hu, Yigong Hu, Baris Kasikci",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08365v1",
    "source": "arXiv",
    "abstract": "The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them. We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.",
    "title_zh": "Magneton：通过差分能耗调试优化机器学习系统的能效",
    "abstract_zh": "机器学习（ML）模型的训练与部署已变得极为耗能。尽管现有的优化工作主要集中在硬件能效上，但一个被严重忽视的低效来源是软件层面的能量浪费，这通常源于不良的软件设计。这类问题常常表现为冗余或设计不佳的操作，消耗更多能量却并未提升性能。这些低效现象广泛存在于主流的机器学习框架和应用中，然而开发者往往缺乏可见性及工具来检测和诊断这些问题。为此，我们提出了“差分能效调试”这一新方法，其核心思想是：在功能相似的多个ML系统之间，其能耗可能存在巨大差异。基于这一观察，我们设计并实现了一款名为Magneton的能效分析工具，该工具可在操作符级别对功能相近的ML系统进行能耗对比，并自动定位导致过度能耗的代码段和配置选择。在涵盖大语言模型推理、通用机器学习框架以及图像生成等领域的9个流行ML系统上应用时，Magneton成功识别并诊断出16个已知的软件能效低效案例，还发现了8个此前未知的低效问题，其中7个已得到开发者的确认。"
  },
  {
    "date": "2025-12-09",
    "title": "LaMoSys3.5D: Enabling 3.5D-IC-Based Large Language Model Inference Serving Systems via Hardware/Software Co-Design",
    "authors": "Qipan Wang, Zhe Zhang, Shuangchen Li, Hongzhong Zheng, Zheng Liang, Yibo Lin, Runsheng Wang, Ru Huang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08731v1",
    "source": "arXiv",
    "abstract": "The success of large language models LLMs amplifies the need for highthroughput energyefficient inference at scale. 3DDRAMbased accelerators provide high memory bandwidth and therefore an opportunity to accelerate the bandwidthbound decode phase. However, how to adequately balance compute density for prefill with bandwidthcapacity for decode remains open. Moreover, most prior designs do not target endtoend serving, leaving the codesign of dataflow, parallel mapping, and scheduling underexplored. To bridge the gap, we present LaMoSys3.5D, to our knowledge the first scalable 3.5DIC architecture for LLM serving. LaMoSys3.5D composes heterogeneous 3DDRAM chiplets on a 2.5D interposer: computerich chiplets for prefill and bandwidthcapacityrich chiplets for decode. To realize efficient serving, we adopt a hardwaresoftware codesign spanning dataflow, parallel mapping, and introduce a thermalaware modeling and hierarchical designspace exploration framework. Across diverse LLMs and workloads, LaMoSys3.5D improves throughputperwatt over DGXA100 systems by 62 and achieves a 4.87 better endtoend latency geomean versus prior 3D designs. We further distill intriguing design guidelines for 3.5DIC architectures and endtoend inference serving.",
    "title_zh": "LaMoSys 3.5D：通过软硬件协同设计实现基于 3.5D 封装集成电路的大语言模型推理服务系统",
    "abstract_zh": "大型语言模型（LLMs）的成功进一步凸显了在大规模场景下实现高吞吐、低功耗推理的迫切需求。基于3D DRAM的加速器能够提供高内存带宽，因而为加速受带宽限制的解码阶段提供了契机。然而，如何在预填充阶段的计算密度与解码阶段的带宽容量之间实现有效平衡，仍是未解难题。此外，大多数现有设计并未面向端到端服务，导致数据流、并行映射和调度的协同设计仍处于探索不足的状态。为弥合这一差距，我们提出LaMoSys3.5D——据我们所知，首个专为LLM服务设计的可扩展3.5D集成电路架构。LaMoSys3.5D在2.5D中介层上集成异构的3D DRAM芯片小块：以计算能力丰富的芯片小块用于预填充，以带宽容量丰富的芯片小块用于解码。为实现高效的服务性能，我们采用贯穿数据流、并行映射的软硬件协同设计，并引入一种考虑热特性的建模方法以及分层的设计空间探索框架。在多种LLM和工作负载下，LaMoSys3.5D相较DGXA100系统实现了62%的每瓦吞吐量提升，且端到端延迟几何平均值比以往的3D设计优化了4.87倍。我们还提炼出适用于3.5D IC架构及端到端推理服务的若干关键设计准则。"
  },
  {
    "date": "2025-12-09",
    "title": "Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework",
    "authors": "Liao Hu, Qiteng Wu, Ruoyu Qi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.08286v1",
    "source": "arXiv",
    "abstract": "The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.",
    "title_zh": "利用SolidGPT赋能智能应用开发：一种边缘-云混合的AI代理框架",
    "abstract_zh": "将大型语言模型（LLMs）集成到移动和软件开发工作流中，始终面临三大需求之间的持续矛盾：语义理解能力、开发者生产力以及数据隐私保护。传统的基于云的工具虽然具备强大的推理能力，但存在数据泄露风险和延迟问题；而本地设备解决方案则难以实现对整个代码库及开发工具链的完整上下文理解。\n\n我们推出了 SolidGPT——一个基于 GitHub 构建的开源边缘-云端混合型开发者助手，旨在提升代码与工作空间的语义搜索能力。SolidGPT 能够帮助开发者实现以下功能：\n\n- **与你的代码库对话**：通过交互式查询代码与项目结构，无需手动查找即可快速定位所需方法和模块。  \n- **自动化软件项目流程**：自动生成产品需求文档（PRD）、任务拆解、看板（Kanban）甚至搭建网页应用的初始框架，并通过 VSCode 和 Notion 实现深度集成。  \n- **配置私有可扩展的智能代理**：支持导入私有代码文件夹（最多约 500 个文件），连接 Notion，通过嵌入与上下文训练自定义 AI 代理角色，并可通过 Docker、命令行或 VSCode 插件进行部署。\n\n在实际应用中，SolidGPT 通过以下方式显著提升开发者效率：\n\n- **语义丰富的代码导航**：告别在文件间盲目翻找，彻底解决“某个功能到底放在哪里”的困扰。  \n- **集成化文档与任务管理**：自动生成的 PRD 内容和任务看板可无缝同步至开发工作流中。  \n- **以隐私为核心的设计理念**：支持本地运行（通过 Docker 或 VSCode），完全掌控代码与数据安全，同时可根据需要选择性调用 LLM API。\n\n通过结合交互式代码查询、自动化项目初始化以及人机协同机制，SolidGPT 提供了一种实用且尊重隐私的边缘智能助手，有效加速真实世界的开发流程，特别适用于智能化的移动与软件工程场景。"
  },
  {
    "date": "2025-12-9",
    "title": "You Shall Not Stall: Achieving RISC-V On-Demand Runtime-Reconfiguration using SCAIE-V",
    "authors": "Tobias Scheipel, Maximilian Ogris, Marcel Baunach",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00049",
    "source": "IEEE",
    "abstract": "Embedded computing platforms increasingly require adaptable architectures to meet varying application demands. This paper explores how the instruction set of RISC-V-based microcontrollers can be extended at runtime using reconfigurable hardware accelerators and the SCAIE-V framework. Building on a flexible infrastructure based on dynamic partial reconfiguration and a scalable, standardized instruction interface within the pipeline, we demonstrate the integration of application-specific accelerators without sacrificing general-purpose capabilities. The experimental proof of concept based on the ASCON lightweight cryptographic algorithm validates the approach across two soft microcontrollers. With this, we showcase the usage of a software fallback mechanism to support seamless hardware/software transitions and enable memory handover between the software and hardware execution domains. The results show clear advantages in execution time and flexibility over static hardware designs while supporting long-term maintainability and sustainability through hardware reuse.",
    "title_zh": "你不得拖延：利用 SCAIE-V 实现 RISC-V 的按需运行时重构",
    "abstract_zh": "嵌入式计算平台日益需要可适应的架构，以满足不断变化的应用需求。本文探讨了如何利用可重构硬件加速器与SCAIE-V框架，在运行时对基于RISC-V的微控制器的指令集进行扩展。在基于动态部分重构的灵活基础设施以及流水线内可扩展、标准化的指令接口基础上，我们展示了在不牺牲通用处理能力的前提下，集成应用特定加速器的方法。基于ASCON轻量级加密算法的实验性验证在两种软核微控制器上成功实现了该方法。通过这一实现，我们展示了软件回退机制的使用，支持硬件与软件之间的无缝切换，并实现软件与硬件执行域之间的内存交接。结果表明，相较于静态硬件设计，该方法在执行时间与灵活性方面具有显著优势，同时通过硬件复用支持长期的可维护性与可持续性。"
  },
  {
    "date": "2025-12-9",
    "title": "MAGISTER: LLM-Based Test Generation with Role-Specialized Agents",
    "authors": "Abdellatif Ahammad, Manal El Bajta, Maryam Radgui",
    "publish": "2025 International Conference on Intelligent Systems: Theories and Applications (SITA)",
    "url": "https://doi.org/10.1109/sita67914.2025.11273637",
    "source": "IEEE",
    "abstract": "Automated test generation is one of the most critical topics in software testing, with numerous challenges due to the need for deep code understanding and the creation of meaningful assertions. Traditional approaches often generate low-quality and difficult-to-read tests that lack real code understanding and rely on statistical and dynamic analysis. With the recent advances in Large Language Models (LLMs), new opportunities emerge for generating unit tests that prioritize readability and context understanding. In this paper, we introduce MAGISTER, a multiagent framework for LLM-based unit test generation, where each agent (Analyzer Agent, Test Generation Agent, Executor Agent, and Refiner Agent) specializes in a specific role within the framework workflow, which involves analyzing the codebase to identify testable units and generating test code with feedbackdriven refinement. We evaluated MAGISTER on five open-source Python projects, demonstrating a significant improvement in code coverage for modular codebases compared to the original userwritten tests. However, it still has limitations when handling large and complex projects requiring domain-specific knowledge. Our results demonstrate the promising potential of LLM-driven and agent-based architectures in advancing test automation while highlighting directions for future improvement.",
    "title_zh": "MAGISTER：基于大语言模型的角色专业化代理测试生成",
    "abstract_zh": "自动化测试生成是软件测试领域中最为关键的主题之一，由于需要深入理解代码并生成有意义的断言，因此面临诸多挑战。传统方法往往生成质量较低、可读性差的测试用例，缺乏真正的代码理解能力，且依赖于统计和动态分析。随着大型语言模型（LLM）的最新进展，利用LLM生成注重可读性和上下文理解的单元测试成为可能。本文提出了一种名为MAGISTER的多智能体框架，用于基于LLM的单元测试生成。该框架中的每个智能体（分析智能体、测试生成智能体、执行智能体和优化智能体）在工作流程中各司其职，通过分析代码库以识别可测试单元，并在反馈驱动的迭代中生成测试代码。我们在五个开源Python项目上对MAGISTER进行了评估，结果表明，与原始用户编写的测试相比，MAGISTER在模块化代码库上的代码覆盖率有显著提升。然而，在处理需要领域专业知识的大型复杂项目时，仍存在一定的局限性。实验结果展示了基于LLM和多智能体架构在推动测试自动化方面的巨大潜力，同时也指出了未来改进的方向。"
  },
  {
    "date": "2025-12-9",
    "title": "LiteInjector: A LiteX Extension for Fault Injection",
    "authors": "Adam Henault, Philippe Tanguy, Vianney Lapôtre",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00067",
    "source": "IEEE",
    "abstract": "In this article, we focus on the emulation of fault injections on FPGA boards using saboteurs by presenting LiteInjector. LiteInjector is an open source bit- and cycle-accurate logic fault emulator written in Python, developed with the aim of accelerating security evaluation campaigns for systems-on-chip (SoC). LiteInjector has been tested on several use cases. These use cases rely on VerifyPin codes from the FISSC security benchmark running on a Linux-capable system-on-chip with a RISC-V core. We demonstrate that the tool provides a flexible solution that supports a large set of fault models. Furthermore, results show that, compared to an HDL simulation-based tool, LiteInjector allows reducing the fault injection campaign time by a factor of 210 considering a Linux-based system.",
    "title_zh": "LiteInjector：用于故障注入的LiteX扩展",
    "abstract_zh": "本文聚焦于利用“破坏者”（saboteurs）在FPGA板上模拟故障注入，提出了LiteInjector。LiteInjector是一款开源的、比特级和周期级精确的逻辑故障模拟器，采用Python编写，旨在加速片上系统（SoC）的安全评估工作。LiteInjector已在多个应用场景中进行了测试，这些场景基于运行在具备Linux功能的RISC-V核心SoC上的FISSC安全基准中的VerifyPin代码。实验结果表明，该工具提供了一种灵活的解决方案，支持广泛的故障模型。此外，与基于HDL仿真工具相比，LiteInjector可将基于Linux系统的故障注入测试周期缩短210倍。"
  },
  {
    "date": "2025-12-9",
    "title": "Comparative Evaluation of Reinforcement Learning and Evolutionary Algorithms for Effective Analog Circuit Sizing",
    "authors": "E. Papageorgiou, C. Vişan, A. Buzo, G. Pelz, H. Cucu, T. Noulis",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270536",
    "source": "IEEE",
    "abstract": "This work compares two modern optimization approaches for analog integrated circuit sizing: evolutionary algorithms (EAs) and reinforcement learning (RL). While EAs have demonstrated consistent performance with minimal configuration effort, RL offers a data-driven alternative that can learn adaptive strategies through interaction. Both methods are evaluated within a unified framework that accounts for multiple process, voltage, and temperature (PVT) corners. The comparison highlights differences in efficiency, convergence behavior, and generalization capability. Results aim to clarify the strengths and limitations of each approach and guide their applicability in automated analog design.",
    "title_zh": "强化学习与进化算法在模拟电路尺寸优化中的对比评估",
    "abstract_zh": "本文比较了两种现代优化方法在模拟集成电路尺寸设计中的应用：进化算法（EAs）和强化学习（RL）。尽管进化算法表现出稳定的性能且配置工作量小，但强化学习提供了一种数据驱动的替代方案，能够通过交互学习自适应策略。两种方法均在一个统一的框架下进行评估，该框架考虑了多种工艺、电压和温度（PVT）条件。对比结果突显了两者在效率、收敛行为以及泛化能力方面的差异。研究旨在阐明各自方法的优势与局限性，并指导其在自动化模拟设计中的适用性。"
  },
  {
    "date": "2025-12-9",
    "title": "RRTL: Red Teaming Reasoning Large Language Models in Tool Learning",
    "authors": "Yifei Liu, Yu Cui, Haibin Zhang",
    "publish": "IEEE Internet of Things Journal",
    "url": "https://doi.org/10.1109/jiot.2025.3642164",
    "source": "IEEE",
    "abstract": "While tool learning significantly enhances the capabilities of large language models (LLMs), it also introduces substantial security risks to real-world systems, such as the Internet of Things (IoT). Prior research has revealed various vulnerabilities in traditional LLMs during tool learning. However, the safety of newly emerging reasoning LLMs (RLLMs), such as DeepSeek-R1, in the context of tool learning remains underexplored. To bridge this gap, we propose RRTL, a red teaming approach specifically designed to evaluate RLLMs in tool learning. RRTL covers three critical components: (1) the identification of deceptive threats, aimed at evaluating RLLM’s behavior in concealing the usage of unsafe tools and their potential risks; (2) the use of Chain-of-Thought (CoT) prompting to force tool invocation for malicious queries; and (3) the investigation of tool-calling safety scenarios, which evaluates RLLM’s safety-related behaviors and identifies factors contributing to security risks. We conduct a comprehensive evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs can pose serious deceptive risks by frequently failing to disclose tool usage and to warn users of potential tool output risks; (2) CoT prompting reveals multilingual safety vulnerabilities in RLLMs; and (3) RLLMs exhibit significant variability in security behaviors across different tool invocation scenarios. Our work deepens the understanding of RLLM safety in tool learning and provides insights for developing secure and trustworthy IoT-enabled agents. The code and data are available at: https://github.com/liuyifeiaaa/RRTL.",
    "title_zh": "RRTL：在工具学习中对大型语言模型进行红队推理",
    "abstract_zh": "尽管工具学习显著提升了大型语言模型（LLMs）的能力，但也给现实世界系统（如物联网IoT）带来了重大的安全风险。以往的研究揭示了传统LLMs在工具学习过程中存在多种漏洞，然而，新兴的推理型大语言模型（RLLMs，如DeepSeek-R1）在工具学习背景下的安全性仍缺乏深入探索。为填补这一空白，我们提出了RRTL——一种专为评估RLLMs在工具学习中表现而设计的红队测试方法。RRTL涵盖三个核心组成部分：(1) 识别欺骗性威胁，旨在评估RLLM在隐藏不安全工具使用及其潜在风险方面的行为；(2) 利用思维链（Chain-of-Thought, CoT）提示技术，强制触发恶意查询对应的工具调用；(3) 探究工具调用的安全场景，评估RLLM在安全相关行为上的表现，并识别导致安全风险的关键因素。\n\n我们在七种主流RLLMs上进行了全面评估，发现了三个关键结论：(1) RLLMs可能带来严重的欺骗性风险，表现为频繁隐瞒工具使用情况，且未向用户警示工具输出可能带来的风险；(2) CoT提示暴露了RLLMs在多语言环境下的安全漏洞；(3) RLLMs在不同工具调用场景下表现出显著的安全行为差异。\n\n本研究深化了对RLLM在工具学习中安全性的理解，为构建安全可信的物联网赋能智能体提供了重要参考。代码与数据已公开，获取地址为：https://github.com/liuyifeiaaa/RRTL。"
  },
  {
    "date": "2025-12-9",
    "title": "Nail: Not Another Fault-Injection Framework for Chisel-generated RTL",
    "authors": "Robin Sehm, Christian Ewert, Rainer Buchty, Mladen Berekovic, Saleh Mulhem",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00013",
    "source": "IEEE",
    "abstract": "Fault simulation and emulation are essential techniques for evaluating the dependability of integrated circuits, enabling early-stage vulnerability analysis and supporting the implementation of effective mitigation strategies. High-level hardware description languages such as Chisel facilitate the rapid development of complex fault scenarios with minimal modification to the design. However, existing Chisel-based fault injection (FI) frameworks are limited by their coarse-grained, instruction-level controllability, which restricts the precision of fault modeling. This work introduces Nail, a Chisel-based open-source FI framework that overcomes these limitations by introducing statebased faults. This approach allows fault scenarios based on specific system states instead of just instruction-level triggers, removing the need for precise timing of fault activation. For greater controllability, Nail allows users to arbitrarily modify internal trigger states via software at runtime. To support this, Nail automatically generates a software interface, offering straightforward access to the instrumented design. This enables fine-tuning of fault parameters during active fault-injection campaigns, a feature particularly beneficial for FPGA emulation, where synthesis is time-consuming. Utilizing these features, Nail narrows the gap between the high speed of emulation-based FI frameworks, the usability of software-based approaches, and the controllability achieved in simulation. We demonstrate Nail’s state-based fault injection and software framework by modeling a faulty general-purpose register in a RISC-V processor. Although this might appear straightforward, it requires statedependent fault injection and was previously impossible without fundamental changes to the design. The approach was validated in both simulation and FPGA emulation, where the addition of Nail introduced less than $1 \\%$ resource overhead.",
    "title_zh": "Nail：一个针对Chisel生成的RTL的全新故障注入框架",
    "abstract_zh": "故障仿真与模拟是评估集成电路可靠性的重要技术，能够实现早期的漏洞分析，并支持有效缓解策略的实施。高级硬件描述语言（如Chisel）使得在设计改动极少的情况下，快速构建复杂的故障场景成为可能。然而，现有的基于Chisel的故障注入（FI）框架受限于粗粒度、指令级的控制能力，难以实现精确的故障建模。本文提出Nail——一个基于Chisel的开源故障注入框架，通过引入基于状态的故障机制，克服了上述局限性。该方法允许根据特定系统状态来触发故障场景，而不再局限于仅依赖指令级触发，从而无需精确控制故障激活的时间。为了进一步提升可控性，Nail支持用户通过软件在运行时任意修改内部触发状态。为此，Nail会自动生成软件接口，使用户能够便捷地访问被仪器化的设计，从而在实际故障注入过程中精细调节故障参数。这一特性对于FPGA仿真尤其有益，因为FPGA综合过程耗时较长。借助这些功能，Nail弥合了基于仿真的故障注入框架的高速优势、基于软件方法的易用性，以及仿真中所达到的高可控性之间的差距。我们通过在RISC-V处理器中建模一个带故障的一般用途寄存器，展示了Nail的状态驱动故障注入及其软件框架的能力。尽管该案例看似简单，但其实际实现需要依赖状态相关的故障注入，此前若不改变设计架构则无法完成。该方法已在仿真和FPGA仿真环境中得到验证，结果显示引入Nail所带来的资源开销低于1%。"
  },
  {
    "date": "2025-12-9",
    "title": "Impact of Contention-Aware Placement in Heterogeneous Edge Devices",
    "authors": "Jeremy Giesen, Ibai Irigoyen, Enrico Mezzetti, Jaume Abella, Francisco J. Cazorla",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00041",
    "source": "IEEE",
    "abstract": "Time predictability is an increasing concern in functionally-rich mixed-criticality applications at the Edge, which often carry different timing requirements. Edge devices, in turn, are increasingly complex to sustain the increasing computational requirements, which hinders providing predictable performance without seriously affecting performance. One of the main threats to predictable performance is the impact of timing interference arising from contention in an increasing number of shared hardware resources. The impact of software to hardware mapping on performance is a well-studied topic, seeking optimal memory mappings to reduce average and worst-case performance, and, more recently, to control and limit timing interference. These methods normally focus on code and data placement, especially in relation to specific properties of the memory hierarchy, either architectural (e.g. heterogeneous memory modules) or obtained through partitioning techniques. In practice, however, these works build on a uniform memory hierarchy model, where the source of a memory request, namely, where a task accessing a given memory is eventually executed, is not directly relevant. In this work, we consider a large class of systems (e.g., TriCore families) where memory hierarchies are non-uniform, and access latency depends on the computing element issuing the request. In those architectures, the impact of code and data placement on timing interference cannot be addressed without considering architectural constraints and task locality. Through empirical exploration, we show that code, data, and locality collectively have a substantial impact on contention bounds, leading to a significantly expanded optimization space compared to approaches considering only code and data placement under the uniform memory assumption. Our results motivate the need for novel, efficient optimization approaches that integrate task mapping and architectural constraints to reduce timing interference.",
    "title_zh": "异构边缘设备中考虑争用的部署策略影响",
    "abstract_zh": "时间可预测性在功能丰富的边缘混合关键性应用中日益成为关注焦点，这类应用通常具有不同的时序需求。与此同时，边缘设备的复杂性也在不断提升，以满足日益增长的计算需求，但这却给提供可预测性能带来了挑战，且若不严重影响性能则难以实现。其中，影响可预测性能的主要威胁之一是由于共享硬件资源数量增加所引发的争用导致的时间干扰。软件到硬件映射对性能的影响是一个被广泛研究的课题，其目标是通过优化内存映射来降低平均和最坏情况下的性能开销，近年来更进一步致力于控制和限制时间干扰。这些方法通常聚焦于代码与数据的布局，尤其是与内存层次结构的特定属性相关，无论是架构层面的（如异构内存模块）还是通过分区技术获得的。然而，在实践中，这些研究大多基于统一的内存层次结构模型，忽略了内存请求源——即访问特定内存的任务最终在哪个计算单元上执行——这一因素的重要性。在本研究中，我们考虑了一类广泛的系统（例如TriCore系列），其内存层次结构是非均匀的，访问延迟取决于发出请求的计算单元。在这些架构中，若不考虑架构约束和任务局部性，就无法有效解决代码与数据布局对时间干扰的影响。通过实证分析，我们发现代码、数据以及局部性三者共同对竞争边界产生显著影响，相较于仅在统一内存假设下考虑代码与数据布局的方法，其优化空间得到了显著扩展。我们的研究结果凸显了开发新型、高效优化方法的必要性，这些方法需将任务映射与架构约束相结合，以有效减少时间干扰。"
  },
  {
    "date": "2025-12-9",
    "title": "Leveraging Design Static Analysis for Vertical Reuse in Functional Verification",
    "authors": "Petr Bardonek, Marcela Zachariášová",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00068",
    "source": "IEEE",
    "abstract": "The Portable Test and Stimulus Standard (PSS) is an emerging standard enabling higher abstraction for simulation-based verification through graph-based stimulus generation, promoting modular reuse. However, achieving vertical reuse—integrating block-level PSS models into top-level ones—remains a significant challenge due to the manual effort required. This paper introduces static analysis as an essential phase in automating vertical reuse. It applies data and control flow analyses combined with a Satisfiability Modulo Theories solver to trace signal paths from the top-level design to its submodules. Experimental validation demonstrates the applicability of the approach on the execution stage of a RISC-V processor. Its scalability and efficiency are further evaluated using a configurable benchmark with varying design sizes and hierarchy levels. The findings suggest that the static analysis provides sufficient information about interconnections and dependencies in the top-level design to determine connections necessary for automating the vertical reuse of PSS models.",
    "title_zh": "利用设计静态分析实现功能验证中的垂直复用",
    "abstract_zh": "便携式测试与激励标准（PSS）是一项新兴标准，通过基于图的激励生成，实现了仿真验证的更高抽象层次，促进了模块化复用。然而，由于需要大量手动操作，实现垂直复用——即将底层模块级的PSS模型集成到顶层模型中——仍面临重大挑战。本文提出将静态分析作为自动化垂直复用的关键步骤，结合数据流和控制流分析，并利用满足可判定理论（SMT）求解器，追踪从顶层设计到其子模块的信号路径。实验验证表明，该方法在RISC-V处理器执行阶段具有实际应用价值。通过一个可配置的基准测试，针对不同设计规模和层级结构进一步评估了该方法的可扩展性和效率。研究结果表明，静态分析能够提供足够的顶层设计中互连关系与依赖信息，从而确定实现PSS模型垂直复用所必需的连接关系。"
  },
  {
    "date": "2025-12-9",
    "title": "Leveraging Large Language Models for Operational Specification Mining in Industrial Control Systems",
    "authors": "Zhiwen Pan, Kaige Zhang, Likai Sun, Zhicheng Zhang, Dongliang Fang, Limin Sun",
    "publish": "2025 International Conference on Networking and Network Applications (NaNA)",
    "url": "https://doi.org/10.1109/nana66698.2025.00014",
    "source": "IEEE",
    "abstract": "As a cyber-physical system, Industrial Control Systems (ICS) involve operators strictly following operational specifications and controllers executing automated control programs. This makes anomaly based detection techniques particularly suitable for intrusion detection in ICS. However, operational specifications, which contain a large amount of domain-specific knowledge in natural language text, suffer from challenges such as difficulties in knowledge extraction, ambiguity, and incomplete information. Meanwhile, extracting control behavior rules from operational specifications often relies on manual extraction, leading to low accuracy and efficiency. This paper proposes an automated rule extraction method based on Large Language Models (LLMs). We construct a domain-specific knowledge base to systematically improve terminology precision in prompt engineering. Building on this foundation, the framework employs Chain of Thought (CoT) techniques to guide the LLM through hierarchical rule extraction: initially recognizing single devices, then building system topology, and finally extracting control behavior rules. The proposed method is validated through its application to the thermal power plant simulation platform, demonstrating its effectiveness in extracting control behavior rules from operational specifications.",
    "title_zh": "利用大语言模型在工业控制系统中进行运行规范挖掘",
    "abstract_zh": "作为信息物理系统，工业控制系统（ICS）涉及操作员严格遵循操作规范，以及控制器执行自动化控制程序。这使得基于异常的检测技术特别适用于ICS中的入侵检测。然而，操作规范中包含大量以自然语言文本形式呈现的领域专业知识，存在知识提取困难、语义模糊以及信息不完整等挑战。同时，从操作规范中提取控制行为规则通常依赖人工完成，导致准确率和效率较低。本文提出一种基于大语言模型（LLM）的自动化规则提取方法。我们构建了一个领域专用的知识库，以系统性地提升提示工程中的术语精确度。在此基础上，该框架采用思维链（Chain of Thought, CoT）技术，引导大语言模型分层进行规则提取：首先识别单个设备，继而构建系统拓扑结构，最终提取控制行为规则。通过在热电厂仿真平台上的应用验证，该方法在从操作规范中提取控制行为规则方面表现出良好的有效性。"
  },
  {
    "date": "2025-12-9",
    "title": "Bridging Formal and Dynamic Verification: A Unified Methodology for Design Verification",
    "authors": "Kareem Waseem Elsaid, Magdy Ahmed Abbas, Ahmed Hassan Abdelmonem, Philopateer Awny Abdullah, Anton Emad Saber",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270540",
    "source": "IEEE",
    "abstract": "As system-on-chip (SoC) designs increase in complexity, a robust, scalable, and effective verification process is essential. Simulation-based verification is the industry standard to verify the functionality of a design, but it lacks exhaustive coverage. On the other hand, formal verification provides exhaustive verification, but with scalability and convergence limitations. To overcome these limitations, this paper presents a unified hybrid methodology that combines the advantages of both. A block classification framework is proposed to allocate formal or simulation techniques based on design complexity and criticality, embedding formal tools within simulation environments. By using Synopsys VC Formal, the proposed methodology achieved coverage closure time reduction by more than 50%, eliminated recurring top-level connectivity bugs, and improved bug detection latency by 47% on average compared to a simulation-only approach.",
    "title_zh": "连接形式化与动态验证：一种统一的设计验证方法论",
    "abstract_zh": "随着片上系统（SoC）设计复杂度的不断提升，建立一个稳健、可扩展且高效的验证流程变得至关重要。基于仿真的验证是行业标准，用于验证设计的功能正确性，但其覆盖范围并不全面。相比之下，形式化验证能够实现全面的验证，却存在可扩展性和收敛性方面的局限。为克服这些限制，本文提出了一种统一的混合验证方法，融合了两种技术的优势。文中提出了一种模块分类框架，根据设计的复杂度和关键性，智能分配使用形式化验证或仿真技术，并将形式化工具嵌入到仿真环境中。通过采用Synopsys VC Formal工具，所提出的验证方法在覆盖率收敛时间上实现了超过50%的缩短，彻底消除了反复出现的顶层连接错误，并相较于纯仿真方法，平均将缺陷检测延迟降低了47%。"
  },
  {
    "date": "2025-12-9",
    "title": "Hardware-Level Adaptive Scheduling for Reconfigurable Accelerators on Virtualized FPGAs",
    "authors": "Lu Jiang, Zuwen Ou, Diana Göhringer",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00083",
    "source": "IEEE",
    "abstract": "Robot systems are facing a growing demand for realtime performance of multiple computing tasks and autonomous decision-making. However, limited computing resources make efficient scheduling of computing tasks and optimizing resource utilization a key challenge. Virtualization of field-programmable gate arrays enables the dynamic sharing of hardware resources, while dynamic partial reconfiguration allows for flexible adjustment of hardware accelerators based on task requirements, making it suitable for complex and fluctuating computing loads. However, virtualization scheduling based on operating systems or microkernels usually introduces high software overhead. This paper proposes a hardware-level adaptive scheduling mechanism integrated with spatiotemporal scheduling to efficiently share reconfigurable computing resources among multiple users. This mechanism is transparent to users and can adaptively adjust the spatiotemporal allocation strategy according to the characteristics of tasks. In addition, the scheduling scheme introduces access rights management to strictly prevent unauthorized resource access. Hardware scheduling system achieves a speedup of over $1000 \\times$ compared to a software-based scheduling system when executing 100 hardware tasks, reducing execution time from seconds to milliseconds.",
    "title_zh": "虚拟FPGA上可重构加速器的硬件级自适应调度",
    "abstract_zh": "机器人系统正面临多重计算任务实时性能及自主决策日益增长的需求。然而，有限的计算资源使得计算任务的高效调度与资源利用率优化成为关键挑战。现场可编程门阵列（FPGA）的虚拟化技术实现了硬件资源的动态共享，而动态部分重配置则可根据任务需求灵活调整硬件加速器，因而适用于复杂且波动频繁的计算负载。然而，基于操作系统或微内核的虚拟化调度通常会引入较高的软件开销。本文提出一种集成时空调度的硬件级自适应调度机制，能够高效地在多个用户间共享可重构计算资源。该机制对用户透明，并可根据任务特征自适应调整时空分配策略。此外，调度方案引入了访问权限管理，严格防止未经授权的资源访问。实验结果表明，与基于软件的调度系统相比，该硬件调度系统在执行100个硬件任务时，性能提升超过1000倍，将执行时间从秒级降低至毫秒级。"
  },
  {
    "date": "2025-12-9",
    "title": "Are Static Analysis Tools Still Working during the Evolution of Smart Contracts? A Comprehensive Empirical Study",
    "authors": "Cuifeng Gao, Ao Chen, Chengze Wu, Wenzhang Yang, Jiaming Ye, Yinxing Xue",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3779429",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "智能合约演进过程中，静态分析工具是否依然有效？一项全面的实证研究",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-9",
    "title": "Efficient Fuzzing Seed Generation Method Based on Diffusion Generation Model",
    "authors": "Yanning Du, Xianglong Li, Xinhong Hei, Yichuan Wang, Xin Song",
    "publish": "2025 International Conference on Networking and Network Applications (NaNA)",
    "url": "https://doi.org/10.1109/nana66698.2025.00081",
    "source": "IEEE",
    "abstract": "Application vulnerabilities pose a major threat to software security, hence the need for effective and efficient vulnerability detection techniques. Fuzzing has become a popular method for detecting applications by generating many random inputs. However, the effectiveness of fuzzing largely depends on the quality of the seeds used to guide the fuzzing process. Existing seed selection strategies often have limitations in terms of efficiency and ability to explore deep paths and complex program logic. To address these challenges, this paper proposes a seed generation method using a diffusion generation model, which consists of collecting high-quality training datasets using parallel fuzzing, feeding them into a diffusion generation model after data preprocessing to learn the characteristics of the data distributions, and generating diverse seeds. The experiments are fuzzing using the newly generated seeds, and the results are improved in the three metrics of branch coverage, path triggering, and crash triggering, proving the effectiveness of the method.",
    "title_zh": "基于扩散生成模型的高效模糊测试种子生成方法",
    "abstract_zh": "应用漏洞对软件安全构成重大威胁，因此需要高效且有效的漏洞检测技术。模糊测试（Fuzzing）已成为一种流行的漏洞检测方法，通过生成大量随机输入来探测应用程序中的缺陷。然而，模糊测试的效果在很大程度上取决于用于引导测试过程的种子质量。现有的种子选择策略在效率以及探索深层路径和复杂程序逻辑方面存在局限性。为解决这些问题，本文提出了一种基于扩散生成模型的种子生成方法：首先通过并行模糊测试收集高质量的训练数据集，经过数据预处理后输入到扩散生成模型中，学习数据分布特征，进而生成多样化的种子。实验采用新生成的种子进行模糊测试，结果在分支覆盖率、路径触发率和崩溃触发率三个指标上均得到显著提升，验证了该方法的有效性。"
  },
  {
    "date": "2025-12-9",
    "title": "Execution Platform Contracts",
    "authors": "Dorian Bourgeoisat, Ulrich Kühne, Florian Brandner",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00065",
    "source": "IEEE",
    "abstract": "Confidentiality is a crucial security property for many critical applications. As a response to the discovery of numerous micro-architectural side channel attacks such as Spectre, allowing an attacker to extract secret information in pernicious ways, the notion of hardware/software contracts was proposed to formalise the guarantees provided by the hardware to the software. In this paper, we propose to extend this notion to include the guarantees provided by the operating system (OS), so far unspecified in such contracts. We formalize an attacker model adapted to a typical execution model on a shared platform. More precisely, we formalize common thread and memory management policies provided by the OS on top of a hardware model and explore the consequences of potential leaks emerging on such a platform. Our investigation shows that the OS policies play a crucial role in providing security guarantees to code processing sensitive data and thus have to be taken into consideration when writing such code through platform contracts.",
    "title_zh": "执行平台合同",
    "abstract_zh": "机密性是许多关键应用中至关重要的安全属性。针对Spectre等众多微架构侧信道攻击的发现，这些攻击使攻击者能够以有害方式提取秘密信息，硬件/软件契约的概念被提出，用以形式化硬件向软件提供的保证。在本文中，我们提出将这一概念扩展至包含操作系统（OS）所提供的保证，而这些保证在以往的契约中尚未明确说明。我们针对共享平台上的典型执行模型，形式化了一种适配的攻击者模型。具体而言，我们在硬件模型的基础上，形式化了操作系统常见的线程和内存管理策略，并探讨了此类平台上可能出现的信息泄露所导致的后果。我们的研究结果表明，操作系统策略在为处理敏感数据的代码提供安全保证方面起着至关重要的作用，因此在编写此类代码时，必须通过平台契约充分考虑操作系统的相关策略。"
  },
  {
    "date": "2025-12-9",
    "title": "CAHLS: Source-to-Source Transformation to Generate Cycle Accurate Models for High-Level Synthesis",
    "authors": "Yuhan She, Yanlong Huang, Jierui Liu, Ray Cheung, Hong Yan",
    "publish": "Proceedings of the International Conference on Hardware/Software Codesign and System Synthesis",
    "url": "https://doi.org/10.1145/3742873.3755984",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "CAHLS：一种用于高层次综合的源到源转换方法，以生成周期精确的模型",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-9",
    "title": "CORA-OpAmp: A Compact Open-Source Reinforcement-Learning Approach for Operational Amplifier Optimization in IHP SG13G2",
    "authors": "Maximilian Scherzer, Mario Auer",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270673",
    "source": "IEEE",
    "abstract": "Automated optimization of operational amplifiers is a complex task that demands efficient methods. This paper presents a compact, open-source reinforcement learning approach for op-amp optimization within the IHP SG13G2 process. Our approach adopts and compares the TD3 and SAC algorithms against DDPG, showcasing more reliable learning and faster progression. Using a compact MLP rather than a GNN for the agent reduces complexity and computing effort, while still achieving competitive results. The proposed framework makes use of the g<inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">m</inf>/I<inf xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">d</inf> design method as the primary state input for the RL agent, thereby effectively embedding well-known analog design principles to improve guidance. We also apply online normalization to the agent inputs, thus enhancing stability and significantly reducing the initial random search phase. Reward functions are specifically designed to address op-amp performance metrics. Findings from an experimental folded-cascode optimization study demonstrate that, compared to DDPG-based methods, convergence is rapid and learning rates are stable. Compared to prior GNN-based RL approaches, our lightweight framework offers improved usability, reproducibility, and accessibility as an open-source tool.",
    "title_zh": "CORA-OpAmp：一种用于IHP SG13G2工艺中运算放大器优化的紧凑型开源强化学习方法",
    "abstract_zh": "运算放大器的自动化优化是一项复杂任务，需要高效的方法。本文提出了一种紧凑且开源的强化学习方法，用于在IHP SG13G2工艺下对运算放大器进行优化。我们的方法对比了TD3和SAC算法与DDPG算法的表现，结果表明前两者具有更可靠的训练过程和更快的收敛速度。通过采用简洁的多层感知机（MLP）而非图神经网络（GNN）作为智能体，显著降低了模型复杂度和计算开销，同时仍取得了具有竞争力的优化效果。所提出的框架以gm/Id设计方法作为强化学习智能体的主要状态输入，有效融入了经典的模拟电路设计原则，从而提升了优化引导能力。此外，我们对智能体输入采用了在线归一化处理，进一步增强了训练稳定性，并大幅缩短了初始随机搜索阶段。针对运算放大器性能指标，我们专门设计了奖励函数。实验结果基于折叠式共源共栅结构的优化研究显示，相较于基于DDPG的方法，本方法实现了快速收敛和稳定的训练速率。与以往基于GNN的强化学习方法相比，本轻量级框架在可用性、可复现性以及作为开源工具的可访问性方面均表现出显著优势。"
  },
  {
    "date": "2025-12-9",
    "title": "Using Automatic Code Generation to Streamline the Real-Time FPGA Deployment of DNNs at the Edge",
    "authors": "João Rodrigo Faria, Fábio D. L. Coutinho, Arnaldo S. R. Oliveira",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270598",
    "source": "IEEE",
    "abstract": "This paper explores the automatic code generation of Deep Neural Networks (DNNs) for accelerating the real-time deployment of DNNs on Field-Programmable Gate Arrays (FPGAs) at the network edge. Using a language-based High-Level Synthesis (HLS) tool, we present a generator that produces HLS-compatible source code for user-defined DNN architectures. The tool enables rapid prototyping and deployment of machine learning-based physical layer functions in communication systems. It supports diverse DNN configurations, including fully connected, convolutional, and pooling layers, as well as various activation functions, providing flexibility for edge applications. By releasing the generator as open source, we aim to lower the entry barrier for DNN development and promote reproducibility in the design of hardware-accelerated neural networks.",
    "title_zh": "利用自动代码生成加速边缘设备上DNN的实时FPGA部署",
    "abstract_zh": "本文探讨了深度神经网络（DNN）在边缘计算场景下，针对现场可编程门阵列（FPGA）实现实时部署的自动代码生成技术。通过使用基于语言的高层次综合（HLS）工具，我们提出了一种生成器，能够为用户自定义的DNN架构生成符合HLS要求的源代码。该工具支持通信系统中基于机器学习的物理层功能的快速原型设计与部署，兼容多种DNN配置，包括全连接层、卷积层、池化层以及各类激活函数，为边缘应用提供了高度灵活性。通过将该生成器开源发布，我们旨在降低DNN开发的入门门槛，并推动硬件加速神经网络设计的可复现性。"
  },
  {
    "date": "2025-12-9",
    "title": "Modular 1 Gbps Hardware TCP/IP FPGA Readout for MPGD Applications with APV25",
    "authors": "Fatima Bzeih, Chiara Micheli, Paolo Musico, Evaristo Cisbani",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270496",
    "source": "IEEE",
    "abstract": "The APV25 front-end chip is widely used in Micro Pattern Gaseous Detectors (MPGDs), including Gas Electron Multiplier (GEM) systems, due to its robust performance and integration flexibility. Traditional readout architectures based on the VME bus impose limitations in bandwidth, scalability, and compatibility with modern data acquisition frameworks. This work presents a fully hardware-integrated 1 Gbps Ethernet-based DAQ system implemented on an Xilinx Kintex-7 FPGA. The design replaces the VME back-end with a memory-mapped TCP/IP interface and an optional streaming mode, enabling deterministic, real-time data transmission without embedded processors. The system was validated using real detector signals from a µRWELL chamber, with successful capture of cosmic events and multi-channel charge readout. Benchmarking confirms reliable multi-word transactions over TCP and a sustained streaming throughput of up to 100 MB/s using generated data. The complete architecture, including deserialization, event building, histogramming, and TCP interface, fits comfortably within available FPGA resources and provides a scalable upgrade path toward 10 Gbps Ethernet for high-luminosity experiments.",
    "title_zh": "用于MPGD应用的基于APV25的1 Gbps硬件TCP/IP FPGA读出模块",
    "abstract_zh": "APV25前端芯片因其出色的性能和集成灵活性，被广泛应用于微结构气态探测器（MPGDs），包括气体电子倍增器（GEM）系统。传统的基于VME总线的读出架构在带宽、可扩展性以及与现代数据采集框架的兼容性方面存在局限。本文提出了一种完全由硬件实现的1 Gbps以太网数据采集（DAQ）系统，该系统基于Xilinx Kintex-7 FPGA构建。设计中用内存映射的TCP/IP接口及可选的流式传输模式替代了原有的VME后端，实现了无需嵌入式处理器即可进行确定性、实时的数据传输。系统通过μRWELL探测腔产生的真实探测信号进行了验证，成功捕获了宇宙射线事件并完成了多通道电荷读出。基准测试结果表明，系统可在TCP协议下可靠地完成多字节事务操作，并在使用生成数据时实现高达100 MB/s的持续流式吞吐量。整个架构，包括解串行化、事件构建、直方图统计以及TCP接口功能，均能良好地适配FPGA的可用资源，同时为未来向10 Gbps以太网升级提供了可扩展路径，适用于高亮度实验需求。"
  },
  {
    "date": "2025-12-9",
    "title": "Unraveling Parallelism in Automated Workload Modeling for Distributed Cyber-Physical Systems",
    "authors": "Faezeh Sadat Saadatmand, Todor Stefanov, Andy D. Pimentel, Benny Akesson, Ignacio Gonález Alonso",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00016",
    "source": "IEEE",
    "abstract": "Designing next generation distributed CyberPhysical Systems (dCPS) requires effective Design Space Exploration (DSE) methods to evaluate system design alternatives and their impact on performance. While existing DSE approaches focus on hardware optimization and software-to-hardware mapping, they often overlook parallel execution opportunities within software tasks. Current application workload models for complex dCPS assume fixed execution orders, limiting the ability to explore and exploit software parallelism. To address this issue, we propose refined workload models derived from execution traces that capture both inter- and intra-process dependencies. Building on these models, we present a method to identify tasks that can be safely reordered or executed in parallel without modifying the existing software implementation. We validate our approach through a case study on the ASML Twinscan lithography machine, demonstrating measurable performance improvements without impacting the system functional correctness.",
    "title_zh": "分布式网络物理系统自动化工作负载建模中的并行性解析",
    "abstract_zh": "设计下一代分布式网络物理系统（dCPS）需要有效的设计空间探索（DSE）方法，以评估系统设计方案及其对性能的影响。尽管现有的DSE方法主要关注硬件优化和软件到硬件的映射，但往往忽略了软件任务内部的并行执行机会。当前复杂dCPS的应用工作负载模型通常假设任务执行顺序固定，这限制了对软件并行性的探索与利用。为解决这一问题，我们提出了一种基于执行轨迹构建的精细化工作负载模型，能够捕捉进程间及进程内依赖关系。在此基础上，我们提出一种方法，用于识别可在不修改现有软件实现的前提下安全重排或并行执行的任务。通过在ASML Twinscan光刻机上的案例研究验证了该方法的有效性，结果表明在不影响系统功能正确性的前提下，实现了可量化的性能提升。"
  },
  {
    "date": "2025-12-9",
    "title": "Application of Artificial Intelligence for Enhancing Travel Preparedness",
    "authors": "Arpa Pramualsap, Poohridate Arpasat, Kwanchai Kungcharoen, Wichian Premchaiswadi",
    "publish": "2025 23rd International Conference on ICT and Knowledge Engineering (ICT&amp;amp;KE)",
    "url": "https://doi.org/10.1109/ictke67052.2025.11274438",
    "source": "IEEE",
    "abstract": "This research presents an iOS-based luggage management system designed to mitigate the risks of forgetting essential items and packing prohibited articles for international travel. The system architecture features a Flutter-based frontend, a FastAPI backend, and utilizes Firebase for data management and authentication. The prototype leverages two core Artificial Intelligence (AI) components: the Ultralytics YOLOv8 model for multi-object detection to verify luggage contents against aviation regulations, and OpenAI's GPT-5.0 to generate personalized travel checklists. The system's performance evaluation, conducted on a test dataset, demonstrated the YOLOv8 model's practical viability with a mean Average Precision (mAP@50) of 50% and an F1-score of 50%. These findings confirm that the application can effectively enhance the accuracy and confidence of users' travel preparations, highlighting the potential of AI in streamlining complex travel logistics.",
    "title_zh": "人工智能在提升旅行准备方面的应用",
    "abstract_zh": "本研究提出了一种基于iOS的行李管理系统，旨在降低国际旅行中遗忘重要物品或误带违禁品的风险。该系统采用Flutter构建前端界面，FastAPI作为后端服务，并利用Firebase进行数据管理与用户认证。原型系统集成了两项核心人工智能（AI）技术：使用Ultralytics YOLOv8模型实现多物体检测，以核对行李内容是否符合航空安全规定；以及借助OpenAI的GPT-5.0生成个性化的旅行清单。在测试数据集上的性能评估表明，YOLOv8模型具备实际应用可行性，其平均精度（mAP@50）达到50%，F1分数也为50%。研究结果证实，该应用能够有效提升用户出行准备的准确性和信心，凸显了人工智能在简化复杂旅行流程方面的巨大潜力。"
  },
  {
    "date": "2025-12-9",
    "title": "OpenFPGA-NoC: Automated Fabric and Bitstream Generation for NoC-based FPGAs",
    "authors": "Ruthwik Reddy Sunketa, Muhammad Ali Farooq, Ganesh Gore, Allen Boston, Pierre-Emmanuel Gaillardon, Aman Arora",
    "publish": "ACM Transactions on Reconfigurable Technology and Systems",
    "url": "https://doi.org/10.1145/3779449",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "OpenFPGA-NoC：面向NoC架构FPGA的自动化结构与比特流生成",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-9",
    "title": "Tutorial: CEDR: A Holistic Software and Hardware Design Environment for Hardware Agnostic Application Development and Deployment on FPGA-Integrated Heterogeneous Systems",
    "authors": "Serhan Gener, Sahil Hassan, Ali Akoglu",
    "publish": "Proceedings of the International Conference on Hardware/Software Codesign and System Synthesis",
    "url": "https://doi.org/10.1145/3742873.3758335",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "教程：CEDR：一种面向FPGA集成异构系统上硬件无关应用程序开发与部署的综合性软硬件设计环境",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-9",
    "title": "Target-Aware Automated RTL Generation for RISC-V SoC Designs: A Model-Driven Approach",
    "authors": "Mohamed Badawy, Jiulong Wang, Paritosh Kumar Sinha, Wolfgang Ecker",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270631",
    "source": "IEEE",
    "abstract": "Register-Transfer Level (RTL) generators streamline hardware design but often produce generic designs that fail to leverage platform-specific resources, resulting in suboptimal performance or non-deployable implementations. This paper introduces a novel model-driven methodology that enhances RTL generation by seamlessly adapting to the target platform’s unique capabilities. By incorporating platform-specific constraints, our approach automates the mapping of RTL constructs to optimized resources, overcoming the limitations of platform-agnostic designs. We validate the methodology through the implementation of a RISC-V-based System-on-Chip (SoC) on an AMD Zynq UltraScale+ MPSoC board. Experimental results demonstrate remarkable improvements, reducing lookup table (LUT) usage by up to 97%, flip-flop (FF) usage by up to 99%, achieving over 3× higher maximum frequency, lowering power consumption by approximately 10%, and reducing design implementation run time by up to 94% compared to platform-agnostic designs.",
    "title_zh": "面向目标的RISC-V SoC设计自动化RTL生成：一种模型驱动的方法",
    "abstract_zh": "寄存器传输级（RTL）生成工具虽能简化硬件设计流程，但通常生成的方案较为通用，难以充分利用特定平台的资源，导致性能不佳或无法部署。本文提出一种新型的模型驱动方法，通过无缝适配目标平台的独特能力，显著提升RTL生成效率。该方法引入平台相关的约束条件，自动将RTL结构映射到优化后的硬件资源，克服了传统无平台依赖设计的局限性。我们通过在AMD Zynq UltraScale+ MPSoC开发板上实现基于RISC-V的片上系统（SoC），验证了该方法的有效性。实验结果表明，与无平台依赖的设计相比，本方法实现了显著改进：查找表（LUT）使用量减少高达97%，触发器（FF）使用量减少高达99%，最大工作频率提升超过3倍，功耗降低约10%，设计实现时间缩短高达94%。"
  },
  {
    "date": "2025-12-9",
    "title": "Tutorial: Hardware-Aware Compilation and Simulation for In-Memory Computing",
    "authors": "Asif Ali Khan, Hadjer Benmeziane, Hamid Farzaneh, Joao Lima, William Simon, Yiyu Shi, Zheyu Yan, Abu Sebastian, X. Sharon Hu, Jeronimo Castrillon, Corey Lammie",
    "publish": "Proceedings of the International Conference on Compilers, Architecture, and Synthesis for Embedded Systems",
    "url": "https://doi.org/10.1145/3742872.3758333",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "教程：面向内存计算的硬件感知编译与仿真",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-9",
    "title": "Modeling and Scheduling of Composable Instruction Set",
    "authors": "Yu Yang, Paul Delestrac, Ahmed Hemani",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00015",
    "source": "IEEE",
    "abstract": "State-of-the-art hardware accelerators with custom instruction set architectures (ISAs) are widely used in AI/ML applications. Despite the outstanding progress of modern accelerators, their performance potential is limited by their ISA. Recently, research around composable instruction sets (CIS) has emerged with the aim of improving the efficiency of modern accelerator ISAs. However, while CIS significantly outperforms existing ISAs with near-optimal PE utilization, no existing instruction scheduling approach supports the temporal composability requirements to correctly schedule CIS programs. The goal of this work is to propose a scalable scheduling algorithm that can meet CIS requirements. We propose a novel timing model structure built upon five fundamental concepts - operations, events, transformations, anchors, and constraints - that accurately capture CIS timing behavior and constraints. Using this model, we automatically formulate scheduling problems that can be solved using constraint programming (CP) solvers. In addition, we propose a methodology to synchronize scheduled instructions and generate functional CIS assembly code. Finally, we design experiments to study the scalability and quality of our scheduling approach. Across multiple dimensions of complexity, our approach scales linearly and produces near-optimal scheduling solutions.",
    "title_zh": "可组合指令集的建模与调度",
    "abstract_zh": "采用定制指令集架构（ISA）的先进硬件加速器在人工智能/机器学习（AI/ML）应用中得到广泛应用。尽管现代加速器取得了显著进展，但其性能潜力仍受限于其指令集架构。近年来，可组合指令集（CIS）的研究逐渐兴起，旨在提升现代加速器ISA的效率。然而，尽管CIS在接近最优处理单元（PE）利用率方面显著优于现有ISA，目前尚无任何现有的指令调度方法能够满足CIS程序所要求的时间可组合性，从而正确地调度CIS程序。本文的目标是提出一种可扩展的调度算法，以满足CIS的需求。我们提出了一种基于五个基本概念——操作、事件、变换、锚点和约束——的新颖时序模型结构，能够精确捕捉CIS的时序行为与约束。基于该模型，我们自动构建调度问题，并利用约束编程（CP）求解器进行求解。此外，我们还提出了一种同步已调度指令并生成功能完整的CIS汇编代码的方法。最后，我们设计了一系列实验，评估所提调度方法的可扩展性与调度质量。在多个复杂度维度上，我们的方法表现出线性可扩展性，并生成接近最优的调度方案。"
  },
  {
    "date": "2025-12-9",
    "title": "A Structured Approach to Verification of Digital Hardware in Scala",
    "authors": "Tjark Petersen, Luca Pezzarossa, Martin Schoeberl",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00014",
    "source": "IEEE",
    "abstract": "Functional verification accounts for a significant portion of the design effort in modern digital hardware development. As projects grow in complexity, maintaining and extending verification code becomes increasingly difficult, particularly in collaborative environments. This calls for a methodology that defines a clear structure and promotes reuse through modular, composable testbench components. In this paper, we present a Scala-based verification framework that adopts a structured approach to building modular and reusable testbenches, inspired by the Universal Verification Methodology (UVM). We analyze the core mechanisms through which UVM achieves modularity and reusability, and identify a minimal subset that provides equivalent functionality with reduced complexity. The result is a lightweight verification framework in Scala 3 using Verilator as a backend, which allows for simple unit-test-style testing as well as complex UVM-style testbench environments.",
    "title_zh": "一种面向数字硬件验证的结构化Scala方法",
    "abstract_zh": "功能验证在现代数字硬件开发中占据了相当大的设计工作量。随着项目复杂度的不断提升，维护和扩展验证代码变得愈发困难，尤其是在协作开发环境中。这要求一种能够定义清晰结构、并通过模块化、可组合的测试平台组件促进代码复用的方法。本文提出了一种基于Scala的验证框架，采用结构化方法构建模块化且可复用的测试平台，其设计灵感源自通用验证方法学（UVM）。我们分析了UVM实现模块化与复用的核心机制，并识别出一个最小功能子集，该子集在保持等效功能的同时显著降低了复杂性。最终，我们构建了一个轻量级的Scala 3验证框架，以Verilator作为后端，既支持类似单元测试的简单测试，也支持复杂的UVM风格测试平台环境。"
  },
  {
    "date": "2025-12-9",
    "title": "LLM4CVE: Enabling Iterative Automated Vulnerability Repair with Large Language Models",
    "authors": "Mohamad Fakih, Rahul Dharmaji, Halima Bouzidi, Gustavo Quiros Araya, Oluwatosin Ogundare, Mst Ayesha Siddika, Mohammad Abdullah Al Faruque",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00087",
    "source": "IEEE",
    "abstract": "Software vulnerabilities remain pervasive, even with the rise of AI-powered code assistants, advanced static analysis tools, and comprehensive testing frameworks. It’s clear that we must move beyond merely preventing these bugs; we need to eliminate them swiftly and efficiently. However, manual code intervention is slow, expensive, and can often introduce new security flaws, especially in legacy codebases. The advent of highly advanced Large Language Models (LLMs) presents a significant opportunity for automated software defect patching. We introduce LLM4CVE, an LLM-based iterative pipeline designed for robust and accurate repair of vulnerable functions in real-world code. We evaluate our pipeline using State-of-the-Art LLMs, including GPT-3.5, GPT-4o, Llama 3 8B, and Llama 3 70B. Our results demonstrate a human-verified quality score of 8.51/10 and a 20% increase in ground-truth code similarity with Llama 3 70B. To foster further research in LLM-based vulnerability repair, we release our evaluation framework, fine-tuned model weights, and experimental results on our website: https://sites.google.com/view/llm4cve",
    "title_zh": "LLM4CVE：利用大型语言模型实现迭代式自动化漏洞修复",
    "abstract_zh": "尽管人工智能驱动的代码助手、先进的静态分析工具以及全面的测试框架不断涌现，软件漏洞依然普遍存在。显然，我们不能再仅仅停留在预防漏洞的层面，而必须能够迅速、高效地彻底消除这些缺陷。然而，手动修复代码不仅耗时耗力，成本高昂，而且在遗留代码库中尤其容易引入新的安全漏洞。高度先进的大型语言模型（LLMs）的出现，为自动化软件缺陷修复带来了重大机遇。本文提出 LLM4CVE——一种基于大语言模型的迭代式修复流程，旨在对真实世界代码中的脆弱函数进行稳健且精准的修复。我们使用当前最先进的 LLM 模型（包括 GPT-3.5、GPT-4o、Llama 3 8B 和 Llama 3 70B）对本方法进行了评估。实验结果表明，经人工验证的质量得分为 8.51/10，且在与真实修复代码的相似度上，Llama 3 70B 实现了 20% 的提升。为推动基于 LLM 的漏洞修复研究，我们已将评估框架、微调后的模型权重及实验结果公开发布于我们的网站：https://sites.google.com/view/llm4cve"
  },
  {
    "date": "2025-12-9",
    "title": "Design-Space Exploration of Serialized Floating-Point Division for DLP Architectures",
    "authors": "Louis Ledoux",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00020",
    "source": "IEEE",
    "abstract": "We propose a framework for generating floatingpoint division units that leverages data-level parallelism (DLP) at the hardware level to enhance scalability, energy efficiency, and design flexibility. While modern ISAs support DLP via SIMD and vector extensions, arithmetic units often remain optimized for latency and do not scale efficiently across parallel workloads. Our approach revisits the microarchitecture of division units by trading latency for area, exposing parallelism through the instantiation of multiple smaller, slower units replicated spatially to exploit data-level parallelism. The framework operates as a high-level generator written in Python, automatically producing synthesizable ASIC designs across multiple floating-point formats and technology nodes. It supports format-agnostic and processagnostic design exploration, enabling rapid evaluation of tradeoffs between latency, area, and power. We validate our contributions through extensive evaluations encompassing 11 floating-point formats, including posits, IEEE754, and recent formats like EXMY, as well as process nodes ranging from 180 nm to 7 nm, demonstrating the scalability of our approach. Notably, area and power reductions reach up to $9.18 \\times$ and $61.28 \\times$, respectively, for Posit64 and IEEE754 double precision. When constraining designs to preserve performance, the best gains reach $3.12 \\times$ and $8.03 \\times$, demonstrating the viability of the approach for parallel workloads and vector datapaths.",
    "title_zh": "DLP架构中序列化浮点除法的设计空间探索",
    "abstract_zh": "我们提出了一种生成浮点除法单元的框架，该框架在硬件层面利用数据级并行性（DLP）以提升可扩展性、能效和设计灵活性。尽管现代指令集架构（ISA）通过SIMD和向量扩展支持数据级并行性，但算术单元通常仍针对延迟优化，难以高效扩展至并行工作负载。我们的方法重新审视了除法单元的微架构设计，通过牺牲延迟来节省面积，将多个较小且较慢的单元在空间上复制，从而暴露并利用数据级并行性。该框架以Python编写，作为高层生成器运行，能够自动为多种浮点格式和工艺节点生成可综合的ASIC设计。它支持格式无关和工艺无关的设计探索，可快速评估延迟、面积与功耗之间的权衡。我们通过广泛评估验证了本工作的贡献，涵盖11种浮点格式（包括Posit、IEEE754以及EXMY等新兴格式），以及从180 nm到7 nm的多个工艺节点，充分展示了该方法的可扩展性。值得注意的是，在Posit64和IEEE754双精度格式下，面积和功耗分别最多降低9.18倍和61.28倍；在保持性能约束的前提下，最佳优化效果可达3.12倍和8.03倍，证明了该方法在并行工作负载和向量数据通路中的可行性与有效性。"
  },
  {
    "date": "2025-12-9",
    "title": "Automatic Polynomial Formal Verification of a Floating-Point Multiplier",
    "authors": "Jan Kleinekathöfer, Rolf Drechsler",
    "publish": "2025 28th Euromicro Conference on Digital System Design (DSD)",
    "url": "https://doi.org/10.1109/dsd67783.2025.00088",
    "source": "IEEE",
    "abstract": "Floating-point multipliers play a crucial role in accelerating modern computing and are therefore implemented regularly, e.g., as part of AI accelerators or FPUs. They are built as a complex combination of control flow and integer arithmetic elements. This requires fast and easily applicable formal verification methods to ensure correctness. While formal verification is required to prove 100% correctness of the circuit, state-of-the-art methods fail to provide bounds for the resource and time consumption of the verification.We introduce an automated approach for formally verifying an IEEE-754-compliant floating-point multiplier with polynomial bound complexity. While state-of-the-art verification approaches based on SAT and Binary Decision Diagrams (BDDs) fail, our technique based on an adapted symbolic simulation using BDDs enables a fast and predictable verification process. Extensive case splitting is performed to prevent exponential BDD growth. The applicability is proven by verifying a single precision floating-point multiplier used in a popular floating-point unit.",
    "title_zh": "浮点乘法器的自动多项式形式化验证",
    "abstract_zh": "浮点乘法器在加速现代计算中起着至关重要的作用，因此经常被实现，例如作为人工智能加速器或浮点运算单元（FPU）的一部分。它们由复杂的控制流和整数算术元件组合而成，这要求具备快速且易于应用的形式化验证方法以确保其正确性。尽管形式化验证是证明电路100%正确性的必要手段，但当前最先进的方法无法为验证过程中的资源消耗和时间开销提供有效边界。本文提出了一种自动化的形式化验证方法，用于验证符合IEEE-754标准的浮点乘法器，并具有多项式复杂度的性能保证。与基于SAT求解器和二进制决策图（BDDs）的现有先进方法失败的情况不同，我们采用一种改进的符号仿真技术，结合BDDs，实现了快速且可预测的验证过程。通过大量案例分解策略，有效防止了BDD规模的指数级增长。该方法的有效性通过验证一个广泛使用的浮点运算单元中所采用的单精度浮点乘法器得到了充分证明。"
  },
  {
    "date": "2025-12-9",
    "title": "Systematic Framework Leveraging Early Assessment of DNN Reliability for Efficient and Reliable FPGA-Based Inference Deployment",
    "authors": "Vu Dang Nguyen Trinh, Otmane Ait Mohamed, Fakhreddine Ghaffari",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270647",
    "source": "IEEE",
    "abstract": "Deep Neural Networks (DNNs) deployed on edge AI hardware accelerators are increasingly susceptible to transient faults caused by environmental disturbances, often resulting in soft errors that degrade system reliability. In this work, we present a comprehensive and extensible framework that enables early evaluation of DNN reliability purely at the Python software level, supporting arbitrary user-defined models and datasets. The framework systematically incorporates the effects of aforementioned optimization techniques and introduces integrated error mitigation strategies to assess their effectiveness pre-hardware deployment. To bridge software-level evaluation with hardware implementation, we extend our framework to a hardware/software codesign platform featuring a configurable systolic array on FPGA. This array, enhanced with built-in fault mitigation mechanisms, accelerates convolution and linear layers, while an ARM CPU core completes the remaining inference tasks. The proposed system achieves resilient DNN inference with a 2.2% increase in FPGA LUT utilization (for LeNet-5 on MNIST). By enabling early-stage analysis and optimization, our approach facilitates robust and resource-efficient DNN deployment for edge AI applications.",
    "title_zh": "基于早期评估深度神经网络可靠性以实现高效可靠FPGA推理部署的系统性框架",
    "abstract_zh": "部署在边缘AI硬件加速器上的深度神经网络（DNN）正日益受到环境干扰引起的瞬态故障影响，这些故障常导致软错误，从而降低系统的可靠性。本文提出了一种全面且可扩展的框架，能够在纯Python软件层面实现对DNN可靠性的早期评估，支持任意用户自定义的模型和数据集。该框架系统性地整合了前述优化技术的影响，并引入集成的错误缓解策略，以在硬件部署前评估其有效性。为实现软件级评估与硬件实现之间的衔接，我们进一步将该框架扩展为一个软硬件协同设计平台，该平台基于FPGA构建了一个可配置的阵列结构。该阵列通过内置的故障缓解机制，加速卷积层和线性层的计算，而ARM CPU核心则负责完成其余推理任务。所提出的系统在MNIST数据集上的LeNet-5模型上仅带来2.2%的FPGA LUT资源增加，即可实现具备韧性的DNN推理。通过支持早期分析与优化，本方法有助于实现面向边缘AI应用的鲁棒且资源高效的DNN部署。"
  },
  {
    "date": "2025-12-9",
    "title": "Predictive Modeling of FPGA Resource and Power Consumption for Configurable CNN Operators",
    "authors": "Philippe Magalhães, Virginie Fresse, Benoît Suffran, Olivier Alata",
    "publish": "2025 32nd IEEE International Conference on Electronics, Circuits and Systems (ICECS)",
    "url": "https://doi.org/10.1109/icecs66544.2025.11270578",
    "source": "IEEE",
    "abstract": "As Convolutional Neural Networks (CNNs) continue to grow in complexity and accuracy, their deployment on embedded platforms requires hardware-aware optimizations to meet stringent constraints on logic resources, power, and latency. FPGAs offer an attractive solution because of their parallelism, reconfigurability, and energy efficiency. However, conventional FPGA design flows remain time-consuming and often lack early-stage estimation capabilities. This work introduces a library of parameterizable Intellectual Properties (IPs) for convolution, activation, and pooling, developed in VHDL and optimized for fixed-point arithmetic. IPs were designed to address various architectural trade-offs involving logic usage, DSP allocation strategy, parallelism, and power efficiency, while also supporting faster development through modular reuse. To accelerate design space exploration, a methodology is proposed for generating predictive mathematical models capable of estimating key FPGA resource metrics as functions of input bit widths. The models were validated with low prediction errors and coefficient of determination (R<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>) values greater than 0.94, allowing accurate estimation of the resources and dynamic power without synthesis. This supports fast architectural decisions and paves the way for automated, resource-aware CNN deployment on FPGAs.",
    "title_zh": "可配置CNN算子的FPGA资源与功耗预测建模",
    "abstract_zh": "随着卷积神经网络（CNN）在复杂性和准确性方面的持续提升，其在嵌入式平台上的部署需要硬件感知的优化，以满足逻辑资源、功耗和延迟等方面的严格约束。FPGA因其并行性、可重构性以及能效优势，成为一种极具吸引力的解决方案。然而，传统的FPGA设计流程仍存在耗时长且缺乏早期阶段估算能力的问题。本文提出了一种基于VHDL实现的可参数化知识产权核（IP）库，涵盖卷积、激活和池化操作，并针对定点运算进行了优化。这些IP核旨在解决逻辑资源使用、DSP分配策略、并行度与能效之间的多种架构权衡，同时通过模块化复用支持更快速的开发。为加速设计空间探索，本文还提出了一种方法，用于生成能够预测关键FPGA资源指标的数学模型，这些指标作为输入位宽的函数。验证结果表明，该模型具有较低的预测误差，决定系数（R²）超过0.94，能够在无需综合的情况下准确估算资源占用和动态功耗。这一能力显著支持了快速的架构决策，为实现自动化、资源感知的CNN在FPGA上的部署奠定了基础。"
  }
]