[
  {
    "date": "2025-11-17",
    "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer",
    "authors": "Zhixin Ou, Peng Liang, Jianchen Han, Baihui Liu, Linbo Qiao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13198v1",
    "source": "arXiv",
    "abstract": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.",
    "title_zh": "ParaDySe：一种用于Transformer中动态序列长度的并行策略切换框架",
    "abstract_zh": "动态序列因其长度可变，在基于Transformer的大语言模型（LLM）训练中被广泛使用。然而，当前的训练框架对这些序列采用预定义的静态并行策略，导致在短序列上无法消除通信并行化开销，在长序列上则容易引发内存溢出（OOM）问题。为缓解这些问题，我们提出了ParaDySe——一种针对动态序列的新型自适应并行策略切换框架。ParaDySe能够根据实时输入序列动态选择最优的并行策略。该框架首先构建了具有统一张量布局规范的模块化并行策略函数库，随后结合混合方法建立面向序列的内存与时间开销预测模型。在成本模型的引导下，ParaDySe通过高效的启发式算法为不同层选择最优的并行策略。通过整合上述技术，ParaDySe凭借其精心设计的函数库实现了最优策略的无缝热切换。我们在包含长达624K序列长度的数据集上，将ParaDySe与多个基线方法在代表性LLM上的表现进行了对比。实验结果表明，ParaDySe通过系统性地融合长序列优化技术与现有训练框架，有效解决了LLM训练中的OOM和CPC瓶颈问题。"
  },
  {
    "date": "2025-11-17",
    "title": "Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT",
    "authors": "Zhiteng Chao, Yonghao Wang, Xinyu Zhang, Jiaxin Zhou, Tenghui Hua, Husheng Han, Tianmeng Yang, Jianan Mu, Bei Yu, Rui Zhang, Jing Ye, Huawei Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13139v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability.",
    "title_zh": "自解耦与自验证思维：具备回溯思维链（Backtrack-ToT）的自动化RTL设计",
    "abstract_zh": "大型语言模型（LLMs）在使用寄存器传输级（RTL）硬件描述语言（HDL），如Verilog，自动化集成电路（IC）工程方面展现出巨大潜力。然而，确保Verilog生成质量仍面临诸多挑战。复杂设计往往因缺乏有针对性的分解策略，在单次生成中即告失败，且对分解后子任务正确性的评估依然困难。尽管思维链（Chain-of-Thought, CoT）方法常被用于提升LLM的推理能力，但在自动化IC设计流程中效果有限，通常仍需人工干预。其核心问题在于难以控制CoT推理的方向性和步骤粒度，这些特性与专家级RTL设计知识并不匹配。本文提出VeriBToT——一种专为自动化Verilog生成设计的LLM推理范式。通过融合自顶向下设计与面向验证的设计（Design-for-Verification, DFV）方法，VeriBToT实现了中间步骤的自我分解与自我验证，构建了一个带有形式化操作符的回溯思维树（Backtrack Tree of Thought）。相较于传统CoT范式，本方法不仅提升了Verilog生成质量，还通过灵活的模块化、层次结构和可复用性优化了token消耗成本。"
  },
  {
    "date": "2025-11-17",
    "title": "Evaluation of Domain-Specific Architectures for General-Purpose Applications in Apple Silicon",
    "authors": "Álvaro Corrochano López, Carlos García Sánchez",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13450v1",
    "source": "arXiv",
    "abstract": "The rise of AI and its growing computational demands have driven the integration of domain-specific accelerators (such as GPUs, TPUs, and NPUs) across the entire computing infrastructure. Following the precedent set by the GPGPU which popularized GPUs for general-purpose tasks, this research asks whether this phenomenon can be replicated with specialized accelerators like NPUs in new contexts. This paper evaluates the potential of the Apple Neural Engine (ANE) designed for high energy efficiency in Machine Learning workloads, in the context of general-purpose HPC applications. We evaluate the performance and energy consumption of classic HPC algorithms such as GEMM, Jacobi or Multigrid methods on Apple's ANE across the M1 and the latest M4 architectures. Results confirm that, when algorithms are properly adapted, the ANE achieves competitive performance (up to 3.8 TFlops on the M4-Pro, comparable to the GPU's 4.7 TFlops on the same SoC for GEMM operation) while demonstrating significantly superior energy efficiency (e.g., GEMM consumes 5.2 Watts on the ANE versus 24 Watts on GPU counterpart in M4 architectures).",
    "title_zh": "苹果硅芯片中特定领域架构在通用应用中的评估",
    "abstract_zh": "人工智能的兴起及其日益增长的计算需求，推动了专用加速器（如GPU、TPU和NPU）在整个计算基础设施中的集成。继通用图形处理器（GPGPU）将GPU普及应用于通用计算任务之后，本研究探讨这一趋势是否能在新的应用场景中通过类似NPU等专用加速器得以复制。本文评估了苹果公司设计的神经引擎（Apple Neural Engine, ANE）在通用高性能计算（HPC）应用中的潜力，该引擎专为机器学习工作负载的高能效而优化。我们针对苹果M1和最新M4架构上的ANE，评估了经典HPC算法（如GEMM、雅可比法及多重网格方法）的性能与能耗表现。结果表明，当算法经过适当适配后，ANE在M4-Pro上实现了高达3.8 TFlops的竞争力性能（与同一SoC中GPU的4.7 TFlops GEMM性能相当），同时展现出显著更优的能效表现——例如，在M4架构中，GEMM操作在ANE上仅消耗5.2瓦，而GPU counterpart则需24瓦。"
  },
  {
    "date": "2025-11-17",
    "title": "Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph",
    "authors": "Zhuo Chen, Gaoqiang Ji, Yiling He, Lei Wu, Yajin Zhou",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12971v1",
    "source": "arXiv",
    "abstract": "Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection. Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.",
    "title_zh": "Esim：基于稳定语义图的EVM字节码相似性检测",
    "abstract_zh": "去中心化金融（DeFi）正经历迅猛发展。然而，普遍存在的代码复用以及开源贡献有限等问题，给区块链生态系统带来了显著挑战，包括代码剽窃和存在漏洞的代码传播。因此，迫切需要一种高效且准确的以太坊虚拟机（EVM）字节码相似性检测方法，用于识别相似的智能合约。传统的二进制相似性检测方法通常基于指令流或控制流图（CFG），但由于EVM字节码具有低级别特性以及大量重用的基本块等特殊性质，这些方法在EVM场景下存在局限性。此外，Solidity编译器（Solc）版本的高度多样性进一步增加了精确相似性检测的难度。\n\n针对上述挑战，我们提出了一种新型的EVM字节码表示方法——稳定语义图（Stable-Semantic Graph, SSG），该方法能够捕捉“稳定指令”（由我们研究识别出的一类特殊指令）之间的关系。同时，我们实现了一个原型系统Esim，将SSG嵌入矩阵，并利用异构图神经网络进行相似性检测。Esim在SSG构建方面表现出高精度，控制流和数据流的F1得分分别达到100%和95.16%；其相似性检测性能达到96.3% AUC，优于传统方法。我们的大规模实证研究分析了过去一年内在六个EVM兼容链上共计2,675,573个智能合约的数据，结果表明，Esim在漏洞搜索方面表现优于当前最先进的工具Etherscan。"
  },
  {
    "date": "2025-11-17",
    "title": "Online Learning of HTN Methods for integrated LLM-HTN Planning",
    "authors": "Yuesheng Xu, Hector Munoz-Avila",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12901v1",
    "source": "arXiv",
    "abstract": "We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.",
    "title_zh": "基于LLM-HTN规划的集成方法中HTN方法的在线学习",
    "abstract_zh": "我们提出了在集成HTN规划与基于大语言模型（LLM）的聊天机器人背景下，对层次化任务网络（HTN）方法进行在线学习的方法。这些方法决定了何时以及如何将任务分解为子任务。我们的方法学习器建立在ChatHTN规划器之上：当某个任务没有可用的方法时，ChatHTN会调用ChatGPT来生成该任务到基本任务的分解。在本工作中，我们对ChatHTN进行了扩展：当ChatGPT生成任务分解时，ChatHTN会从中学习，类似于记忆化机制。然而，与传统的记忆化不同，它学习的是一个泛化的通用方法，不仅适用于当前遇到的具体实例，还能应用于同一任务的其他实例。我们在两个领域上进行了实验，结果表明，我们的在线学习过程在解决至少同样多的问题的同时，显著减少了对ChatGPT的调用次数，甚至在某些情况下能够解决更多问题。"
  },
  {
    "date": "2025-11-17",
    "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
    "authors": "Kajetan Dymkiewicz, Ivan Vulic, Helen Yannakoudakis, Eilam Shapira, Roi Reichart, Anna Korhonen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13368v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.",
    "title_zh": "捐赠者与接收者：基于参数高效微调的任务间与语言间不对称迁移",
    "abstract_zh": "大型语言模型（LLMs）在多种任务和语言上表现优异，但不同任务或语言之间的改进如何相互影响，以及这些影响在任务与语言组合中的表现，目前仍缺乏深入理解。我们针对多个开源权重的LLM家族及其不同规模，开展了一项受控的PEFT/LoRA研究，将任务和语言作为迁移轴，同时固定模型家族和规模；我们对每个模型仅在单一任务-语言组合上进行微调，并以基准分数为参照，测量其在所有其他任务-语言目标组合上的表现变化，以百分点差值表示迁移效果。我们将迁移效应分解为三类：(i) 匹配任务（跨语言），(ii) 匹配语言（跨任务），以及(iii) 跨任务（跨语言）。我们发现了两个一致性的普遍规律：第一，显著的任务内与任务外不对称性：匹配任务（跨语言）的迁移效果始终为正，而任务外迁移常导致性能退化；第二，在不同任务和语言之间存在稳定的“捐赠者-接收者”结构——即某些语言或任务作为“枢纽型捐赠者”表现出强迁移能力，而另一些则作为“脆弱型接收者”难以有效吸收迁移知识。这些发现对风险敏感的微调策略及模型专业化设计具有重要启示。"
  },
  {
    "date": "2025-11-17",
    "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
    "authors": "Alexandru-Mihai Apostu, Andrei Preda, Alexandra Daniela Damir, Diana Bolocan, Radu Tudor Ionescu, Ioana Croitoru, Mihaela Gaman",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13333v1",
    "source": "arXiv",
    "abstract": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
    "title_zh": "AutoMalDesc：面向网络威胁研究的大规模脚本分析",
    "abstract_zh": "尽管自动化恶意软件检测系统取得了显著进展，但为威胁检测生成全面、自然语言描述的问题在网络安全研究中仍是一个未解难题。本文提出了一种名为AutoMalDesc的自动化静态分析摘要框架，该框架在经过少量专家精心标注示例的初步训练后，能够独立地大规模运行。该方法采用一种迭代式的自适应学习流程，通过合成数据生成与验证循环逐步提升输出质量，从而无需依赖大量人工数据标注。在五种脚本语言共3,600个多样化样本上的评估表明，各迭代阶段之间均表现出统计学意义上的显著改进，摘要质量与分类准确率持续提升。我们的综合验证方法结合了基于已有恶意软件标签的定量指标，以及来自人类专家和大语言模型（LLM）评判者的定性评估，充分验证了生成摘要在技术准确性与语言连贯性方面的优势。为促进该领域的可复现性并推动研究发展，我们公开发布了包含超过10万条脚本样本的完整数据集，其中包括标注的种子数据集（0.9K）和测试数据集（3.6K），以及我们的方法论与评估框架。"
  },
  {
    "date": "2025-11-17",
    "title": "Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training",
    "authors": "Xinyuan Zhou, Yi Lei, Xiaoyu Zhou, Jingyi Sun, Yu Zhu, Zhongyi Ye, Weitai Zhang, Quan Liu, Si Wei, Cong Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13043v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a \"CoT-augmented state prediction\" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.",
    "title_zh": "Spark-Prover-X1：通过多样化数据训练实现形式化定理证明",
    "abstract_zh": "大型语言模型（LLMs）在自动化定理证明领域展现出巨大潜力，但其进展常受限于多样化且高质量的形式化语言数据的稀缺。为解决这一问题，我们提出了 Spark-Prover-X1，一个拥有70亿参数的模型，通过一种三阶段训练框架，旨在激发更易获取且中等规模的LLM的推理潜能。第一阶段通过在广泛的数学语料库上进行持续预训练，并结合一系列创新的数据任务，注入深层知识。其中关键创新在于“思维链增强的状态预测”任务，以实现细粒度的推理能力。第二阶段采用监督微调（SFT）并融入专家迭代循环，专门优化 Spark-Prover-X1-7B 和 Spark-Formalizer-X1-7B 两个模型。最后，通过一轮针对性的组相对策略优化（GRPO），进一步提升证明器在最具挑战性问题上的表现。为支持稳健评估，特别是针对真实考试中的题目，我们还推出了 ExamFormal-Bench——一个包含402个形式化问题的新基准数据集。实验结果表明，Spark-Prover-X1-7B 在同类开源模型中达到了领先水平，平均通过率（pass@32）达到37.0%。在高难度竞赛基准测试中表现尤为突出：在 PutnamBench 上成功解决27道题（pass@32），在 CombiBench 上取得24.0%的通过率（pass@32）。我们的工作验证了多样化训练数据与逐步精炼的训练流程，为提升轻量级LLM的形式化推理能力提供了一条有效路径。Spark-Prover-X1-7B 和 Spark-Formalizer-X1-7B 模型，以及 ExamFormal-Bench 数据集，现已公开发布，可访问以下链接：https://www.modelscope.cn/organization/iflytek，https://gitcode.com/ifly_opensource。"
  },
  {
    "date": "2025-11-17",
    "title": "Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection",
    "authors": "Sadegh Mahdavi, Branislav Kisacanin, Shubham Toshniwal, Wei Du, Ivan Moshkov, George Armstrong, Renjie Liao, Christos Thrampoulidis, Igor Gitman",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13027v1",
    "source": "arXiv",
    "abstract": "Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.",
    "title_zh": "用于自然语言数学证明验证与选择的生成式验证器的扩展",
    "abstract_zh": "大型语言模型在最终答案型数学问题上取得了显著成功，这主要得益于可验证奖励的强化学习方法易于应用。然而，这些解决方案背后的推理过程往往存在缺陷。要迈向严格的证明型数学，必须具备可靠的证明验证能力。我们首先分析了多种评估设置，发现仅依赖单一基准测试可能导致脆弱或误导性的结论。为解决这一问题，我们同时评估基于证明和最终答案的推理，以获得更可靠的模型性能度量。随后，我们将两种主要的生成式验证方法（GenSelect 和 LLM-as-a-Judge）扩展至数百万个标记，并识别出两者的结合是解决方案验证与选择最有效的框架。我们进一步发现，LLM-as-a-Judge 的提示设计对模型表现有显著影响，但强化学习可以降低这种敏感性。然而，尽管强化学习提升了证明层面的指标，却并未提高最终答案的精确度，表明当前模型往往奖励的是形式或程序上的正确性，而非数学上的有效性。我们的研究结果为设计和评估可扩展的证明验证与选择系统提供了实用指导。"
  },
  {
    "date": "2025-11-17",
    "title": "Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation",
    "authors": "Yafang Wang, Yangjie Tian, Xiaoyu Shen, Gaoyang Zhang, Jiaze Sun, He Zhang, Ruohua Xu, Feng Zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12916v1",
    "source": "arXiv",
    "abstract": "Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.",
    "title_zh": "Fault2Flow：一种基于AlphaEvolve优化的人机协同多智能体系统，实现故障到工作流的自动化",
    "abstract_zh": "电网故障诊断是一个关键过程，但其进展受到对人工、易出错方法的依赖所阻碍。技术人员必须手动从密集的规程中提取推理逻辑，并尝试将其与隐性专家知识相结合，这一过程效率低下、容易出错，且随着规程更新和经验积累，难以维护。尽管大型语言模型（LLMs）在解析非结构化文本方面展现出巨大潜力，但目前尚无任何框架能够将这两种异构知识源整合为一个统一、可验证且可执行的工作流。为弥合这一差距，我们提出了 Fault2Flow——一种基于大语言模型的多智能体系统。Fault2Flow 系统性地实现以下四点：(1) 将规程逻辑提取并结构化为 PASTA 格式的故障树；(2) 通过人机协同接口集成专家知识以进行验证；(3) 利用一种新颖的 AlphaEvolve 模块优化推理逻辑；(4) 将最终经过验证的逻辑合成生成可由 n8n 执行的工作流。在变压器故障诊断数据集上的实验验证表明，该方法实现了 100% 的拓扑一致性以及高度的语义保真度。Fault2Flow 建立了一条从故障分析到操作自动化的可复现路径，显著降低了专家的工作负担。"
  },
  {
    "date": "2025-11-17",
    "title": "On the Fundamental Limits of LLMs at Scale",
    "authors": "Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zeeshan Memon, Muhammad Ibtsaam Qadir, Sagnik Bhattacharya, Hassan Rizwan, Abhiram R. Gorle, Maahe Zehra Kazmi, Ayesha Mohsin, Muhammad Usman Rafique, Zihao He, Pulkit Mehta, Muhammad Ali Jamshed, John M. Cioffi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12869v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.",
    "title_zh": "大模型在规模化下的根本局限",
    "abstract_zh": "大规模语言模型（LLMs）在规模扩展方面获得了巨大收益，但这些收益受到五个根本性限制的制约：（1）幻觉现象，（2）上下文压缩，（3）推理能力退化，（4）检索脆弱性，以及（5）多模态错位。尽管现有综述从经验层面描述了这些现象，却缺乏将它们与计算、信息和学习的基本极限相联系的严谨理论整合。本文通过提出一个统一且基于证明的框架，填补了这一空白，形式化地刻画了LLM扩展所固有的理论上限。\n\n首先，可计算性与不可计算性的原理意味着误差存在不可消除的残余：对于任何可枚举的模型族，对角化论证保证了某些输入上必然存在模型失效的情况；而不可判定的问题（如停机类任务）则会导致所有可计算预测器都面临无限的失败集合。\n\n其次，信息论与统计约束限制了即使在可判定任务上所能达到的精度上限：有限描述长度强制产生压缩误差，而长尾事实知识则需要难以企及的样本复杂度。\n\n第三，几何与计算效应导致长上下文被压缩至远低于其名义大小的程度，原因包括位置训练不足、编码衰减以及softmax拥挤现象。\n\n我们进一步揭示，基于似然的训练方式倾向于偏好模式补全而非真正推理；在token数量受限的情况下，检索会遭受语义漂移与耦合噪声的影响；而多模态扩展则继承了浅层的跨模态对齐问题。\n\n在各章节中，我们结合定理与实证证据，系统阐明了扩展在何处有效、何处趋于饱和，以及何处无法再取得进展。这不仅提供了坚实的理论基础，也提出了切实可行的缓解路径，例如有界oracle检索、位置课程设计，以及稀疏或分层注意力机制。"
  },
  {
    "date": "2025-11-17",
    "title": "Bootstrapping LLMs via Preference-Based Policy Optimization",
    "authors": "Chen Jia",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12867v1",
    "source": "arXiv",
    "abstract": "Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.",
    "title_zh": "通过基于偏好的策略优化来自举大语言模型",
    "abstract_zh": "通过基于偏好的策略优化来实现大语言模型（LLM）的自举（bootstrapping），为在不依赖大量人工标注的情况下使模型行为与人类偏好对齐提供了有前景的方向。在本工作中，我们提出了一种新颖的基于偏好的策略优化（PbPO）框架，将学习过程建模为主体策略与奖励模型（RM）之间的极小极大博弈。该奖励模型被约束在由偏好数据导出的置信集内，以确保其利用的可靠性。我们设计的迭代在线算法通过引导对不断演化的策略进行探索，主动收集偏好数据，从而实现策略与奖励模型的持续自我改进。我们为该方法提供了理论保证，建立了在序列级奖励模型和词元级奖励模型两种情形下的高概率后悔界，证明了其在自举大语言模型方面的有效性。在五个基准测试上的大量实验表明，我们的方法始终优于现有的最先进偏好优化技术。"
  },
  {
    "date": "2025-11-17",
    "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators",
    "authors": "Taras Sereda, Tom St. John, Burak Bartan, Natalie Serrino, Sachin Katti, Zain Asgar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13274v1",
    "source": "arXiv",
    "abstract": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms. We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.",
    "title_zh": "KForge：面向多样化AI硬件加速器的程序生成",
    "abstract_zh": "GPU内核对机器学习性能至关重要，但要在多种异构加速器上进行优化却十分困难。我们提出了KForge，一个与平台无关的框架，其基于两个协同工作的基于大语言模型（LLM）的智能体：生成智能体通过编译和正确性反馈不断生成并迭代优化程序；性能分析智能体则解析性能剖析数据，指导优化方向。这种基于智能体的架构仅需一次示例即可适配新平台。本文做出三项关键贡献：（1）提出一种迭代优化机制，使生成智能体与性能分析智能体通过功能性和优化性遍历协同工作，能够解读多样化的性能剖析数据（包括程序化API和图形化工具），生成可操作的优化建议，从而指导任意加速器上的程序合成；（2）证明生成智能体能有效利用跨平台知识迁移——来自某一架构的参考实现显著提升了在不同硬件目标上的生成质量；（3）通过在根本不同的并行计算平台（NVIDIA CUDA与Apple Metal）上均实现高效程序合成，验证了本方法的平台无关特性。"
  },
  {
    "date": "2025-11-17",
    "title": "Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting",
    "authors": "Yunhun Nam, Jaehyung Kim, Jongheon Jeong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13052v1",
    "source": "arXiv",
    "abstract": "Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to \"undesirable\" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.",
    "title_zh": "从不良经验中学习：语言模型在不遗忘的情况下实现稳健适应",
    "abstract_zh": "语言模型（LMs）通常通过监督微调（SFT）来适应特定下游任务，以增强其在该任务上的能力。然而，在微调数据有限的典型场景下（例如，与预训练数据相比），SFT可能导致模型过拟合，使其依赖目标任务中的虚假模式，或在狭义专业化过程中损害其他广泛有用的通用能力。本文提出一种名为“从不良行为中学习”（Learning-from-the-Undesirable, LfU）的简单而有效的正则化方法，用于缓解在数据量有限时微调语言模型所引发的过拟合问题。具体而言，我们旨在对微调过程进行正则化，以偏好那些对“不良”模型更新具有鲁棒性的解，例如导致模型走向不良行为的梯度上升步骤。为此，我们提出了一种新颖的一致性正则化形式，直接将模型内部表示与经历不良更新后的表示对齐。通过利用不良更新带来的表示层面的数据增强，LfU在数据有限的情况下有效促进了泛化能力。我们在多种语言模型下游任务上的实验表明，LfU作为一种有效的先验知识，能够提升模型的适应性，同时保留预训练阶段的知识。例如，在相同数据集上，采用LfU训练的语言模型在数学任务上平均性能提升了16.8%，而传统的SFT方法甚至导致这些任务性能下降。此外，LfU在应对提示变化方面表现出更强的鲁棒性，例如输出性能的标准差比SFT降低了92.1%，凸显了其多方面的优势。"
  },
  {
    "date": "2025-11-17",
    "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
    "authors": "Chunshi Wang, Junliang Ye, Yunhan Yang, Yang Li, Zizhuo Lin, Jun Zhu, Zhuo Chen, Yawei Luo, Chunchao Guo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13647v1",
    "source": "arXiv",
    "abstract": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
    "title_zh": "Part-X-MLLM：面向部件的3D多模态大语言模型",
    "abstract_zh": "我们提出了 Part-X-MLLM，这是一种原生的3D多模态大语言模型，通过将各种3D任务统一为结构化、可执行的语法程序，实现了对多样3D任务的整合。给定一个RGB点云和自然语言提示，我们的模型能够自回归地生成单一、连贯的标记序列，该序列编码了部件级的边界框、语义描述以及编辑命令。这种结构化的输出作为通用接口，可驱动下游的几何感知模块，实现基于部件的生成与编辑。通过将符号规划与几何合成解耦，我们的方法使得任何兼容的几何引擎均可通过单一、语言原生的前端进行控制。我们采用双编码器架构进行预训练，以分离结构与语义，并在大规模、以部件为中心的数据集上对模型进行指令微调。实验表明，该模型在生成高质量、结构化计划方面表现卓越，通过统一接口实现了在接地问答、组合生成和局部编辑任务上的最先进性能。项目页面：https://chunshi.wang/Part-X-MLLM/"
  },
  {
    "date": "2025-11-17",
    "title": "The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training",
    "authors": "Subramanyam Sahoo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13016v1",
    "source": "arXiv",
    "abstract": "Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.",
    "title_zh": "好、坏与混合：推理模型训练中的奖励机制大比拼",
    "abstract_zh": "奖励设计在人类反馈强化学习（RLHF）和对齐研究中占据核心地位。本文提出一个统一框架，用于研究针对大语言模型（LLMs）在数学推理任务上微调时的硬性奖励、连续奖励以及混合奖励结构。我们基于Qwen3-4B模型，采用LoRA微调方法，在GSM8K数据集上形式化并实证评估了融合正确性、困惑度、推理质量与一致性的奖励函数。我们引入一种自适应的混合奖励调度机制，能够在离散信号与连续信号之间动态切换，有效平衡探索与稳定性。实验结果表明，相较于纯粹的硬性或连续奖励方法，混合奖励结构能够显著提升收敛速度与训练稳定性，为通过自适应奖励建模实现对齐提供了重要启示。"
  },
  {
    "date": "2025-11-17",
    "title": "SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports",
    "authors": "Longfei Chen, Ruibin Yan, Taiyu Wong, Yiyang Chen, Chao Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12993v1",
    "source": "arXiv",
    "abstract": "Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.",
    "title_zh": "SmartPoC：为智能合约漏洞报告生成可执行且经过验证的PoC",
    "abstract_zh": "智能合约容易存在漏洞，通常由专家以及自动化系统（如静态分析和AI辅助解决方案）进行分析。然而，审计报告的产物具有异构性，且往往缺乏可复现、可执行的PoC（概念验证）测试用例，难以用于自动化验证，导致需要耗费大量成本进行临时性的手动验证。大型语言模型（LLMs）可用于将审计报告转化为PoC测试用例，但面临三大挑战：输入噪声、幻觉现象以及缺少运行时断言（runtime oracles）。本文提出SmartPoC，一个自动化的框架，能够将文本形式的审计报告转换为可执行且经过验证的测试用例。首先，对输入的审计报告进行降噪处理，仅提取与漏洞相关的函数，并将其作为上下文输入给LLM。为抑制幻觉并确保生成的测试用例具备编译与运行能力，我们设计了特殊的预/后执行修复机制，利用LLM合成PoC测试用例。此外，我们采用差分验证作为断言，以确认所生成PoC测试用例的可利用性。在SmartBugs-Vul和FORGE-Vul基准测试中，SmartPoC分别成功生成了85.61%和86.45%目标的可执行、已验证Foundry测试用例。应用于最新的Etherscan已验证源码语料库，SmartPoC以每条发现仅0.03美元的成本，确认了545个审计发现中的236个真实漏洞。"
  },
  {
    "date": "2025-11-17",
    "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
    "authors": "Haoyang Hong, Jiajun Yin, Yuan Wang, Jingnan Liu, Zhe Chen, Ailing Yu, Ji Li, Zhiling Ye, Hansong Xiao, Yefei Chen, Hualei Zhou, Yun Yue, Minghui Yang, Chunxiao Guo, Junwei Liu, Peng Wei, Jinjie Gu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13288v1",
    "source": "arXiv",
    "abstract": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
    "title_zh": "多智能体深度研究：使用M-GRPO训练多智能体系统",
    "abstract_zh": "多智能体系统在通用推理任务中表现良好，但缺乏在特定领域的训练限制了其准确性。当前的训练方法通常为系统中的所有智能体统一训练一个大型语言模型（LLM），这可能因不同智能体背后的数据分布差异而制约性能。因此，采用各自独立的LLM来训练多智能体系统应成为下一步的发展方向。然而，这一方法带来了新的优化挑战：例如，各智能体运行频率不同，轨迹中涉及的子智能体调用次数不一，且智能体常部署于不同的服务器上，导致端到端梯度流中断。为解决这些问题，我们提出M-GRPO，这是一种专为具有主智能体（规划者）和多个子智能体（多轮工具执行器）的垂直型多智能体系统设计的分层扩展方法，基于组相对策略优化（Group Relative Policy Optimization）。M-GRPO同时为高层主智能体与底层子智能体计算组相对优势，保持层级化的信用分配机制。此外，该方法引入了一种轨迹对齐方案，即使在子智能体调用数量不固定的情况下，也能生成固定大小的训练批次。我们采用解耦式训练流程，使各智能体在独立服务器上运行，并通过共享存储仅交换少量统计信息，从而实现无需跨服务器反向传播的可扩展训练。在真实世界基准测试（如GAIA、XBench-DeepSearch和WebWalkerQA）上的实验表明，M-GRPO始终优于单智能体GRPO以及使用冻结子智能体的多智能体GRPO，展现出更高的稳定性与样本效率。结果表明，对异构轨迹进行对齐并解耦专业化智能体间的优化过程，能够显著提升工具增强型推理任务的表现。"
  },
  {
    "date": "2025-11-17",
    "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
    "authors": "Rufeng Chen, Shuaishuai Jiang, Jiyun Shen, AJung Moon, Lili Wei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13271v1",
    "source": "arXiv",
    "abstract": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
    "title_zh": "生成式AI模型在学生软件编程学习活动中的应用研究",
    "abstract_zh": "生成式人工智能（GenAI）工具如ChatGPT的兴起，为计算教育带来了新的机遇与挑战。现有研究主要关注GenAI完成教育任务的能力及其对学生学业表现的影响，却常常忽视了其对知识获取的影响。在本研究中，我们探讨了GenAI辅助与传统在线资源相比，在不同能力水平学生中支持知识增长的效果。我们开展了一项受控用户实验，共招募24名具有不同编程经验水平的本科生（初学者和中级水平），考察学生在解决编程任务时与ChatGPT的互动方式。我们分析了任务表现、概念理解以及交互行为。研究结果表明，使用GenAI生成完整解决方案显著提升了任务表现，尤其对初学者效果更为明显，但并未能持续带来知识增长。重要的是，不同经验水平的学生使用策略存在差异：初学者往往高度依赖GenAI以完成任务，过程中缺乏知识积累；而中级学习者则采取更为选择性的使用方式。我们发现，过度依赖或几乎不使用GenAI都会导致整体知识增长较弱。基于上述发现，我们呼吁学生和教育工作者将GenAI视为一种学习工具，而非单纯的问题解决工具。本研究强调，在将GenAI融入编程教育时，亟需提供有效指导，以促进更深层次的理解。"
  },
  {
    "date": "2025-11-17",
    "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation",
    "authors": "Hao Hu, Yifan Feng, Ruoxue Li, Rundong Xue, Xingliang Hou, Zhiqiang Tian, Yue Gao, Shaoyi Du",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13201v1",
    "source": "arXiv",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.",
    "title_zh": "Cog-RAG：受认知启发的双超图主题对齐检索增强生成",
    "abstract_zh": "检索增强生成（RAG）通过引入外部知识来缓解大语言模型（LLMs）的幻觉问题，从而提升其响应质量与领域特定性能。在近期的研究中，图结构被引入RAG以更好地捕捉实体之间的语义关系。然而，这类方法主要关注低阶的成对实体关系，难以充分建模多个实体间的高阶关联。超图增强方法通过超边建模多实体交互，弥补了这一不足，但通常仅限于块间实体级别的表示，忽略了不同文本块之间的全局主题组织与对齐。受人类认知推理自上而下的思维过程启发，我们提出一种主题对齐的双超图RAG框架（Cog-RAG），该框架利用主题超图捕捉块间主题结构，同时通过实体超图建模高阶语义关系。此外，我们设计了一种受认知启发的两阶段检索策略：首先从主题超图中激活与查询相关的主题内容，随后在实体超图中引导细粒度的召回与信息扩散，实现从全局主题到局部细节的语义对齐与一致生成。大量实验结果表明，Cog-RAG显著优于现有的先进基线方法。"
  },
  {
    "date": "2025-11-17",
    "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction",
    "authors": "Quanjiang Guo, Sijie Wang, Jinchuan Zhang, Ben Zhang, Zhao Kang, Ling Tian, Ke Yan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13118v1",
    "source": "arXiv",
    "abstract": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.",
    "title_zh": "像代码一样提取事件：一种用于零样本事件抽取的多智能体编程框架",
    "abstract_zh": "零样本事件抽取（ZSEE）对大型语言模型（LLMs）而言仍是一项重大挑战，原因在于其需要复杂的推理能力与领域特定的理解。直接提示往往导致输出不完整或结构上无效——例如触发词分类错误、论元缺失以及违反模式规范等问题。为解决这些局限性，我们提出了一种名为Agent-Event-Coder（AEC）的新型多智能体框架，将事件抽取视为软件工程过程：一种结构化、迭代式的代码生成流程。AEC将零样本事件抽取分解为若干专门子任务——检索、规划、编码与验证，每个任务均由一个专用的LLM智能体负责处理。事件模式以可执行的类定义形式表示，从而实现确定性验证，并通过验证智能体提供精确反馈。这种受编程启发的方法，能够通过迭代优化实现系统性的歧义消解与模式约束强制。借助协同智能体工作流，AEC使LLM能够在零样本设置下生成精确、完整且符合模式一致性的事件抽取结果。在五个不同领域和六种LLM上的实验表明，AEC始终优于以往的零样本基线方法，充分展现了将事件抽取视为代码生成的强大潜力。代码与数据已发布于 https://github.com/UESTC-GQJ/Agent-Event-Coder。"
  },
  {
    "date": "2025-11-17",
    "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains",
    "authors": "Zihe Yan, Kai Luo, Haoyu Yang, Yang Yu, Zhuosheng Zhang, Guancheng Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13341v1",
    "source": "arXiv",
    "abstract": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.",
    "title_zh": "基于大语言模型的开源软件供应链高隐蔽性后门风险评估定量框架",
    "abstract_zh": "在现代软件开发工作流程中，开源软件供应链极大地促进了工程实践的高效与便捷。随着系统复杂性的不断增加，将开源软件作为第三方依赖已成为普遍做法。然而，底层依赖项缺乏维护以及社区审计不足，给源代码安全性和仓库维护者合法性带来了挑战，尤其是在像 XZ-Util 事件所体现的高隐蔽性后门攻击面前尤为突出。为应对这些问题，我们提出了一种细粒度的项目评估框架，用于评估开源软件中的后门风险。该框架从攻击者的视角建模隐蔽性后门攻击，并针对每个攻击阶段定义了相应的量化指标。此外，为克服静态分析在评估仓库维护活动可靠性（如不规则的提交者权限提升、评审参与度有限）方面的局限性，该框架采用大语言模型（LLMs）对代码仓库进行语义层面的评估，无需依赖人工设计的模式匹配。我们在 Debian 生态系统中66个高优先级软件包上对该框架进行了验证。实验结果表明，当前的开源软件供应链面临着多种安全风险。"
  },
  {
    "date": "2025-11-17",
    "title": "Towards Quantum Software for Quantum Simulation",
    "authors": "Maja Franz, Lukas Schmidbauer, Joshua Ammermann, Ina Schaefer, Wolfgang Mauerer",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13520v1",
    "source": "arXiv",
    "abstract": "Quantum simulation is a leading candidate for demonstrating practical quantum advantage over classical computation, as it is believed to provide exponentially more compute power than any classical system. It offers new means of studying the behaviour of complex physical systems, for which conventionally software-intensive simulation codes based on numerical high-performance computing are used. Instead, quantum simulations map properties and characteristics of subject systems, for instance chemical molecules, onto quantum devices that then mimic the system under study. Currently, the use of these techniques is largely limited to fundamental science, as the overall approach remains tailored for specific problems: We lack infrastructure and modelling abstractions that are provided by the software engineering community for other computational domains. In this paper, we identify critical gaps in the quantum simulation software stack-particularly the absence of general-purpose frameworks for model specification, Hamiltonian construction, and hardware-aware mappings. We advocate for a modular model-driven engineering (MDE) approach that supports different types of quantum simulation (digital and analogue), and facilitates automation, performance evaluation, and reusability. Through an example from high-energy physics, we outline a vision for a quantum simulation framework capable of supporting scalable, cross-platform simulation workflows.",
    "title_zh": "面向量子模拟的量子软件",
    "abstract_zh": "量子模拟是展示实际量子优势超越经典计算的有力候选技术，因为它被认为能提供比任何经典系统都呈指数级增长的计算能力。它为研究复杂物理系统提供了新途径，而这些系统传统上依赖于基于数值高性能计算的软件密集型仿真代码。相比之下，量子模拟将目标系统（如化学分子）的属性和特征映射到量子设备上，使这些设备能够模拟所研究的系统。目前，这类技术的应用主要局限于基础科学领域，因为整体方法仍针对特定问题进行定制：我们缺乏软件工程界在其他计算领域提供的基础设施和建模抽象。本文指出了量子模拟软件栈中的关键差距——特别是缺乏通用框架来支持模型定义、哈密顿量构建以及硬件感知的映射。我们倡导采用模块化、模型驱动的工程（MDE）方法，以支持多种类型的量子模拟（数字与模拟），并促进自动化、性能评估和可重用性。通过高能物理领域的一个实例，我们勾勒出一个能够支持可扩展、跨平台模拟工作流的量子模拟框架的愿景。"
  },
  {
    "date": "2025-11-17",
    "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding",
    "authors": "Jiyang Zheng, Islam Nassar, Thanh Vu, Xu Zhong, Yang Lin, Tongliang Liu, Long Duong, Yuan-Fang Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13361v1",
    "source": "arXiv",
    "abstract": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",
    "title_zh": "MedDCR：学习设计医疗编码的智能工作流",
    "abstract_zh": "医疗编码将自由文本的临床记录转化为标准化的诊断与操作代码，对于医保报销、医院运营及医学研究至关重要。与普通文本分类不同，医疗编码需要多步推理：提取诊断概念、应用指南约束、映射到层级代码本，并确保跨文档的一致性。尽管近期研究采用了代理型大语言模型（LLM），但大多数方法依赖于僵化且手工设计的工作流程，难以捕捉真实临床文档中的复杂性和多样性，因而仍存在如何系统性地学习高效工作流程的开放问题。我们提出 MedDCR，一个闭环式框架，将工作流程设计视为一个可学习的问题。该框架中，设计师（Designer）提出工作流程，编码员（Coder）执行流程，反思者（Reflector）评估预测结果并提供建设性反馈，同时记忆库（memory archive）保存以往的设计以供复用和迭代优化。在基准数据集上的实验表明，MedDCR优于当前最先进的基线方法，生成了可解释且可适应的工作流程，更贴近真实的编码实践，显著提升了自动化系统的可靠性与可信度。"
  },
  {
    "date": "2025-11-17",
    "title": "MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications",
    "authors": "Gagan Raj Gupta, Anshul Kumar, Manish Rai, Apu Chakraborty, Ashutosh Modi, Abdelaali Chaoub, Soumajit Pramanik, Moyank Giri, Yashwanth Holla, Sunny Kumar, M. V. Kiran Sooraj",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13131v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.",
    "title_zh": "MM-Telco：电信应用中的基准测试与多模态大语言模型",
    "abstract_zh": "大型语言模型（LLMs）已成为自动化复杂推理与决策任务的强大工具。在电信领域，它们有望在网络优化、故障自动排查、客户支持增强以及合规性保障等方面带来变革。然而，其在电信行业的部署受到特定领域挑战的制约，亟需针对性的适应与改进。为克服这些挑战并加速LLMs在电信领域的应用落地，我们提出了MM-Telco——一个专为电信领域量身定制的综合性多模态基准测试套件及模型体系。该基准涵盖多种任务（包括文本和图像类），覆盖了网络运维、网络管理、文档质量提升以及相关文本与图像检索等实际应用场景。此外，我们还对多种LLM和VLM进行了基线实验。在我们的数据集上微调后的模型表现出显著的性能提升。同时，实验也揭示了当前先进多模态大模型在实际应用中的薄弱环节，为后续的模型优化与研究提供了重要方向。"
  },
  {
    "date": "2025-11-17",
    "title": "AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models",
    "authors": "Declan Jackson, William Keating, George Cameron, Micah Hill-Smith",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13029v1",
    "source": "arXiv",
    "abstract": "Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.",
    "title_zh": "AA-全知性：评估大型语言模型在跨领域知识可靠性中的表现",
    "abstract_zh": "现有的语言模型评估主要衡量通用能力，然而在多个领域中可靠地使用这些模型，需要具备事实准确性以及对知识盲区的识别能力。我们提出了AA-Omniscience基准，旨在衡量6000个问题上的事实回忆能力与知识校准水平。这些问题源自权威的学术和行业资料，涵盖六个不同领域中的42个与经济相关的主题。该评估通过“全知指数”（Omniscience Index）来衡量模型的表现，这是一个取值范围为-100至100的有界指标，同时惩罚幻觉并奖励在不确定时选择不回答，得分为0表示模型正确回答与错误回答的次数相等。在所评估的模型中，Claude 4.1 Opus得分最高（4.8），是仅有的三个得分高于零的模型之一。这些结果揭示了前沿模型在事实性和校准性方面仍存在持续性的弱点。此外，不同领域的表现也存在差异，来自三家不同研究机构的模型在六个领域中分别领先。这种性能差异表明，在知识至关重要的任务中，应根据具体应用场景选择模型，而非仅依据整体性能进行判断。"
  },
  {
    "date": "2025-11-17",
    "title": "Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities",
    "authors": "Zirui Chen, Zhipeng Xue, Jiayuan Zhou, Xing Hu, Xin Xia, Xiaohu Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12950v1",
    "source": "arXiv",
    "abstract": "Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.",
    "title_zh": "Diffploit：促进开源库漏洞跨版本漏洞利用迁移",
    "abstract_zh": "漏洞利用（Exploits）常被用于演示库中存在漏洞，并验证其在不同版本间的实际影响。然而，由于在版本演进过程中引入的破坏性变更（breaking changes），这些漏洞利用直接应用于其他版本时往往失败。这类失败主要源于触发条件的变化（如API重构）以及动态环境的破坏（如构建或运行时错误），而这些因素难以手动解读和调整。现有技术主要依赖模糊测试（fuzzing）进行代码级的追踪对齐，但这种方法耗时且无法有效处理环境层面的故障；此外，在面对跨版本复杂触发条件变化时也常常力不从心。\n\n为克服上述挑战，我们提出了 Diffploit——一种基于差异驱动、迭代式的漏洞利用迁移方法，其结构包含两个核心模块：**上下文模块**（Context Module）与**迁移模块**（Migration Module）。  \n- **上下文模块**通过分析目标版本与参考版本之间的行为差异，动态构建上下文信息，捕捉故障症状及其相关的差异补丁片段（diff hunks）。  \n- 借助这些上下文信息，**迁移模块**利用基于大语言模型（LLM）的自适应机制，通过一个迭代反馈循环，平衡对差异候选方案的探索与逐步优化，从而高效解决漏洞复现失败的问题。\n\n我们在涵盖79个开源库、102个Java CVE漏洞及689个版本迁移任务的大规模数据集上评估了Diffploit。实验结果表明，Diffploit成功迁移了84.2%的漏洞利用，相较于改进建议感知的测试修复工具TARGET提升了52.0%，较基于规则的IDEA工具提高了61.6%。  \n\n除了技术上的优越表现，Diffploit还发现了5个CVE报告中受影响版本范围标注错误的情况，其中3个已获确认；同时，它还在GitHub安全公告数据库中识别出111个此前未报告的脆弱版本，显著增强了漏洞发现与管理的能力。"
  },
  {
    "date": "2025-11-17",
    "title": "Agent READMEs: An Empirical Study of Context Files for Agentic Coding",
    "authors": "Worawalan Chatlatanagulchai, Hao Li, Yutaro Kashiwa, Brittany Reid, Kundjanasith Thonglek, Pattara Leelaprute, Arnon Rungsawang, Bundit Manaskasemsak, Bram Adams, Ahmed E. Hassan, Hajimu Iida",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12884v1",
    "source": "arXiv",
    "abstract": "Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files (\"READMEs for agents\") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.",
    "title_zh": "代理READMEs：面向代理编程的上下文文件实证研究",
    "abstract_zh": "代理式编码工具接收以自然语言编写的任务目标作为输入，将其分解为具体任务，并在极少人工干预的情况下编写或执行实际代码。这一过程的核心是代理上下文文件（“代理的README”），它们提供持久的、项目级别的指令。本文首次对来自1,925个代码仓库的2,303份代理上下文文件进行了大规模实证研究，旨在刻画其结构、维护方式及内容特征。研究发现，这些文件并非静态文档，而是类似于配置代码般复杂且难以阅读的动态产物，通过频繁的小规模更新进行维护。我们对16种指令类型的分析表明，开发者更重视功能性的上下文信息，例如构建与运行命令（62.3%）、实现细节（69.9%）以及系统架构（67.7%）。然而，我们也发现一个显著的缺口：非功能性需求如安全（14.5%）和性能（14.5%）很少被明确说明。这些发现表明，尽管开发者利用上下文文件使代理具备功能性，但很少为其生成的代码提供保障安全或高性能的约束条件，凸显了改进工具链与开发实践的迫切需求。"
  },
  {
    "date": "2025-11-17",
    "title": "Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs",
    "authors": "Yongjoo Jang, Sangwoo Hwang, Hojin Lee, Sangwoo Jung, Donghun Lee, Wonbo Shim, Jaeha Kung",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.12860v1",
    "source": "arXiv",
    "abstract": "The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead.",
    "title_zh": "用于高效单批令牌生成的大型语言模型中3D NAND闪存PIM阵列的解构与重构",
    "abstract_zh": "大规模语言模型的发展使得模型参数量达到数十亿级别，显著增加了对内存和计算资源的需求。在传统硬件上部署此类模型面临挑战，主要受限于DRAM容量的瓶颈以及GPU成本高昂的问题。因此，本文提出将单批次token生成任务卸载至3D NAND闪存存内计算（PIM）设备上，利用其高存储密度突破DRAM容量限制。我们研究了多种3D NAND闪存配置，并设计了一种重构的PIM阵列结构，采用H树网络以实现最优延迟与单元密度的平衡。结合精心选择的PIM阵列尺寸，我们开发了适用于LLM各层的操作分块与映射方法，在性能上实现了相较于四块RTX4090使用vLLM的2.4倍加速，且与四块A100的性能相当，仅带来4.9%的延迟开销。详细的面积分析表明，所提出的3D NAND闪存PIM架构可在4.98mm²的芯片面积内集成于存储阵列下方，无需额外面积开销。"
  },
  {
    "date": "2025-11-17",
    "title": "SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents",
    "authors": "Rangeet Pan, Raju Pavuluri, Ruikai Huang, Rahul Krishna, Tyler Stennett, Alessandro Orso, Saurabh SInha",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13305v1",
    "source": "arXiv",
    "abstract": "Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.",
    "title_zh": "SAINT：基于程序分析与大语言模型代理的服务级集成测试生成",
    "abstract_zh": "企业应用通常在多个层级上进行测试，其中服务级别测试在验证应用功能方面发挥着重要作用。现有的服务级别测试工具，尤其是针对RESTful API的工具，往往依赖模糊测试（fuzzing）和/或OpenAPI规范，而这些规范在真实的企业代码库中并不容易获取。此外，这些工具在生成能够有效覆盖有意义场景的功能性测试方面能力有限。本文提出SAINT，一种针对企业Java应用的服务级别测试的新型白盒测试方法。SAINT结合了静态分析、大型语言模型（LLMs）以及基于LLM的智能体（agents），以自动生成功能端点和场景驱动的测试用例。该方法构建了两个关键模型：一是端点模型，用于捕捉服务端点的语法与语义信息；二是操作依赖图，用于刻画端点之间的调用顺序约束。随后，SAINT利用基于LLM的智能体来生成测试。以端点为中心的测试旨在最大化代码与数据库交互的覆盖率；而场景化测试则通过从代码中提取应用用例，并通过智能体循环中的规划（planning）、执行（action）和反思（reflection）三个阶段，将这些用例精炼为可执行的测试。我们在八个Java应用（包括一个专有的企业应用）上对SAINT进行了评估，结果表明SAINT在覆盖率、缺陷检测和场景生成方面均表现出色。此外，开发人员调查也对SAINT生成的场景化测试给予了高度认可。总体而言，本研究证明，将静态分析与基于智能体的LLM工作流相结合，能够实现更高效、更具功能性且更符合开发者实际需求的服务级别测试生成。"
  },
  {
    "date": "2025-11-17",
    "title": "Cost-Driven Synthesis of Sound Abstract Interpreters",
    "authors": "Qiuhan Gu, Avaljot Singh, Gagandeep Singh",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13663v1",
    "source": "arXiv",
    "abstract": "Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.",
    "title_zh": "基于成本驱动的声学抽象解释器综合",
    "abstract_zh": "构建能够提供全局正确性保证的抽象解释器，仍然是抽象解释领域面临的一大挑战。我们研究了现代大语言模型（LLM）是否能够通过在神经网络验证场景中，跨多个抽象域合成出既正确又非平凡的抽象解释器，从而减轻这一负担。我们将合成问题形式化为一个带约束的优化问题，并提出了一种新颖的、基于数学原理的成本函数，用于在严格的语法和语义约束下度量不正确性。基于这一形式化方法，我们开发了一个统一框架，该框架将基于LLM的生成与语法及语义验证相结合，并引入了一种定量成本引导的反馈机制。实验结果表明，我们的框架不仅在性能上达到了手工设计转换器的水平，更重要的是，成功发现了现有文献中尚未报道的、针对复杂非线性算子的正确且高精度的抽象解释器。"
  },
  {
    "date": "2025-11-17",
    "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
    "authors": "Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, Lingming Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2511.13646v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
    "title_zh": "实时SWE代理：软件工程代理能否即时自我进化？",
    "abstract_zh": "大型语言模型（LLMs）正在重塑几乎所有行业，包括软件工程领域。近年来，研究者提出了多种基于LLM的智能体（agents），以解决现实世界中的软件问题。这些软件智能体通常配备一系列编码工具，能够自主决定下一步行动，从而形成完整的执行轨迹，实现端到端的软件任务求解。尽管前景广阔，但这类智能体往往需要专门的设计，且性能可能仍不理想，因为全面探索智能体架构设计空间既极其困难又成本高昂。\n\n认识到软件智能体本质上也是软件，可以进一步优化和改进，研究人员最近提出了一系列具备自我提升能力的软件智能体，例如达尔文-哥德尔机器（Darwin-Gödel Machine, DGM）。然而，这些自进化智能体通常需要在特定基准上进行昂贵的离线训练，且在不同LLM或基准之间泛化能力有限。\n\n本文提出Live-SWE-agent，这是首个能够在运行时实时、持续地自主演化自身架构的“活”软件智能体，用于解决真实世界的软件问题。具体而言，Live-SWE-agent从最基础的智能体架构开始——仅具备对bash工具（如mini-SWE-agent）的访问权限——并在求解实际软件问题的过程中，自主演化其自身的架构实现。\n\n我们在广泛研究的SWE-bench Verified基准上进行了评估，结果表明，Live-SWE-agent在无需测试阶段扩展（test-time scaling）的情况下，达到了75.4%的惊人求解率，超越了所有现有的开源软件智能体，并接近最佳专有解决方案的性能。此外，在最新的SWE-Bench Pro基准上，Live-SWE-agent的表现也优于当前最先进的手动设计软件智能体，实现了45.8%的最佳已知求解率。"
  },
  {
    "date": "2025-11-17",
    "title": "Cycle-Accurate ML-Based Power Modeling for RISC-V Cores Using RTL Simulations",
    "authors": "Mekala Bindu Bhargavi, Rachakonda Komal Nayan, Sri Parameswaran, Soumya J",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235459",
    "source": "IEEE",
    "abstract": "Accurate and early-stage power modeling is critical for energy-aware design in modern processors, especially those based on the open-source RISC-V architecture. This work introduces a scalable and interpretable machine learning-based power modeling framework that enables cycle-accurate estimation at the microarchitectural level using RTL simulation data. Targeting an in-order 32-bit RISC-V core derived from the RI5CY architecture, the framework leverages switching activity traces from Verilator simulations to train lightweight decision tree regressors capable of predicting dynamic power with high fidelity. The proposed design pipeline includes structured feature extraction (e.g., toggle counts, Hamming distances) and hierarchical modeling across processor blocks such as Fetch, Decode, Execute, and Load Store (LS) Unit. Compared to state-of-the-art approaches, the proposed model achieves a Mean Absolute Error (MAE) of 2.06% at the full-core level, significantly outperforming prior works while reducing simulation overhead. Experimental results over ten diverse benchmarks demonstrate improved cycle efficiency, enhanced model generalization, and interpretability. These findings establish a practical pathway for integrating fast, explainable, and RTL-aligned power prediction models in early-stage RISC-V processor design workflows, bridging the gap between architectural exploration and physical power estimation.",
    "title_zh": "基于RTL仿真对RISC-V核心进行周期精确的机器学习功耗建模",
    "abstract_zh": "精确且早期的功耗建模对于现代处理器中的能耗感知设计至关重要，尤其是基于开源RISC-V架构的处理器。本文提出了一种可扩展且可解释的基于机器学习的功耗建模框架，该框架利用RTL仿真数据，在微架构层面实现周期级精确的功耗估算。针对源自RI5CY架构的有序32位RISC-V核心，该框架采用Verilator仿真生成的开关活动轨迹，训练轻量级决策树回归模型，以高保真度预测动态功耗。所提出的建模流程包括结构化的特征提取（如翻转次数、汉明距离）以及对处理器各功能模块（如取指、译码、执行和加载存储LS单元）的分层建模。与现有先进方法相比，该模型在全核级别实现了2.06%的平均绝对误差（MAE），显著优于以往工作，同时大幅降低了仿真开销。在十个不同基准程序上的实验结果表明，该模型具备更高的周期效率、更强的泛化能力以及良好的可解释性。这些成果为在RISC-V处理器早期设计流程中集成快速、可解释且与RTL对齐的功耗预测模型提供了切实可行的路径，弥合了架构探索与物理功耗估算之间的鸿沟。"
  },
  {
    "date": "2025-11-17",
    "title": "Model-Driven Generation of Executable Models for Hardware Specification Validation",
    "authors": "Robert Kunzelmann, Raymund Tonyka, Vinod Bangalore Ganesh, Raphael Kunz, Stephanie Ecker, Wolfgang Ecker",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231294",
    "source": "IEEE",
    "abstract": "Formal specification has been introduced to the development of modern digital hardware systems, alleviating the ambiguities and misinterpretations common with informal, natural language specifications. The Universal Specification Format (USF) is an existing approach to formal specification, which is used to prove the correctness of Register-Transfer Level (RTL) designs. Still, using such formal specifications as verification references requires validating that the specifications themselves are correct, which is often performed dynamically through usecase simulation. To fulfill the need for dynamic evaluation, we present a model-driven code generator that translates static USF specifications into untimed software simulations. The translation preserves hardware semantics by reordering operations and utilizing untimed, event-based interface handshakes. Our application to different hardware components, including a RISC-V processor, demonstrates the versatility and reusability of the code generation approach. Due to the abstract nature of the generated models, they execute between <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$1.5-4.5 \\times$</tex> faster than RTL simulations and can be constructed early in the design process without concrete timing information.",
    "title_zh": "基于模型驱动的硬件规范验证可执行模型生成",
    "abstract_zh": "形式化规范已被引入现代数字硬件系统的设计中，有效缓解了非形式化自然语言规范常见的模糊性和误解问题。现有的通用规范格式（USF）是一种形式化规范方法，用于证明寄存器传输级（RTL）设计的正确性。然而，在将此类形式化规范用作验证参考时，必须确保规范本身是正确的，而这一验证通常通过用例仿真进行动态实现。为满足动态评估的需求，本文提出了一种基于模型的代码生成器，可将静态的USF规范转换为无时间信息的软件仿真模型。该转换通过重新排序操作并采用无时间、基于事件的接口握手机制，保留了硬件语义。我们将该方法应用于多种硬件组件，包括RISC-V处理器，展示了代码生成方法的通用性和可重用性。由于生成模型具有抽象性，其执行速度比RTL仿真快1.5至4.5倍，并且可在设计早期无需具体时序信息的情况下构建。"
  },
  {
    "date": "2025-11-17",
    "title": "Systolic Array for Neural Network Acceleration",
    "authors": "Yizhen Liao, Chris Papachristou",
    "publish": "NAECON 2025 - IEEE National Aerospace and Electronics Conference",
    "url": "https://doi.org/10.1109/naecon65708.2025.11235370",
    "source": "IEEE",
    "abstract": "This paper presents a systolic MAC (Multiply-Accumulate) array architecture for accelerating neural network computations. We explore the design and implementation of scalable MAC array that efficiently computes matrix multiplications with extensive reuse of data stored in local MAC units. We demonstrate how this architecture can be extended for multilayer neural network processing through pipelined execution.",
    "title_zh": "用于神经网络加速的阵列",
    "abstract_zh": "本文提出了一种用于加速神经网络计算的脉动MAC（乘累加）阵列架构。我们探讨了可扩展MAC阵列的设计与实现，该阵列能够通过在本地MAC单元中大量重用数据，高效地完成矩阵乘法运算。同时，我们展示了如何通过流水线执行方式将该架构扩展应用于多层神经网络的处理。"
  },
  {
    "date": "2025-11-17",
    "title": "KRS Unleashed: Towards a Robotics FPGA Development Environment for Rapid Prototyping",
    "authors": "Paul Gottschaldt, Diana Goehringer",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231288",
    "source": "IEEE",
    "abstract": "Field-Programmable Gate Array System-on-Chip offer great potential for edge computing, yet their adoption remains limited due to complex development workflows. Despite progress in hardware design tools, deploying applications on these platforms still requires extensive manual effort across multiple stages-operating system integration, runtime configuration, and software framework adaptation. The original Kria Robotics Stack (KRS) partially alleviated this problem by integrating the Vitis Unified Flow into the Robot Operating System to ease the development of robotics applications. The solution is more proof-of-concept, with hidden, fixed configurations enabling reproducible builds of finalized systems. However, support for changing requirements during development is limited, resulting in low productivity. This work proposes KRS Unleashed <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">${ }^{1}$</tex>, an architectural redesign of KRS. By reducing dependencies and modularizing the framework into three separate workspaces, it improves productivity and fosters a parallel development workflow. Further extensions for the operating system and high-level synthesis development have been realized to provide an SDSoClike development experience for System-on-Chips, supporting integration beyond robotics through standard toolchains and extensible, modular components. Case studies for matrix multiplication and the AprilTag algorithm showcase productivity gains by reducing continuous overall build times during development by up to a factor of 80 by employing reusable intermediary artifacts.",
    "title_zh": "KRS 解放：面向快速原型设计的机器人FPGA开发环境",
    "abstract_zh": "现场可编程门阵列系统级芯片（FPGA SoC）在边缘计算领域具有巨大潜力，但由于开发流程复杂，其应用仍受到限制。尽管硬件设计工具已取得进展，但在这些平台上部署应用程序仍需在多个阶段投入大量手动工作——包括操作系统集成、运行时配置以及软件框架适配。原始的Kria机器人堆栈（KRS）通过将Vitis统一流程集成到机器人操作系统（ROS）中，部分缓解了这一问题，从而简化了机器人应用的开发。然而，该方案更偏向于概念验证，其隐藏且固定的配置虽然支持最终系统的可复现构建，但对开发过程中需求变更的支持能力有限，导致开发效率低下。本文提出KRS Unleashed<tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">${ }^{1}$</tex>，对KRS进行架构重构。通过减少依赖关系，并将框架模块化为三个独立的工作区，显著提升了开发效率，促进了并行开发流程。此外，还进一步扩展了操作系统和高层次综合（HLS）开发支持，为片上系统（SoC）提供了类似SDSoC的开发体验，通过标准工具链和可扩展、模块化的组件，实现了对机器人以外领域的良好支持。针对矩阵乘法和AprilTag算法的案例研究显示，通过复用中间产物，开发过程中的持续整体构建时间最高可缩短达80倍，显著提升了开发生产力。"
  },
  {
    "date": "2025-11-17",
    "title": "Overview of the Application of Generative AI in Software Performance Testing",
    "authors": "Alina Verkholomova, Erika Nazaruka",
    "publish": "2025 66th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)",
    "url": "https://doi.org/10.1109/itms67030.2025.11236704",
    "source": "IEEE",
    "abstract": "Generative Artificial Intelligence (AI) tools and Large Language Models (LLMs) were integrated into all stages of the Software Development Life Cycle (SDLC), influencing requirements gathering and analysis, design and planning, development, testing, deployment, maintenance and support, and documentation. Meanwhile, software performance testing is an essential type of software testing that ensures stable software behavior under various scenarios. This paper aims to point out the most common approaches in integrating Generative AI across the stages of software performance testing and in related software testing activities, as well as the main challenges and limitations faced by researchers. This research followed the systematic literature review approach to analyze and synthesize the existing studies on the application of Generative AI in software performance testing. Within the review, eight papers published between 2024 and 2025 in research literature databases, including ScienceDirect, SpringerLink, IEEE Xplore, and Google Scholar, were selected and analyzed. The review results reveal that the application of Generative AI is mainly concentrated in functional testing, specifically in test case generation, with a limited adoption in test scenario generation and capturing non-functional requirements. Key challenges identified include the inconsistency of generated output and hallucinations of LLMs. The findings indicate a significant research gap in applying Generative AI in the process of software performance testing.",
    "title_zh": "生成式AI在软件性能测试中的应用概述",
    "abstract_zh": "生成式人工智能（AI）工具和大型语言模型（LLMs）已融入软件开发生命周期（SDLC）的各个阶段，对需求收集与分析、设计与规划、开发、测试、部署、维护与支持以及文档编制等环节均产生了深远影响。与此同时，软件性能测试是确保软件在各种场景下行为稳定的重要测试类型。本文旨在指出生成式AI在软件性能测试各阶段及相关测试活动中的主要应用方法，以及研究人员面临的主要挑战与局限性。本研究采用系统文献综述法，对现有关于生成式AI在软件性能测试中应用的研究进行分析与综合。在综述过程中，共选取并分析了2024年至2025年间发表于ScienceDirect、SpringerLink、IEEE Xplore和Google Scholar等学术数据库中的8篇相关论文。研究结果表明，生成式AI的应用主要集中于功能测试，尤其是测试用例生成，而在测试场景生成及非功能性需求捕获方面的应用仍较为有限。识别出的关键挑战包括生成结果的一致性问题以及大语言模型的“幻觉”现象。研究发现，生成式AI在软件性能测试流程中的应用仍存在显著的研究空白。"
  },
  {
    "date": "2025-11-17",
    "title": "Comparative Analysis of Human-Designed vs AI-Generated USB4 Logical Layer: RTL Design and Verification",
    "authors": "Ali Ahmed Hassan, Karim Samy, Ahmed Zakaria, Ahmed Tarek, Walid Salah, Sief Hamdy Fadda, Hager Walid, Mohamed Salah Soliman",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235353",
    "source": "IEEE",
    "abstract": "Artificial Intelligence (AI) has revolutionized the way software engineers go by their day-to-day coding tasks. However, AI impact on hardware design and verification is still a territory that is yet to be thoroughly explored [1]. While there are some research papers that explore the significant advancements in AI-driven hardware designs [2], there is a lack of comparative studies that focus specifically on how these designs compare to hardware designs fully designed by expert human engineers. For this reason, this paper presents a detailed comparative analysis between human-written and AI-generated RTL by designing and verifying the USB4 Logical Layer, an integral part of the latest USB Protocol [3]. The complexity and the lack of publicly available USB4 architectures ensures that the used AI tools are fully relying on their own generative capabilities to produce good quality RTL without the usage of any public implementations which further solidifies the outcomes of this paper. The preliminary results indicate that AI-generated designs can significantly reduce design time while maintaining high levels of accuracy. Based on the findings, the research offers practical guidelines for integrating AI into the RTL design process. These include best practices for leveraging AI tools and common pitfalls to avoid.",
    "title_zh": "人设计与AI生成的USB4逻辑层：RTL设计与验证的对比分析",
    "abstract_zh": "人工智能（AI）已彻底改变了软件工程师日常编码任务的方式。然而，AI对硬件设计与验证的影响仍是一个尚未被充分探索的领域[1]。尽管已有部分研究论文探讨了AI驱动的硬件设计所取得的重大进展[2]，但目前缺乏专门针对这些设计与完全由资深人类工程师设计的硬件方案进行对比的系统性研究。因此，本文通过设计并验证USB4逻辑层——最新USB协议中的关键组成部分[3]，对人工编写与AI生成的RTL代码进行了详细的比较分析。由于USB4架构的复杂性以及公开可用实现的稀缺性，所使用的AI工具必须完全依赖其自身的生成能力来产出高质量的RTL代码，而无需参考任何公开的实现，这进一步增强了本文结论的可靠性。初步结果表明，AI生成的设计能够在显著缩短设计周期的同时，保持较高的准确性。基于这些发现，本研究提出了将AI融入RTL设计流程的实际指导建议，包括如何有效利用AI工具的最佳实践，以及应避免的常见陷阱。"
  },
  {
    "date": "2025-11-17",
    "title": "Efficient Prompt Design for Resource-Constrained Deployment of Local LLMs",
    "authors": "Aisvarya Adeseye, Jouni Isoaho, Seppo Virtanen, Mohammad Tahir",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231309",
    "source": "IEEE",
    "abstract": "The local deployment of Large Language Models (LLMs) is essential for privacy and latency in several domains. However, it faces significant challenges in terms of memory, power, and inference speed, particularly in resource-constrained systems such as Internet of Things (IoT) and edge computing devices. Most existing studies emphasize compression and hardware tuning, while holistic system-level optimization remains incomplete, and the role of prompt design is still underexplored. This study introduces a structured evaluation of prompt engineering strategies designed to enhance resource efficiency and accuracy in local LLMs, applied across three textual analysis tasks: theme extraction, frequency analysis, and impact analysis. Four experimental conditions were compared: Baseline, System Prompt Only (SP), User Prompt Only (UP), and System+User Prompt (<tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$S P+U P$</tex>). Using multiple LLMs ranging from 1 B to 70B parameters, we audited tokens generated, latency, VRAM usage, hallucination rates, and other structural errors. The results show that System Prompts alone substantially reduced computational overhead, whereas User Prompts improved accuracy and task alignment. Their combination yields comprehensive improvements, maximizing both efficiency and reliability. The proposed prompt design enabled smaller LLMs to rival larger ones in efficiency and accuracy, with LLaMA-3.2, 3B with SP+UP reducing VRAM by 96%, latency by 85%, and hallucinations by 83% when compared to the 70B with Baseline. Even LLaMA-3.2, 1B proved to be a viable option, especially when VRAM size is a critical factor.",
    "title_zh": "面向资源受限部署的本地大模型高效提示设计",
    "abstract_zh": "本地部署大型语言模型（LLMs）在多个领域中对于保障隐私和降低延迟至关重要。然而，在资源受限的系统（如物联网IoT和边缘计算设备）中，其面临内存、功耗和推理速度等方面的显著挑战。目前大多数研究侧重于模型压缩与硬件调优，而系统级的整体优化仍不完善，提示工程（prompt engineering）的作用也尚未得到充分探索。本研究提出了一种结构化的提示工程策略评估方法，旨在提升本地LLM在资源效率与准确性方面的表现，实验覆盖了主题提取、频率分析和影响分析三项文本分析任务。共设置了四种实验条件进行对比：基线（Baseline）、仅系统提示（SP）、仅用户提示（UP），以及系统+用户提示（SP+UP）。我们使用参数量从10亿到700亿不等的多种LLM进行了测试，评估了生成token数量、延迟、VRAM占用、幻觉率及其他结构性错误。结果表明，仅使用系统提示即可显著降低计算开销，而用户提示则有效提升了准确性和任务对齐度；两者的结合带来了全面的性能提升，实现了效率与可靠性的最大化。所提出的提示设计使小型LLM在效率和准确性上可媲美大型模型：例如，采用SP+UP策略的LLaMA-3.2 3B模型相比70B基线模型，VRAM占用降低96%，延迟减少85%，幻觉率下降83%。即使是最小的LLaMA-3.2 1B模型也展现出可行性，尤其在VRAM容量为关键限制因素时更具优势。"
  },
  {
    "date": "2025-11-17",
    "title": "Author",
    "authors": "N/A",
    "publish": "2025 Artificial Intelligence and Smart Technologies for Sustainability Conference (AISTS)",
    "url": "https://doi.org/10.1109/aists66100.2025.11232750",
    "source": "IEEE",
    "abstract": "Author.",
    "title_zh": "作者",
    "abstract_zh": "作者。"
  },
  {
    "date": "2025-11-17",
    "title": "MDDOAI: A Model-Driven DevOps Approach to CI/CD Automation",
    "authors": "Uldis Karlovs-Karlovskis, Oksana Nikiforova, Oscar Pastor, Kristijans Vēveris",
    "publish": "2025 66th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)",
    "url": "https://doi.org/10.1109/itms67030.2025.11236630",
    "source": "IEEE",
    "abstract": "Since its emergence in 2009, DevOps has significantly transformed software engineering by streamlining integration and delivery workflows. While prior research has extensively examined DevOps practices and their evolution, limited attention has been given to automated generation of Continuous Integration and Continuous Delivery (CI/CD) pipelines directly from high-level software architecture models. Addressing this gap, Model-Driven DevOps with AI (MDDOAI) introduces a model-to-code solution that bridges the abstraction gap between design and deployment. It leverages Atlas Transformation Language (ATL) for model-to-model transformation and Acceleo for code generation, enabling the automated synthesis of deployable CI/CD pipeline configurations. The resulting prototype demonstrates the feasibility of scalable, model-driven automation in DevOps, offering a structured foundation for future extensible and maintainable pipeline engineering.",
    "title_zh": "MDDOAI：一种面向CI/CD自动化的模型驱动DevOps方法",
    "abstract_zh": "自2009年出现以来，DevOps已显著改变了软件工程领域，通过优化集成与交付流程提升了效率。尽管以往研究已广泛探讨了DevOps实践及其演进，但针对如何直接从高层次软件架构模型自动生成功能完整的持续集成与持续交付（CI/CD）流水线，仍关注不足。为弥补这一空白，基于人工智能的模型驱动DevOps（MDDOAI）提出了一种“模型到代码”的解决方案，有效弥合了设计与部署之间的抽象鸿沟。该方法利用ATL（Atlas转换语言）实现模型到模型的转换，并借助Acceleo完成代码生成，从而实现可部署CI/CD流水线配置的自动化合成。所构建的原型验证了在DevOps中实现可扩展、模型驱动自动化的技术可行性，为未来可扩展、易维护的流水线工程提供了结构化基础。"
  },
  {
    "date": "2025-11-17",
    "title": "Interactive Transaction Tracker to Validate Data Integrity of the Compression and Decompression Engines in Neural Camera Applications",
    "authors": "Surajit Bhattacherjee, Dipankar Pal",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235369",
    "source": "IEEE",
    "abstract": "Data integrity is a key measure in signing-off mission-critical design modules like neural camera with compression and decompression engines. Such modules are directly associated with the safety and security of human-life in Automatic Driver Assistance System, remote surgery, security systems etc. The conventional methods namely, functional and formal verification partly fail to cover the end-to-end validation of the data path. Neural camera, signal processor, memory are independent components which interact without the knowledge of authenticity of incoming data. Different image formats with lossy and lossless compression are beyond the scope of functional verification. The proposed methodology offers a transaction tracker with metadata extractor to validate performance of neural camera network, data integrity and other criteria through entire transaction pipeline. The results show significant improvement in error detection, verification time and quality metrics compared to conventional methods. The experimental setup, results and conclusions have been derived with a special reference to automotive use-case but the same can also be implemented over any neural camera application.",
    "title_zh": "用于验证神经相机应用中压缩与解压引擎数据完整性的交互式交易追踪器",
    "abstract_zh": "数据完整性是签核关键设计模块（如具备压缩与解压缩引擎的神经相机）时的一项重要指标。此类模块直接关系到自动驾驶辅助系统、远程手术、安防系统等场景中的人身安全与信息安全。传统的功能验证和形式化验证方法在覆盖数据路径的端到端验证方面存在不足。神经相机、信号处理器、存储器等组件各自独立运行，彼此交互时并不知晓输入数据的真实性。不同图像格式所采用的有损或无损压缩技术，也超出了功能验证的范畴。本文提出的方法通过引入事务跟踪器与元数据提取器，能够对神经相机网络的性能、数据完整性及其他标准进行全流程事务管道的验证。实验结果表明，相较于传统方法，该方法在错误检测、验证时间以及质量度量等方面均有显著提升。实验设置、结果分析及结论均以汽车应用场景为参考，但该方法同样可推广应用于其他神经相机应用。"
  },
  {
    "date": "2025-11-17",
    "title": "Energy-Efficient Llama3 Acceleration on a CGLA by Offloading Computational Kernels",
    "authors": "Takuto Ando, Yu Eto, Ayumu Takeuchi, Yasuhiko Nakashima",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235407",
    "source": "IEEE",
    "abstract": "The evolution of Large Language Models (LLMs) has led to a substantial increase in energy consumption, making energy-efficient accelerator technologies essential. We propose a method to maximize the performance of the Llama3 LLM on IMAX3, a CGRA-based accelerator. We conducted a detailed analysis of the Q3_K_S quantization method. Based on this analysis, we implemented a new custom kernel to offload the Q6_K operations to the hardware. In previous work, this operation was processed on the host CPU. This targeted optimization reduced the end-to-end latency by approximately 11%. Furthermore, an evaluation of a projected ASIC implementation showed that the absolute performance does not match that of a GPU. In contrast, our approach achieves superior power efficiency, as measured by the Power-Delay Product (PDP).",
    "title_zh": "通过将计算内核卸载到CGLA实现Llama3的能效加速",
    "abstract_zh": "大型语言模型（LLMs）的演进导致能耗显著增加，因此亟需高效节能的加速器技术。本文提出一种方法，旨在最大化Llama3模型在基于CGRA架构的IMAX3加速器上的性能表现。我们对Q3_K_S量化方法进行了深入分析，并基于该分析结果，实现了一种新型自定义内核，将原本由主机CPU处理的Q6_K运算操作卸载至硬件执行。这一针对性优化使端到端延迟降低了约11%。此外，对预期ASIC实现的评估表明，其绝对性能尚不及GPU；而相比之下，我们的方案在能效方面表现出色，具体体现为更低的功耗-延迟乘积（Power-Delay Product, PDP）。"
  },
  {
    "date": "2025-11-17",
    "title": "Towards Achieving Vertical Reuse in SoC-Level Verification",
    "authors": "Petr Bardonek, Alessandra Dolmeta, Marcela Zachariášová, Guido Masera",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231287",
    "source": "IEEE",
    "abstract": "Recent trends in AI-assisted design and new approaches to accelerate overall design processes are contributing to the increasing complexity of System-on-Chip (SoC) designs. Unfortunately, advancement in verification methods has not kept pace, leading to a widening verification gap. This gap underscores the growing importance of reusing verification components. Although reusing components at the testbench level is possible, the portability of verification tests and scenarios in complex SoC integrations remains inadequate and underexplored. The Portable Stimulus Standard (PSS) aims to enhance reuse through a model-based approach, where Portable Models (PMs) capture the intent of verification tests in a significantly more abstract manner. Although transitioning to this higher level of abstraction is promising, it still entails a considerable manual burden, even when utilizing PSS. PMs connect to the Design Under Verification (DUV) via realization-layer bindings, which may vary as one moves from a module-level to a SoC-level context. The authors propose an automation approach, represented in their unique toolchain based on static analysis, that facilitates the binding of modular PMs to a top-level PM. This paper demonstrates the application of this toolchain in a full SoC integration context, using a Keccak cryptographic accelerator integrated through both loosely coupled and tightly coupled architectures as the DUV. The study illustrates that static analysis can support vertical reuse across multiple design hierarchy levels. However, as integration reaches the SoC scale, it increasingly intersects with software-driven control and structural encapsulation, necessitating additional modeling efforts. These findings define the limitations of structural automation for vertical reuse and outline directions for enhancing methodologies and tool support for SoCscale verification.",
    "title_zh": "实现SoC级验证中的垂直复用",
    "abstract_zh": "近年来，人工智能辅助设计的新趋势以及加速整体设计流程的创新方法，正推动系统级芯片（SoC）设计日趋复杂。然而，验证方法的进步却未能跟上这一发展步伐，导致验证差距不断拉大。这一现象凸显了复用验证组件的重要性。尽管在测试平台层面实现组件复用是可行的，但在复杂的SoC集成中，验证测试与场景的可移植性仍然不足，且研究尚不充分。为解决这一问题，可移植刺激标准（PSS）采用基于模型的方法，通过可移植模型（PMs）以更高层次的抽象方式捕获验证测试的意图。虽然向更高抽象层次过渡具有广阔前景，但即便使用PSS，仍需承担相当大的人工工作量。PMs通过实现层绑定与待验证设计（DUV）相连，而这些绑定在从模块级到SoC级的上下文中可能发生变化。本文作者提出了一种自动化方法，并构建了基于静态分析的独特工具链，以支持将模块化PM与顶层PM进行绑定。该论文展示了该工具链在完整SoC集成环境中的应用，以通过松耦合和紧耦合架构集成的Keccak密码加速器作为DUV。研究结果表明，静态分析能够支持跨多个设计层级的垂直复用。然而，随着集成规模达到SoC级别，其日益与软件驱动的控制逻辑及结构封装相交织，因此需要额外的建模工作。这些发现揭示了结构化自动化在实现垂直复用方面的局限性，并指明了未来提升SoC级验证方法学与工具支持的方向。"
  },
  {
    "date": "2025-11-17",
    "title": "Hardware Trustworthiness Architecture based on Resilience",
    "authors": "Francis Wolff, William McQuay, Chris Papachistou, William S. Clay",
    "publish": "NAECON 2025 - IEEE National Aerospace and Electronics Conference",
    "url": "https://doi.org/10.1109/naecon65708.2025.11235317",
    "source": "IEEE",
    "abstract": "The trustworthiness of third-party intellectual property design blocks before and after fabricating in a foundry makes the hardware trojan detection problem challenging. The multiplicity of hardware trojans will become commonplace due to the growing complexity of integrated circuits. A trustworthiness architecture based on resilience, redaction and reliability techniques is presented.",
    "title_zh": "基于韧性的硬件可信性架构",
    "abstract_zh": "在代工厂制造前后，第三方知识产权设计模块的可信性使得硬件木马检测问题变得极具挑战性。随着集成电路复杂度的不断提高，硬件木马的多样性将变得普遍。为此，本文提出了一种基于弹性、删减和可靠性技术的可信架构。"
  },
  {
    "date": "2025-11-17",
    "title": "Simopt-Power: Leveraging Simulation Metadata for Low-Power Design Synthesis",
    "authors": "Eashan Wadhwa, Shanker Shreejith",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231274",
    "source": "IEEE",
    "abstract": "Excessive switching activity is a primary contributor to dynamic power dissipation in modern FPGAs, where fine-grained configurability amplifies signal toggling and associated capacitance. Conventional low-power techniques – gating, clockdomain partitioning, and placement-aware netlist rewrites – either require intrusive design changes or offer diminishing returns as device densities grow. In this work, we present Simopt-power, a simulator-driven optimisation framework that leverages simulation analysis to identify and selectively reconfigure high-toggle paths. By feeding activity profiles back into a lightweight transformation pass, Simopt-power judiciously inserts duplicate truth table logic using Shannon Decomposition principle and relocates critical nets, thereby attenuating unnecessary transitions without perturbing functional behaviour. We evaluated this framework on open-source RTLLM benchmark, with Simopt-power achieves an average switching-induced power reduction of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\approx 9 \\%$</tex> while incurring only <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\approx 19 \\%$</tex> additional LUT-equivalent resources for arithmetic designs. These results demonstrate that coupling simulation insights with targeted optimisations can yield a reduced dynamic power, offering a practical path toward using simulation metadata in the FPGA-CAD flow.",
    "title_zh": "Simopt-Power：利用仿真元数据实现低功耗设计综合",
    "abstract_zh": "过度的切换活动是现代FPGA中动态功耗的主要贡献因素，其中细粒度的可配置性加剧了信号翻转及其相关电容。传统的低功耗技术——如门控、时钟域划分以及面向布局的网表重写——要么需要侵入性的设计修改，要么随着器件密度的增加而收益递减。本文提出Simopt-power，一种基于仿真驱动的优化框架，利用仿真分析识别并选择性地重构高翻转路径。通过将活动性分析结果反馈至轻量级变换模块，Simopt-power巧妙地运用香农分解原理插入重复的真值表逻辑，并重新定位关键网络，从而在不改变功能行为的前提下抑制不必要的翻转。我们在开源的RTLLM基准测试上评估了该框架，结果显示，Simopt-power在算术设计中实现了平均约9%的开关功耗降低，同时仅增加了约19%的LUT等效资源开销。这些结果表明，将仿真洞察与针对性优化相结合，能够有效降低动态功耗，为在FPGA-CAD流程中利用仿真元数据提供了一条切实可行的路径。"
  },
  {
    "date": "2025-11-17",
    "title": "AI-Assisted Transformation of the Two-Hemisphere Model into Structured IT Project Backlog",
    "authors": "Oksana Nikiforova, Rihards Bobkovs, Oscar Pastor",
    "publish": "2025 66th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)",
    "url": "https://doi.org/10.1109/itms67030.2025.11236542",
    "source": "IEEE",
    "abstract": "This paper presents an AI-assisted approach for transforming the two-hemisphere model of IT project initiation into structured product backlog with correspondence to Agile software development. The proposed solution enables the automatic generation of IT project scope documentation and task lists based on user stories extracted from the problem domain process model, as well as create the problem domain UML use case diagram from the two-hemisphere model. By analyzing the application of the Agile SCRUM methodology to task organization, the research defines transformation rules for converting process elements into backlog items. Large Language Model is integrated into the solution to automate task generation, prioritization, and decomposition. The resulting data can be seamlessly imported into project management tools such as Jira and Leantime. Emphasis is placed on structured data representation, automated project backlog generation and tool integration.",
    "title_zh": "人工智能辅助的双半球模型向结构化IT项目待办事项列表的转化",
    "abstract_zh": "本文提出了一种人工智能辅助方法，将IT项目启动的双半球模型转化为与敏捷软件开发相对应的结构化产品待办列表。该解决方案能够基于从问题领域过程模型中提取的用户故事，自动生成IT项目范围文档和任务清单，并从双半球模型中创建问题领域的UML用例图。通过分析敏捷Scrum方法在任务组织中的应用，研究定义了将过程元素转换为待办事项的转换规则。大型语言模型被集成到该方案中，以实现任务的自动生成、优先级排序和分解。生成的数据可无缝导入Jira、Leantime等项目管理工具。研究重点在于结构化数据表示、自动化产品待办列表生成以及工具集成。"
  },
  {
    "date": "2025-11-17",
    "title": "PanicFI: An Infrastructure for Fixing Panic Bugs in Real-World Rust Programs",
    "authors": "Yunbo Ni, Zixi Liu, Yang Feng, Runtao Chen, Baowen Xu",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3773991",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "PanicFI：一种用于修复真实世界 Rust 程序中 panic 错误的基础设施",
    "abstract_zh": "None"
  },
  {
    "date": "2025-11-17",
    "title": "Optimizing Embedded Software Platforms Development: A Multi-Stage MDA-Driven Approach to Firmware Generation for Multiple Programming Languages",
    "authors": "Raphael Kunz, Lijun Chen, Stephanie Ecker, Yash Ranjan, Paritosh Kumar Sinha, Wolfgang Ecker",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231316",
    "source": "IEEE",
    "abstract": "This paper presents a Model-Driven Architectureinspired generation methodology for auto-generating C and Rust firmware, enabling the development of complex embedded cross-language systems. The proposed approach leverages the combined power of existing software generators to generate firmware, ensuring interoperability with existing C-based systems while allowing for a gradual migration towards modern Rust-based systems. This enables an improved support of the safety features available in Rust, compared to an interfaced C-code called from Rust, which cannot provide these guarantees. The paper provides an overview of the fundamental building blocks of the approach and discusses the challenges and advantages of chaining both generators. Our Halstead measurments highlight the advantages of utilizing this Model-Driven approach in reduction of manual effort and the risk. This reduction is comparable to the results found in state-of-the-art literature, as highlighted by the comparison presented in this paper, but provides dual-language embedded code. This paper forms another building block towards generating hardware-dependent complex software platforms.",
    "title_zh": "优化嵌入式软件平台开发：一种多阶段MDA驱动的固件生成方法，适用于多种编程语言",
    "abstract_zh": "本文提出了一种受模型驱动架构（Model-Driven Architecture）启发的生成方法，用于自动生成C语言和Rust语言的固件，从而支持复杂嵌入式跨语言系统的开发。所提出的方案充分利用现有软件生成工具的协同能力，生成可与现有基于C语言系统互操作的固件，同时支持逐步向现代基于Rust的系统迁移。相较于从Rust调用接口C代码的方式，该方法能够更好地利用Rust所提供的安全特性，后者无法提供同等的安全保障。本文概述了该方法的基本构成模块，并讨论了串联使用两种生成器所带来的挑战与优势。Halstead度量结果表明，采用这种模型驱动方法显著降低了人工工作量和潜在风险，其效果与当前最先进文献中的研究成果相当。此外，本方法还实现了双语言嵌入式代码的生成。本研究为构建依赖硬件的复杂软件平台又添重要一环。"
  },
  {
    "date": "2025-11-17",
    "title": "Accelerate On-Chip Artificial Neural Network Training: a Customised fNIRS Signals Processing System-on-Chip",
    "authors": "Yunyi Zhao, Hubin Zhao, Shufan Yang",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235415",
    "source": "IEEE",
    "abstract": "During artificial neural network (ANN) training, batch normalisation (BN) techniques improve training performance algorithmically but create hardware implementation challenges. This paper aims to address this specific bottleneck within the broader challenge of on-chip training. We propose an all-onchip artificial neural network (ANN) training framework that utilises AMD’s Versal XCVC1902 heterogeneous SoC with a purpose-built batch-normalisation (BN) accelerator to overcome the two main barriers to edge computing: memory bandwidth and batch normalisation layer latency. By hardwiring batch normalisation calculation to form a hardware primitive and using fixed-point arithmetic, the design improves batch normalisation latency from tens of cycles to nine cycles, improving the efficiency of the ANN training process. The hardware-software co-design strategy demonstrates how deep processing unit cores, BN engine based on FPGA fabric, and CPUs can be orchestrated through a unifying abstraction, pointing toward application-specific SoCs that hide low-level scheduling from machine learning developers. The framework is evaluated as an application of real-time functional near-infrared spectroscopy (fNIRS) signal reconstruction, where cloud off-loading is undesirable for privacy and latency reasons. Although evaluated on fNIRS, the tiled batch normalisation engine and zero-copy dataflow can be reused in any ANN training that spends a significant fraction of time in batch normalisation layers.",
    "title_zh": "加速片上人工神经网络训练：一种定制化的fNIRS信号处理片上系统",
    "abstract_zh": "在人工神经网络（ANN）训练过程中，批归一化（Batch Normalization, BN）技术虽能从算法层面提升训练性能，却带来了硬件实现上的挑战。本文旨在解决这一特定瓶颈，以应对片上训练（on-chip training）这一更广泛问题中的关键障碍。我们提出了一种全片上人工神经网络（ANN）训练框架，采用AMD Versal XCVC1902异构系统级芯片（SoC），并配备专用的批归一化（BN）加速器，从而克服边缘计算中的两大主要障碍：内存带宽限制和批归一化层延迟。通过将批归一化计算固化为硬件原语，并结合定点数运算，该设计将批归一化处理延迟从数十个周期降低至九个周期，显著提升了ANN训练的整体效率。该软硬件协同设计策略展示了如何通过统一抽象，协调深度处理单元核心、基于FPGA结构的BN引擎以及CPU之间的协作，为面向特定应用的SoC发展指明方向——即让机器学习开发者无需关注底层调度细节。本框架被应用于实时功能近红外光谱（fNIRS）信号重建场景，由于隐私保护和低延迟需求，云端卸载在此类应用中并不理想。尽管评估聚焦于fNIRS，但所提出的分块批归一化引擎与零拷贝数据流架构可广泛复用于任何在批归一化层耗费大量时间的ANN训练任务中。"
  },
  {
    "date": "2025-11-17",
    "title": "Programmable and Adaptive Scheduling for Distributed Systems",
    "authors": "Yuyao Wang, Xiangfeng Zhu, Ratul Mahajan, Stephanie Wang",
    "publish": "Proceedings of the 24th ACM Workshop on Hot Topics in Networks",
    "url": "https://doi.org/10.1145/3772356.3772391",
    "source": "ACM",
    "abstract": "Existing frameworks for managing distributed systems hard-code scheduling policies and their implementations (e.g., centralized vs. decentralized), limiting customization and hurting performance across diverse applications and workloads. We argue for an adaptive scheduling approach, where developers express policies in a high-level, framework-agnostic DSL, and a compiler generates optimized implementations based on policy semantics, workload characteristics, and execution environments. We demonstrate that our compiler-guided approach can significantly improve both scheduling quality and performance.",
    "title_zh": "分布式系统中的可编程与自适应调度",
    "abstract_zh": "现有管理分布式系统的框架将调度策略及其具体实现（如集中式与去中心化）硬编码其中，限制了定制化能力，并在不同应用和工作负载下影响性能。我们主张采用一种自适应的调度方法：开发者使用高层、框架无关的领域特定语言（DSL）表达调度策略，由编译器根据策略语义、工作负载特征和执行环境生成优化的实现。我们证明，这种由编译器引导的方法能够显著提升调度质量与系统性能。"
  },
  {
    "date": "2025-11-17",
    "title": "Rust for Safety and Security Critical Systems",
    "authors": "Malte Munch, Marcus Lindner, Johan Eriksson, Pawel Dzialo, Per Lindgren",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231205",
    "source": "IEEE",
    "abstract": "Rust is a modern systems level language with built in safety features providing strong guarantees to memory safety and defined behavior. In this paper we identify challenges and opportunities for adopting Rust in the context of safety-critical systems. In particular we focus on software safety and security requirements akin to ISO 26262, IEC 61508, and ISO/SAE 21434. We discuss Rust based development in relation to commonplace C/C++ software design and validation processes. Furthermore we compare two distinct system categories: hosted and bare-metal systems and which opportunities and challenges developers face in the context of safety and security certification in each of them.",
    "title_zh": "用于安全与安全关键系统的 Rust",
    "abstract_zh": "Rust 是一种现代的系统级编程语言，内置了安全特性，能够提供对内存安全和行为确定性的强有力保障。本文探讨了在安全关键系统背景下采用 Rust 所面临的挑战与机遇。特别地，我们关注与 ISO 26262、IEC 61508 以及 ISO/SAE 21434 等标准相关的软件安全与安全要求。文章讨论了基于 Rust 的开发方法与常见的 C/C++ 软件设计与验证流程之间的关系。此外，我们还对比了两类不同的系统架构：托管系统（hosted systems）与裸机系统（bare-metal systems），并分析了开发者在这两类系统中进行安全与安全认证时所面临的不同机遇与挑战。"
  },
  {
    "date": "2025-11-17",
    "title": "Automated MATLAB™-to-HLS Conversion and Exploration for DoD ASIC/FPGA Development",
    "authors": "Michael Parker, Kirk Ober, Michael Bruennert",
    "publish": "NAECON 2025 - IEEE National Aerospace and Electronics Conference",
    "url": "https://doi.org/10.1109/naecon65708.2025.11235456",
    "source": "IEEE",
    "abstract": "While relatively new when compared to more traditional RTL design entry methods, high-level synthesis now has two decades of successful tapeouts. However, its use with MATLAB was a manual process. Many HLS designs begin as MATLAB models which must first be rewritten by hand in SystemC before they can be turned into hardwareThis paper recounts our experience using a new automated flow available in MATLAB’s HDL Coder app. With this flow, we automatically converted our MATLAB code files into SystemC modules and ran them through the Cadence<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">©</sup> Stratus™ High-Level Synthesis (HLS) tool. We also used Cadence<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">©</sup> Cerebrus™ Intelligent Chip Explorer, a machine learning tool that works with Stratus HLS, to define an exploration space and let it find the gate-level result with the very best area and power. With this approach, we ultimately exceeded the PPA results for an earlier handwritten, manually explored SystemC version of the same design.",
    "title_zh": "面向美国国防部ASIC/FPGA开发的MATLAB™到HLS自动化转换与探索",
    "abstract_zh": "尽管与更传统的RTL设计输入方法相比，高层次综合（HLS）仍属较新的技术，但其已有二十年的成功流片经验。然而，过去在MATLAB中使用HLS时，整个流程是手动完成的。许多HLS设计最初以MATLAB模型的形式存在，必须先由人工重写为SystemC代码，才能进一步转化为硬件。本文回顾了我们使用MATLAB HDL Coder应用程序中一种新型自动化流程的经验。通过该流程，我们能够将MATLAB代码文件自动转换为SystemC模块，并将其导入Cadence® Stratus™ 高层次综合（HLS）工具进行处理。此外，我们还利用Cadence® Cerebrus™ 智能芯片探索工具——一个与Stratus HLS协同工作的机器学习工具——定义了设计探索空间，并让系统自动寻找在面积和功耗方面表现最优的门级实现结果。采用这一方法后，我们最终获得了优于早期手工编写、手动探索的SystemC版本设计的PPA（性能、功耗、面积）指标。"
  },
  {
    "date": "2025-11-17",
    "title": "A DNN-Oriented Rapid Simulation Framework for Digital Processing-In-Memory Architectures",
    "authors": "Yirong Kan, Guangxian Zhu, Renyuan Zhang, Yasuhiko Nakashima",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235475",
    "source": "IEEE",
    "abstract": "Processing-In-Memory (PIM) architectures show great potential in accelerating deep neural networks (DNNs). Although PIM architectures based on various memory devices have been developed, it is challenging to rapidly estimate performance of chips in the early design stage. In this paper, we propose a modular and parameterized simulation framework DDSim, for design and rapid estimation of digital PIM architectures. The proposed framework supports fast simulation of PIM architectures at three levels: chip level, tile level, and array level, providing various trade-offs between speed and accuracy. To enable rapid simulation, the framework allow users to customize the parameters of the PIM architecture with pre-synthesis logic units, instead of estimating hardware performance at the transistor level every time. In addtion, the framework is tightly coupled with Pytorch-based DNN framework to enable automatic extraction and mapping of DNN models on digital PIM architectures. We demonstrate the benchmark performance of the proposed framework on several typical DNN models to prove its ability to fast simulate digital PIM architectures.",
    "title_zh": "面向深度神经网络的数字存内计算架构快速仿真框架",
    "abstract_zh": "存内计算（Processing-In-Memory, PIM）架构在加速深度神经网络（DNN）方面展现出巨大潜力。尽管已开发出基于多种存储器件的PIM架构，但在设计初期快速估算芯片性能仍具挑战性。本文提出了一种模块化且参数化的仿真框架DDSim，用于数字PIM架构的设计与快速性能评估。该框架支持在芯片级、Tile级和阵列级三个层次上进行快速仿真，提供了速度与精度之间的多种权衡选择。为实现快速仿真，该框架允许用户通过预综合逻辑单元自定义PIM架构参数，而无需每次均在晶体管级别估算硬件性能。此外，该框架与基于PyTorch的DNN框架紧密集成，可实现对DNN模型的自动提取与映射到数字PIM架构。我们通过多个典型DNN模型的基准测试，验证了所提框架在快速仿真数字PIM架构方面的有效性。"
  },
  {
    "date": "2025-11-17",
    "title": "Multi-Tool Static Code Analysis for Code Smell and Bug Detection: A Case Study on the Java-Design-Patterns Repository",
    "authors": "Kenny Aldi, Rheno Septianto, Ragil Yulianto, Ryeisha Taskia",
    "publish": "2025 International Conference on Information Technology Research and Innovation (ICITRI)",
    "url": "https://doi.org/10.1109/icitri67507.2025.11232802",
    "source": "IEEE",
    "abstract": "Code smells are indicators of potential problems in software source code that may hinder maintainability, increase complexity, and elevate the likelihood of future defects. This paper presents a case study on the identification and analysis of code smells in the java-designpatterns open-source project hosted on GitHub. Three static analysis tools-SonarQube, PMD, and SpotBugs-were employed to provide a comprehensive evaluation of code quality from different perspectives. SonarQube offered a macro-level overview including maintainability scores and technical debt; PMD identified rule-based structural violations; and SpotBugs revealed bytecode-level runtime bugs. The analysis revealed 899 code smells, 125 bugs, and zero test coverage, highlighting critical areas for improvement. Additionally, differences among tool outputs emphasized the need for a multi-tool strategy to achieve a holistic and accurate assessment. The findings contribute to software quality assurance practices by demonstrating the complementary nature of these tools and offering actionable recommendations for refactoring and code auditing.",
    "title_zh": "多工具静态代码分析在代码异味与缺陷检测中的应用：以Java设计模式仓库为例的案例研究",
    "abstract_zh": "代码异味是软件源代码中潜在问题的指示信号，可能影响代码的可维护性，增加复杂度，并提高未来缺陷出现的可能性。本文以GitHub上托管的java-designpatterns开源项目为案例，研究了代码异味的识别与分析。研究采用了三种静态分析工具——SonarQube、PMD和SpotBugs，从不同角度对代码质量进行了全面评估：SonarQube提供了包括可维护性评分和技术债务在内的宏观视图；PMD识别出基于规则的结构违规；SpotBugs则揭示了字节码层面的运行时缺陷。分析结果显示共发现899个代码异味、125个缺陷，且测试覆盖率为零，凸显了亟需改进的关键领域。此外，各工具输出结果之间的差异也强调了采用多工具策略的重要性，以实现全面而准确的评估。研究结果有助于提升软件质量保障实践，展示了这些工具之间的互补性，并为重构和代码审计提供了可操作的建议。"
  },
  {
    "date": "2025-11-17",
    "title": "Next-Gen Physical Design with AI",
    "authors": "Bindu P Rao, Jagadeesh Gnanasekaran, Prasenjit Ray, V Sai Prashant, Anand Kumaraswamy",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235361",
    "source": "IEEE",
    "abstract": "The Artificial Intelligence (AI) era has significantly increased the demand for semiconductor chips, driving the need for highly optimized designs that balance performance, power, area, schedule and cost (PPASC). AI techniques can augment traditional physical design methods to achieve highly optimized designs through multi-objective exploration, data-driven predictions of outcomes, and accelerate design convergence with clustering and pattern matching solutions. This paper shows the value-add of AI techniques for recipe generation, clock parameter selection, power-performance optimization, at-scale design robustness analysis, and design convergence. Results show AI improves PPASC, design quality and resource efficiency, leading to optimally designed chips.",
    "title_zh": "下一代物理设计与人工智能",
    "abstract_zh": "人工智能（AI）时代显著提升了对半导体芯片的需求，推动了在性能、功耗、面积、进度和成本（PPASC）之间实现平衡的高效优化设计。AI技术能够增强传统的物理设计方法，通过多目标探索、基于数据驱动的结果预测，以及利用聚类和模式匹配方案加速设计收敛，从而实现高度优化的设计。本文展示了AI技术在工艺配方生成、时钟参数选择、功耗-性能优化、大规模设计鲁棒性分析以及设计收敛方面的价值提升。实验结果表明，AI技术能够有效改善PPASC指标，提升设计质量与资源利用效率，最终实现最优的芯片设计。"
  },
  {
    "date": "2025-11-17",
    "title": "Edge-Enabled Intelligent Framework for Cyber Threat Classification and Adaptive Anomaly Mitigation",
    "authors": "Priya Deshmukh, Varun Kowndinya Shankar Prasad, Venkataramani Kumar",
    "publish": "NAECON 2025 - IEEE National Aerospace and Electronics Conference",
    "url": "https://doi.org/10.1109/naecon65708.2025.11235320",
    "source": "IEEE",
    "abstract": "With the evolution of advanced next-generation applications the need to meet the low-latency requirements is necessary. By processing the information at edge, it is feasible to expedite the response. However, the edge processing presents cybersecurity threats. Most of the existing works focus on resolving one aspect such as anomaly detection, or adaptive learning. However, through our work we propose an edge-enabled intelligent framework that offers three main functionalities namely threat level classification, response code prediction, and anomaly score prediction. The proposed monolithic framework is validated on open-source dataset with numerous features. Preliminary evaluation results highlight the significance of our framework.",
    "title_zh": "面向边缘的智能框架：用于网络威胁分类与自适应异常缓解",
    "abstract_zh": "随着下一代先进应用的不断发展，满足低延迟需求变得至关重要。通过在边缘端处理信息，可以有效加快响应速度。然而，边缘计算也带来了网络安全威胁。目前大多数研究工作仅关注单一方面，如异常检测或自适应学习。而通过本研究，我们提出了一种面向边缘的智能框架，具备三大核心功能：威胁等级分类、响应代码预测以及异常评分预测。所提出的统一框架已在包含大量特征的开源数据集上进行了验证，初步评估结果凸显了该框架的重要价值。"
  },
  {
    "date": "2025-11-17",
    "title": "On Enhancing the Security Against Memory Disclosure Attacks",
    "authors": "Prokash Ghosh, Gaurav Kumar, Sohan Lal, Satyadev Ahlawat, Virendra Singh",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235448",
    "source": "IEEE",
    "abstract": "Main memory is a critical component for storing computational data across various applications, including highly sensitive information such as banking transactions, encryption keys, and authentication credentials. However, memory systems are susceptible to side-channel attacks, making it imperative to implement effective countermeasures against unauthorized data extraction. Additionally, counterfeit memory controllers and memory devices pose a significant security risk, as they can facilitate data leakage even in the presence of conventional encryption mechanisms. Traditional security solutions, such as AES encryption integrated into memory controllers, provide strong cryptographic protection; however, they introduce substantial latency and area overhead. This paper presents a novel security countermeasure that ensures secure communication between the memory controller and the memory device by employing XOR-based encryption and decryption at both ends. The proposed technique remains secure even in the presence of unauthenticated memory components, preventing unauthorized data access. Experimental evaluations demonstrate that the proposed approach achieves a high level of security (2<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">512</sup>) while significantly reducing area overhead and incurring no additional access latency.",
    "title_zh": "提升对内存泄露攻击的防御能力",
    "abstract_zh": "主存储器是存储各类应用中计算数据的关键组件，包括银行交易、加密密钥和身份验证凭据等高度敏感的信息。然而，内存系统容易受到旁道攻击的影响，因此必须实施有效的防护措施以防止未经授权的数据提取。此外，假冒的内存控制器和内存设备也构成了重大安全威胁，即使在采用传统加密机制的情况下，仍可能导致数据泄露。传统的安全解决方案，如集成在内存控制器中的AES加密，虽然提供了强大的密码学保护，但会引入显著的延迟和面积开销。本文提出了一种新型安全对策，通过在内存控制器与内存设备两端均采用基于XOR的加密与解密技术，确保两者之间的通信安全。该方法即使在存在未认证内存组件的情况下依然保持安全性，有效防止未经授权的数据访问。实验评估表明，所提方案在实现极高安全性（2<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">512</sup>）的同时，显著降低了面积开销，并且不增加任何额外的访问延迟。"
  },
  {
    "date": "2025-11-17",
    "title": "Performance Evaluation of Flan-T5 in CGLA based Accelerators",
    "authors": "Yu Eto, Takuto Ando, Yasuhiko Nakashima",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235382",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) impose significant computational and energy demands, posing challenges for deployment on edge devices. To address this, we explore the feasibility of executing Flan-T5 an instruction tuned encoder decoder LLM on IMAX3, an in memory accelerator based on a Coarse Grained Linear Array (CGLA). By leveraging existing custom instructions and hardware kernels originally developed for LLaMA v2 models, Flan-T5 inference was successfully achieved on IMAX3 without requiring any modification to the hardware instruction set or the high level inference workflow.To the best of our knowledge, this is the first implementation of Flan-T5 on a CGLA based accelerator. Although IMAX3 does not currently outperform CPUs or GPUs in execution speed or power efficiency, our analysis identifies critical system level bottlenecks particularly in host accelerator coordination and memory bandwidth that limit scalability. These findings highlight the architectural generality of quantized kernel designs across structurally distinct LLMs and provide a valuable foundation for future work on energy efficient LLM deployment using reconfigurable accelerators.",
    "title_zh": "基于CGLA加速器的Flan-T5性能评估",
    "abstract_zh": "大型语言模型（LLMs）带来了巨大的计算与能耗需求，给边缘设备的部署带来了挑战。为应对这一问题，我们探索了在基于粗粒度线性阵列（CGLA）架构的内存内加速器IMAX3上运行经过指令微调的编码器-解码器型语言模型Flan-T5的可行性。通过利用此前为LLaMA v2模型开发的现有自定义指令和硬件核函数，我们成功实现了Flan-T5在IMAX3上的推理，且无需对硬件指令集或高层推理流程进行任何修改。据我们所知，这是首个在基于CGLA架构的加速器上实现的Flan-T5部署。尽管目前IMAX3在执行速度和能效方面尚未超越CPU或GPU，但我们的分析揭示了系统层面的关键瓶颈，尤其是主机与加速器之间的协同以及内存带宽限制，这些因素制约了系统的可扩展性。这些发现凸显了量化内核设计在结构差异较大的各类大模型中具有良好的架构通用性，并为未来利用可重构加速器实现高效节能的大模型部署提供了宝贵的基础。"
  },
  {
    "date": "2025-11-17",
    "title": "Web3Agent: Automating On-Chain Operations via Natural Language Interfaces",
    "authors": "Sizheng Fan, Tian Min",
    "publish": "ACM Transactions on the Web",
    "url": "https://doi.org/10.1145/3777446",
    "source": "ACM",
    "abstract": "Recent advances in large language models (LLMs) have enabled the emergence of intelligent agents capable of performing complex multi-step tasks across various domains. In parallel, the growth of Web3 has introduced a decentralized web infrastructure, yet remains largely inaccessible to non-technical users due to operational complexity, fragmented information, and security risks. In this paper, we present Web3Agent, an AI agent system that integrates LLM-based interaction with blockchain environments to enable language-driven on-chain operations. Web3Agent automatically decomposes user instructions into structured workflows, dynamically queries blockchain data and APIs, and performs multi-step operations such as asset transfers, token swaps, and smart contract execution. Web3Agent incorporates real-time inspection, error handling, and interaction transparency across its operation log, and flow visualization components. We evaluate the system and perform ablation study with customized dataset in a simulated environment, demonstrating its feasibility in orchestrating complex Web3 tasks and highlighting implications for agent-based abstraction in decentralized systems.",
    "title_zh": "Web3Agent：通过自然语言接口自动化链上操作",
    "abstract_zh": "近年来，大型语言模型（LLMs）的快速发展催生了能够跨多个领域执行复杂多步骤任务的智能代理。与此同时，Web3的兴起带来了去中心化网络基础设施，但由于操作复杂性、信息碎片化以及安全风险，仍对非技术用户难以访问。本文提出Web3Agent——一种将基于大语言模型的交互与区块链环境相结合的AI代理系统，实现通过自然语言驱动链上操作。Web3Agent能够自动将用户指令分解为结构化工作流，动态查询区块链数据与API，并完成资产转账、代币兑换及智能合约执行等多步操作。该系统集成了实时检查、错误处理以及操作日志和流程可视化组件，确保交互过程的透明性与可追溯性。我们在模拟环境中使用定制数据集对系统进行了评估，并开展了消融实验，验证了其在协调复杂Web3任务方面的可行性，同时揭示了基于代理的抽象在去中心化系统中的重要意义。"
  },
  {
    "date": "2025-11-17",
    "title": "Blockchain-Based E-Procurement System Enhancing Data Security Within Ethereum Smart Contracts",
    "authors": "Abdelmonem Najjari, Najiba Tagougui",
    "publish": "2025 IEEE International Conference on Advanced Systems and Emergent Technologies (IC_ASET)",
    "url": "https://doi.org/10.1109/ic_aset65966.2025.11232429",
    "source": "IEEE",
    "abstract": "The rapid adoption of blockchain technology in e-procurement systems has revolutionized digital transactions, offering transparency, immutability, and decentralized control. However, despite these advantages, smart contract vulnerabilities pose significant security risks, potentially undermining the integrity of procurement processes. This paper explores the development of a blockchain-based e-procurement system designed to enhance data security while mitigating smart contract vulnerabilities. By integrating advanced cryptographic techniques, formal verification methods, and secure coding practices, the proposed system aims to strengthen transactional security and ensure trust in procurement operations. The study further outlines real scenario test cases designed to evaluate the functionality, security, and reliability of the smart contracts and system components within the Ethereum based procurement model. Our findings suggest that a well-architected blockchain-based e-procurement framework can significantly enhance data security and improve overall trust in digital procurement ecosystems.",
    "title_zh": "基于区块链的电子采购系统在以太坊智能合约中提升数据安全性的研究",
    "abstract_zh": "区块链技术在电子采购系统中的快速应用已彻底改变了数字交易，带来了透明性、不可篡改性和去中心化控制等优势。然而，尽管具备这些优点，智能合约的漏洞仍带来显著的安全风险，可能破坏采购流程的完整性。本文探讨了一种基于区块链的电子采购系统的设计与开发，旨在提升数据安全性的同时有效降低智能合约的潜在漏洞。通过整合先进的加密技术、形式化验证方法以及安全编码实践，所提出的系统致力于增强交易安全性，并确保采购操作的可信度。研究还设计了若干真实场景测试案例，用以评估基于以太坊的采购模型中智能合约及系统组件的功能性、安全性和可靠性。研究结果表明，一个架构合理的区块链电子采购框架能够显著提升数据安全性，并增强对数字采购生态系统的整体信任。"
  },
  {
    "date": "2025-11-17",
    "title": "Synthetic Malware at Scale: Malicious Code Generation with Code Transplanting",
    "authors": "Guangzhan Wang, Diwei Chen, Xiaodong Gu, Yuting Chen, Beijun Shen",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2025.3633280",
    "source": "IEEE",
    "abstract": "Malicious code detection is one of the most essential tasks in safeguarding against security breaches, data compromise, and related threats. While machine learning has emerged as a predominant method for pattern detection, the training process is intricate due to the severe scarcity of malicious code samples. Consequently, machine learning detectors often encounter malicious patterns in limited and isolated scenarios, hindering their ability to generalize effectively across diverse threat landscapes. In this paper, we introduce MalCoder, a novel method for synthesizing malicious code samples. MalCoder enlarges the quantity and diversity of malicious instances by transplanting a set of malicious prototypes into a vast pool of benign code, thereby crafting a diverse array of malicious instances tailored to various application scenarios. For each malware prototype, MalCoder treats it as an incomplete code fragment and crafts its preceding and subsequent contexts through right-to-left and left-to-right code completion respectively. By leveraging GPTs with various sampling strategies, we can instantiate a large number of code samples bearing the malware prototype. Subsequently, MalCoder masks the original prototypes within the transplanted samples and fine-tunes an LLM code generator to reconstruct the original prototype. This process enables the model to seamlessly transplant malicious code fragments into benign code. During inference, MalCoder can automatically insert malicious fragments into benign samples at random positions, transforming benign code into malicious code. We apply MalCoder to a large pool of benign code in CodeSearchNet and craft over 50,000 malicious samples stemming from 39 malicious prototypes. Both qualitative and quantitative analyses show that the generated samples maintain key characteristics of malicious code while blending seamlessly with benign code, which helps in creating realistic and varied training data. Additionally, by using the generated samples as augmented training data, we witness a remarkable surge in malicious code detection capabilities. Specifically, the F1-score experiences a significant increase compared to utilizing only the original prototype samples.",
    "title_zh": "大规模合成恶意软件：基于代码移植的恶意代码生成",
    "abstract_zh": "恶意代码检测是防范安全漏洞、数据泄露及相关威胁中最关键的任务之一。尽管机器学习已成为模式检测的主要方法，但其训练过程十分复杂，主要受限于恶意代码样本的严重稀缺性。因此，基于机器学习的检测器往往只能在有限且孤立的场景中识别恶意模式，难以有效泛化到多样化的威胁环境中。本文提出一种名为 MalCoder 的新方法，用于合成恶意代码样本。MalCoder 通过将一组恶意原型代码移植到大量良性代码池中，显著增加了恶意实例的数量与多样性，从而生成适用于不同应用场景的多样化恶意样本。对于每个恶意原型，MalCoder 将其视为不完整的代码片段，并分别通过从右向左和从左向右的代码补全方式，构建其前后上下文。借助具有不同采样策略的 GPT 模型，我们能够生成大量包含恶意原型的代码样本。随后，MalCoder 在这些移植后的样本中掩码原始原型，并微调一个大语言模型（LLM）代码生成器，使其能够重建原始原型。这一过程使模型能够无缝地将恶意代码片段嵌入良性代码中。在推理阶段，MalCoder 可以自动在良性代码的随机位置插入恶意片段，从而将良性代码转化为恶意代码。我们将 MalCoder 应用于 CodeSearchNet 中的大规模良性代码库，成功生成了超过 50,000 个源自 39 个恶意原型的恶意样本。定性和定量分析均表明，生成的样本保留了恶意代码的关键特征，同时与良性代码高度融合，具备极强的真实感和多样性，有助于构建逼真且多样的训练数据。此外，利用生成的样本作为增强训练数据后，恶意代码检测能力显著提升，F1 分数相比仅使用原始原型样本的情况实现了显著增长。"
  },
  {
    "date": "2025-11-17",
    "title": "A Deep Learning Framework for Verilog Autocompletion Towards Design and Verification Automation",
    "authors": "Enrique Dehaerne, Bappaditya Dey, Sandip Halder, Stefan De Gendt",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235413",
    "source": "IEEE",
    "abstract": "Innovative Electronic Design Automation (EDA) solutions are crucial for meeting the design requirements of increasingly complex electronic devices. Verilog, a hardware description language, is widely used for the design and verification of digital circuits and is synthesized using specific EDA tools. However, writing code is a repetitive and time-intensive task. This paper proposes a deep learning framework for training a Verilog auto-completion model along with a dataset of Verilog code files and snippets from open-source repositories. The framework involves integrating models pretrained on general programming language data and finetuning them on a dataset curated to be similar to a target downstream task. This is validated by comparing different pretrained models trained on different subsets of the proposed Verilog dataset using multiple evaluation metrics. These experiments demonstrate that the proposed framework achieves better BLEU, ROUGE-L, and chrF scores by 9.5%, 6.7%, and 6.9%, respectively, compared to a model trained from scratch. This validates our framework, which has already inspired more recent related works.",
    "title_zh": "面向设计与验证自动化的Verilog代码补全深度学习框架",
    "abstract_zh": "创新的电子设计自动化（EDA）解决方案对于满足日益复杂的电子设备设计需求至关重要。Verilog 作为一种硬件描述语言，被广泛用于数字电路的设计与验证，并通过特定的 EDA 工具进行综合。然而，编写代码是一项重复性高且耗时的任务。本文提出了一种基于深度学习的框架，用于训练 Verilog 自动补全模型，并构建了一个包含来自开源仓库的 Verilog 代码文件和代码片段的数据集。该框架通过将预训练于通用编程语言数据的模型，进一步在与目标下游任务相似的数据集上进行微调来实现。通过使用多种评估指标对比在所提出的 Verilog 数据集不同子集上训练的不同预训练模型，验证了该方法的有效性。实验结果表明，与从零开始训练的模型相比，所提出的框架在 BLEU、ROUGE-L 和 chrF 指标上分别提升了 9.5%、6.7% 和 6.9%。这一成果验证了本框架的有效性，且该框架已激发了后续多项相关研究工作。"
  },
  {
    "date": "2025-11-17",
    "title": "Hardware-Software Co-Design for Efficient LLM Inference on PCIe-Based FPGAs Using Coarse-Grained Systolic Arrays",
    "authors": "Yun-Nan Chang",
    "publish": "2025 IEEE 38th International System-on-Chip Conference (SOCC)",
    "url": "https://doi.org/10.1109/socc66126.2025.11235351",
    "source": "IEEE",
    "abstract": "Inference of large language models (LLMs) requires extensive matrix-multiplication operations that place heavy demands on both compute and memory resources. Traditional systolic arrays (SAs), typically built from scalar multiply-accumulate processing elements (PEs), exhibit low compute density and high routing overhead when scaled to support the large matrix dimensions of transformer-based models. To address these limitations, we present a hardware-software co-design approach that introduces a coarse-grained \"Big PE\" architecture for FPGA acceleration. Each Big PE performs a complete 4 × 4 matrix multiplication per clock cycle using sixteen parallel multipliers and local accumulators, significantly reducing inter-PE communication and flip-flop usage. We implement a 2×32 array of these Big PEs on a Xilinx Alveo U55 FPGA and tightly integrate the design into the open-source Llama2 inference framework using an XDMA PCIe–AXI bridge. Experimental evaluation with an 8-bit quantized TinyStories-15M LLM shows more than 2.15× improvement in token processing speed during the prompt stage compared to a CPU-only baseline, with no degradation in output quality. Under a fixed budget of 4,096 DSP multipliers, our SA reduces the cycle count for 64 × 64 matrix multiplication from 190 to approximately 120 cycles and cuts total flip-flop usage from 194,560 to 45,056. These results highlight the benefits of hardware-software co-design in deploying LLM inference on resource-constrained FPGA platforms and suggest promising directions for optimizing on-chip memory hierarchies and enabling multi-FPGA scalability.",
    "title_zh": "基于PCIe接口FPGA的粗粒度流水线阵列实现高效大模型推理的软硬件协同设计",
    "abstract_zh": "大型语言模型（LLM）的推理需要大量矩阵乘法运算，对计算和内存资源提出了极高要求。传统的阵列结构（SAs）通常由标量乘加处理单元（PE）构成，在扩展以支持基于Transformer模型的大规模矩阵时，表现出计算密度低、布线开销高的问题。为解决这些局限性，我们提出一种软硬件协同设计方法，引入一种粗粒度的“大处理单元”（Big PE）架构，用于FPGA加速。每个Big PE利用十六个并行乘法器和本地累加器，每时钟周期完成一次完整的4×4矩阵乘法，显著减少了PE之间的通信开销和触发器使用量。我们在Xilinx Alveo U55 FPGA上实现了2×32的Big PE阵列，并通过XDMA PCIe–AXI桥接器，将该设计紧密集成到开源Llama2推理框架中。实验结果表明，在8位量化TinyStories-15M LLM上的评估显示，与纯CPU基线相比，提示阶段的令牌处理速度提升超过2.15倍，且输出质量无下降。在固定4,096个DSP乘法器预算下，我们的阵列将64×64矩阵乘法的周期数从190降至约120个周期，总触发器用量也从194,560减少至45,056。这些成果凸显了软硬件协同设计在资源受限FPGA平台上部署LLM推理中的优势，并为优化片上存储层次结构以及实现多FPGA可扩展性指明了有前景的方向。"
  },
  {
    "date": "2025-11-17",
    "title": "A Maintainability Framework to Ensure the Software Quality in Object-Oriented Programming",
    "authors": "Siti Rochimah, Tiara Rahmania Hadiningrum, Bella Dwi Mardiana, Daniel Oranova Siahaan, Rizky Januar Akbar, Ary Mazharuddin Shiddiqi",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3633265",
    "source": "IEEE",
    "abstract": "In recent years, there have been significant challenges in the attempt to improve modular structure and code reusability in software development. Software developers should ensure that refactoring not only addresses code smells and provides tangible improvements to software quality metrics. Although metric-based approaches allow for objective and systematic measurements, they have limitations. They cannot ascertain the effectiveness of refactoring techniques in terms of readability, code maintainability, and their impact on system performance and developer productivity. This study proposes mathematical formulations for five key metrics: Modularity (MMo-1-G, MMo-2-S), Analysability (MAn-2-S), Reusability (MRe-1-G), and Testability (MTe-1-G). These metrics were used to evaluate and verify the effectiveness of refactoring in improving module separability and code reuse rate. The proposed model is presented in mathematical notation to link the concepts of modularity and reusability with the corresponding refactoring implementation. Case studies were conducted by applying this formulation to various refactoring techniques aimed at addressing specific types of code smells. Based on the metrics analysis conducted on 17 types of code smell with 3 refactoring techniques, the results demonstrate an improvement in MMo-1-G by 11.46%, MMo-2-S by 0.8%, MAn-2-S by 1.2%, MRe-1-G by 0.82% and MTe-1-G by 3.07%. These findings demonstrate that the proposed formulation can be effectively applied to evaluate code quality changes after refactoring and provide more objective insights into code improvement decision-making.",
    "title_zh": "面向对象编程中确保软件质量的可维护性框架",
    "abstract_zh": "近年来，软件开发中在提升模块化结构和代码可重用性方面面临诸多挑战。软件开发者应确保重构不仅能够解决代码异味问题，还能切实改善软件质量度量指标。尽管基于度量的方法能够实现客观且系统化的评估，但仍存在局限性：它们无法准确衡量重构技术在代码可读性、可维护性以及对系统性能和开发人员生产力影响方面的实际效果。本研究提出了五个关键度量的数学表达式：模块化（MMo-1-G、MMo-2-S）、可分析性（MAn-2-S）、可重用性（MRe-1-G）和可测试性（MTe-1-G）。这些度量被用于评估和验证重构在提升模块分离度和代码重用率方面的有效性。所提出的模型以数学符号形式呈现，旨在将模块化与可重用性的概念与具体的重构实现相联系。通过将该公式应用于多种旨在解决特定代码异味类型的重构技术，开展了案例研究。基于对17类代码异味及3种重构技术的度量分析，结果表明：MMo-1-G提升了11.46%，MMo-2-S提升了0.8%，MAn-2-S提升了1.2%，MRe-1-G提升了0.82%，MTe-1-G提升了3.07%。这些发现表明，所提出的度量公式能够有效评估重构后代码质量的变化，并为代码改进决策提供更加客观的洞察。"
  },
  {
    "date": "2025-11-17",
    "title": "Real Time Trade Settlement for the US Market",
    "authors": "Debarshi Chakraborty, Chung-Sheng Li, Rajdeep Mazumder",
    "publish": "2025 3rd World Conference on Communication &amp;amp; Computing (WCONF)",
    "url": "https://doi.org/10.1109/wconf64849.2025.11233239",
    "source": "IEEE",
    "abstract": "The US market recently moved to a T+1 settlement cycle, which is quite a big step forward, but the real goal here is achieving real time settlement - basically T + 0 or instant settlement - with the ultimate goal of achieving real-time (T+0) settlement, which could significantly enhance the efficiency and stability of U.S. financial markets. This paper proposes a complete solution for instant trade settlement in the US market. Our approach involves using technologies like Distributed Ledger Technology (DLT) and Blockchain and combining them with intelligent automation using Agentic Artificial Intelligence (AI) and Large Language Models (LLMs) for their analytical power. The system being detailed here is built around a private, permissioned blockchain setup. This approach gives us immutable record keeping and atomic settlement capabilities through smart contracts. What this means is that tokenized securities and cash can be transferred instantaneously. When it comes to Agentic AI, this technology is being looked at to handle various tasks autonomously. Tasks like pre trade validation, managing real time liquidity, optimizing collateral, and dealing with post trade exceptions. These capabilities collectively aim to reduce both operational and counterparty risks. As for LLMs, these will work as intelligent interfaces for monitoring and reporting. They will handle regulatory compliance automation and improve market insights using natural language processing. This whole integrated framework being proposed should help drastically cut down counterparty risk, boost liquidity, reduce operational costs, and make things more transparent and auditable. This paper also tackles some key challenges - regulatory alignment, interoperability issues, scalability concerns, and cybersecurity aspects. This provides a realistic roadmap for where U.S. financial market infrastructure might head in the future. This work introduces a comprehensive framework that brings together permissioned distributed ledger technology, autonomous AI agents, and large language models to streamline regulatory processes. The result is a real-time (T+0) settlement system specifically designed to meet the demands of institutional trading in the U.S. financial markets.",
    "title_zh": "美国市场的实时交易结算",
    "abstract_zh": "美国市场最近已过渡至T+1结算周期，这无疑是一个重大进步。然而，真正的目标是实现真正意义上的实时结算——即T+0或即时结算。最终目标是达成实时（T+0）结算，这将显著提升美国金融市场的效率与稳定性。本文提出了一套完整的解决方案，旨在实现美国市场的即时交易结算。我们的方法结合了分布式账本技术（DLT）和区块链技术，并融合了具有强大分析能力的代理型人工智能（AI）与大型语言模型（LLMs）的智能自动化。本文所描述的系统基于一个私有、许可型区块链架构，该架构具备不可篡改的记录保存功能以及通过智能合约实现的原子结算能力。这意味着，经过代币化的证券和现金可以实现瞬时转移。在代理型AI方面，这项技术正被用于自主处理各类任务，包括事前交易验证、实时流动性管理、抵押品优化以及事后交易异常处理等。这些能力共同致力于降低操作风险和对手方风险。至于大型语言模型（LLMs），它们将作为智能化的监控与报告接口，负责自动化监管合规流程，并利用自然语言处理技术提升市场洞察力。这一整合框架有望大幅降低对手方风险，增强市场流动性，减少运营成本，并使整个系统更加透明且可审计。本文还深入探讨了若干关键挑战，包括监管协调、互操作性问题、可扩展性顾虑以及网络安全因素。该研究为美国金融市场基础设施未来的发展路径提供了一份切实可行的路线图。本工作提出了一套综合性框架，将许可型分布式账本技术、自主AI代理与大型语言模型相结合，以简化监管流程。其最终成果是一个专为满足美国机构交易需求而设计的实时（T+0）结算系统。"
  },
  {
    "date": "2025-11-17",
    "title": "All About Nothing: Towards Zero-Cost Hardware Accelerated RISC-V Interrupt Handling in Rust",
    "authors": "Pawel Dzialo, Per Lindgren",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231308",
    "source": "IEEE",
    "abstract": "Micro-controllers underlying commonplace embedded systems rely on interrupt and exception mechanisms for event driven scheduling and error handling respectively. At the lowest level of the embedded software stack, the firmware implements the actions to be taken. With outsets from the RISC-V architecture and the modern systems level programming language Rust, we seek solutions leveraging hardware accelerated interrupt and exception mechanisms while at the same time being free of non-inherent overhead - thus the title All About Nothing. The paper provides a comprehensive overview of Rust low-level code generation and identifies an implementation performance gap caused by the current design of low-level abstractions. We propose both hardware and software solutions addressing and eliminating the problem: the former by implementing an additional interrupt/exception return stack in hardware, while the latter by changing the semantics of the riscv-interrupt-m interrupt attribute. As an additional observation we find the performance gap not to be unique to Rust, the problem along with proposed solutions are equally present and applicable to C/C++ as compiled with the latest GCC toolchain targeting the RISC-V architecture.",
    "title_zh": "一文读懂：迈向零成本的硬件加速RISC-V中断处理（基于Rust）",
    "abstract_zh": "微控制器是常见嵌入式系统的核心，其依赖中断和异常机制分别实现事件驱动调度与错误处理。在嵌入式软件栈的最底层，固件负责执行具体操作。本文基于RISC-V架构以及现代系统级编程语言Rust，探索利用硬件加速的中断与异常机制的解决方案，同时避免非本质开销——因此论文标题为《一切皆无》（All About Nothing）。文章全面概述了Rust生成低级别代码的机制，并揭示了当前低级别抽象设计所导致的性能差距。为此，我们提出了软硬件相结合的解决方案：硬件方面，通过在硬件中引入额外的中断/异常返回栈来解决该问题；软件方面，则通过修改riscv-interrupt-m中断属性的语义来优化。此外，我们还发现这一性能差距并非仅存在于Rust语言中，当使用最新GCC工具链针对RISC-V架构编译C/C++代码时，该问题同样存在，且提出的解决方案也完全适用。"
  },
  {
    "date": "2025-11-17",
    "title": "Towards an Agentic Workflow for Internet Measurement Research",
    "authors": "Alagappan Ramanathan, Eunju Kang, Dongsu Han, Sangeetha Abdu Jyothi",
    "publish": "Proceedings of the 24th ACM Workshop on Hot Topics in Networks",
    "url": "https://doi.org/10.1145/3772356.3772409",
    "source": "ACM",
    "abstract": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.",
    "title_zh": "面向智能体工作流的互联网测量研究",
    "abstract_zh": "互联网测量研究正面临可访问性危机：复杂的分析需要将多种专业工具进行定制化集成，而这又要求具备专门的领域知识。当网络出现中断时，运营商需要能够快速执行涵盖基础设施映射、路由分析和依赖关系建模的诊断流程。然而，开发这些流程不仅需要专门的知识，还需投入大量手动工作。"
  },
  {
    "date": "2025-11-17",
    "title": "Compilation Framework for Dynamically Reconfigurable Array Architectures",
    "authors": "Anish Vipperla, Narayanan Murugan, Ali Akoglu, Chaitali Chakrabarti",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3633942",
    "source": "IEEE",
    "abstract": "This paper presents a compilation and scheduling framework for high-performance mapping of computationally-intensive kernels on Dynamically Reconfigurable Array Architectures. We showcase the framework for Domain Adaptive Processor (DAP) - an array accelerator designed for signal processing and communication workloads that has near-ASIC energy efficiency. DAP consists of an array of homogeneous Processing Element (PE) clusters, where each PE cluster consists of heterogeneous PEs capable of executing Very-Long-Instruction-Word (VLIW) instructions. We introduce a virtual Instruction Set Architecture (ISA) and a sequential assembly to low-level micro-code conversion mechanism. The proposed work includes optimization techniques that transform the low-level micro-code into compact VLIW instructions resulting in an average instruction count reduction by 1.8x and a significant increase in PE utilization. In addition, we demonstrate rate-aware software-based pipelining of the VLIW code resulting in an improvement in throughput by up to 15x for linear algebra and signal processing kernels. Further, we propose a scheduler that is capable of automatically resolving data dependencies between the threads at the micro-instruction level and accurately schedule custom dataflows overcoming the limitations of fixed mapping and scheduling schemes used in traditional CGRA compilers. These contributions together, provide a compilation flow for dynamically reconfigurable array architectures leading to an improvement in ease of programmability and productivity while ensuring low-level implementation flexibility for performance-critical applications.",
    "title_zh": "动态可重构阵列架构的编译框架",
    "abstract_zh": "本文提出了一种用于在动态可重构阵列架构上高性能映射计算密集型内核的编译与调度框架。我们以领域自适应处理器（DAP）为例展示了该框架的应用，DAP是一种专为信号处理和通信工作负载设计的阵列加速器，具备接近ASIC的能效。DAP由一组同质的处理单元（PE）集群构成，每个PE集群包含若干异构的PE，能够执行超长指令字（VLIW）指令。本文引入了一种虚拟指令集架构（ISA）以及从顺序汇编代码到低级微码的转换机制。所提出的方案包含一系列优化技术，可将低级微码转换为紧凑的VLIW指令，平均减少1.8倍的指令数量，并显著提升PE的利用率。此外，我们还演示了基于软件的、速率感知的VLIW代码流水线化技术，使线性代数和信号处理内核的吞吐量最高提升15倍。进一步地，我们提出了一种调度器，能够在微指令级别自动解析线程间的数据依赖关系，并精确调度自定义数据流，克服了传统CGRA编译器中固定映射与调度方案的局限性。这些贡献共同构建了一个面向动态可重构阵列架构的完整编译流程，在提升编程便捷性和开发效率的同时，确保了对性能关键应用的底层实现灵活性。"
  },
  {
    "date": "2025-11-17",
    "title": "HyPPA: PPA-Aware Hierarchical RTL Generation and Evaluation of RISC-V Cores Using Hyperparameter Tuning",
    "authors": "Mohamed Badawy, Jiulong Wang, Vijaydeep Yadav, Nguyen Anh Vu Doan, Paritosh Kumar Sinha, Wolfgang Ecker",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231296",
    "source": "IEEE",
    "abstract": "Register-Transfer Level (RTL) generators for complex intellectual property (IP), such as RISC-V processors, leverage specialized sub-generators to produce diverse implementations of submodules. These sub-generators create numerous design variants, each impacting synthesis and physical design outcomes, particularly power, performance, and area (PPA). Navigating this expansive design space to identify optimal configurations poses a significant challenge. This work proposes a framework that enables designers to select optimal submodule combinations while efficiently evaluating their PPA implications. It substantially reduces the effort required compared to exhaustive synthesis and physical design of each core variant. By employing optimization algorithms such as Optuna, HEBO, and Ax, our approach systematically selects and assesses submodule combinations to optimize design outcomes. Experimental results show significant improvements in PPA compared to the worst case, with a 74% increase in maximum frequency, a 48% reduction in power consumption, and a 26% reduction in area. Additionally, the design space exploration (DSE) achieves time savings of at least 50% while yielding designs within 1.2% of optimal PPA metrics (<tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{0. 9 8 8} \\times$</tex> optimal frequency, <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1. 0 0 6} \\times$</tex> optimal power, <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1. 0 0 1} \\times$</tex> optimal area). These outcomes highlight the effectiveness of integrating flexible RTL generation with automated, physicallyaware submodule selection for efficient design space exploration in complex IP core implementations.",
    "title_zh": "HyPPA：基于超参数调优的PPA感知型RISC-V核心分层RTL生成与评估",
    "abstract_zh": "针对复杂知识产权（IP）如RISC-V处理器的寄存器传输级（RTL）生成器，通常会利用专用的子生成器来构建各类子模块的不同实现。这些子生成器能够生成大量设计变体，每种变体都会对综合与物理设计结果产生影响，尤其是在功耗、性能和面积（PPA）方面。在如此庞大的设计空间中寻找最优配置，已成为一项重大挑战。本文提出了一种新框架，使设计者能够在高效评估其PPA影响的前提下，选择最优的子模块组合。相比对每个核心变体进行完整综合与物理设计的 exhaustive 方法，该方法显著降低了工作量。通过采用 Optuna、HEBO 和 Ax 等优化算法，我们的方法系统性地筛选并评估子模块组合，以优化整体设计结果。实验结果表明，与最差情况相比，PPA 性能得到显著提升：最大频率提高 74%，功耗降低 48%，面积减少 26%。此外，设计空间探索（DSE）实现了至少 50% 的时间节省，同时获得的设计性能接近最优——频率达到最优值的 0.988 倍，功耗为最优值的 1.006 倍，面积为最优值的 1.001 倍。这些成果充分证明，将灵活的 RTL 生成与自动化、物理感知的子模块选择相结合，是实现复杂 IP 核高效设计空间探索的有效途径。"
  },
  {
    "date": "2025-11-17",
    "title": "ChiselTrace: Typed Behavioral Debugging in Chisel Through Signal Dependency Tracing",
    "authors": "Jarl Brand, Casper Cromjongh, H. Peter Hofstee, Zaid Al-Ars",
    "publish": "2025 IEEE Nordic Circuits and Systems Conference (NorCAS)",
    "url": "https://doi.org/10.1109/norcas66540.2025.11231292",
    "source": "IEEE",
    "abstract": "While modern HDLs such as Chisel (Constructing Hardware In a Scala Embedded Language) significantly improve the process of design entry, debugging these designs is often problematic, because the tools that aid debugging operate on translated code rather than the original HDL. Furthermore, engineers often resort to manual waveform debugging, undermining productivity gains promised by such a language. We present ChiselTrace, an open-source tool for Chisel that is capable of (dynamic) program slicing and automatic signal dependency tracing, allowing faults to be more easily traced back to their root cause. Where prior work focuses on data-flow analysis at the (compiled) Verilog level, ChiselTrace functions at the Chisel source level. Contributions include: modifications to the Chisel library to enable post-simulation analysis; a library capable of dynamic program slicing and dependence graph generation; and a front-end dependency graph viewer. We demonstrate debugging capabilities by tracing an injected fault in the ChiselWatt processor back to the source. We observe that using ChiselTrace's dynamic program dependence graph, the number of lines of code relevant to the fault path is reduced significantly. Project repository: https://github.com/jarlb/chiseltrace",
    "title_zh": "ChiselTrace：通过信号依赖追踪实现Chisel中的类型化行为调试",
    "abstract_zh": "尽管现代硬件描述语言（HDL）如 Chisel（在 Scala 中嵌入的硬件构造语言）显著提升了设计输入的效率，但调试这些设计往往仍存在困难，因为辅助调试的工具通常作用于翻译后的代码，而非原始的 HDL 代码。此外，工程师常常不得不依赖手动波形调试，这在很大程度上削弱了这类语言所承诺的生产效率提升。本文提出 ChiselTrace，一个针对 Chisel 的开源工具，具备动态程序切片和自动信号依赖追踪功能，能够更轻松地将故障追溯至其根本原因。与以往工作聚焦于编译后 Verilog 级别的数据流分析不同，ChiselTrace 在 Chisel 源代码层面运行。本研究的主要贡献包括：对 Chisel 库的修改，以支持仿真后的分析；一个可实现动态程序切片和依赖图生成的库；以及一个前端依赖图可视化工具。我们通过在一个注入故障的 ChiselWatt 处理器中进行调试演示，验证了该工具的有效性。实验结果表明，利用 ChiselTrace 提供的动态程序依赖图，与故障路径相关的代码行数显著减少。项目仓库地址：https://github.com/jarlb/chiseltrace"
  }
]