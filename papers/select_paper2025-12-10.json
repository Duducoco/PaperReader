[
  {
    "date": "2025-12-10",
    "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
    "authors": "Arihant Tripathy, Ch Pavan Harshit, Karthik Vaidhyanathan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09543v1",
    "source": "arXiv",
    "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood. Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs. Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration. Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency. Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
    "title_zh": "SWEnergy：基于小型语言模型的智能体问题解决框架能效性实证研究",
    "abstract_zh": "背景。基于大语言模型（LLM）的自主智能体在软件工程中的应用依赖于大型专有模型，限制了其本地部署的可能性。这一局限性促使研究者关注小型语言模型（SLMs），但目前对于SLMs在复杂智能体框架中实现自动化问题修复的实际有效性与效率仍缺乏充分理解。\n\n目标。我们系统评估了四种主流智能体问题修复框架在刻意限定使用SLMs条件下的性能表现、能效比及资源消耗情况。旨在评估这些系统在资源受限环境下的可行性，并揭示由此产生的权衡关系。\n\n方法。我们在固定硬件条件下，对四种领先的智能体框架（SWE-Agent、OpenHands、Mini SWE Agent、AutoCodeRover）分别结合两种SLMs（Gemma-3 4B 和 Qwen-3 1.7B）进行了受控实验，测试基准为 SWE-bench Verified Mini。每种配置执行150次运行，测量能耗、执行时长、token使用量和内存占用。\n\n结果。我们发现，框架架构是决定能耗的主要因素。最耗能的框架 AutoCodeRover（Gemma）平均能耗是能耗最低的 OpenHands（Gemma）的9.4倍。然而，这种高能耗几乎全部被浪费：任务解决率接近零，表明当前框架在搭配SLMs使用时，大量能量消耗于无效的推理循环之中。虽然SLMs有限的推理能力是达成成功的关键瓶颈，但框架设计才是导致效率低下的根本原因。\n\n结论。现有智能体框架最初是为强大LLMs设计的，在应用于SLMs时无法实现高效运行。我们确认，框架架构是能耗的主要驱动因素，但该能耗因SLMs推理能力不足而大多被浪费。要实现真正低能耗的解决方案，必须从被动调度转向主动应对SLMs缺陷的新型架构设计。"
  },
  {
    "date": "2025-12-10",
    "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning",
    "authors": "Yucan Guo, Miao Su, Saiping Guan, Zihao Sun, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09487v1",
    "source": "arXiv",
    "abstract": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.",
    "title_zh": "RouteRAG：基于强化学习的文本与图结构高效检索增强生成",
    "abstract_zh": "检索增强生成（Retrieval-Augmented Generation, RAG）将非参数化知识融入大型语言模型（LLMs），通常来源于非结构化文本和结构化图谱。尽管近期进展通过强化学习（RL）使基于文本的RAG实现了多轮推理，但将其扩展到混合检索场景仍面临额外挑战。现有的基于图谱或混合的系统通常依赖于固定或手工设计的检索流程，缺乏在推理过程中动态整合补充证据的能力。此外，虽然图谱证据为多跳推理提供了关键的关联结构，但其检索成本显著更高。为解决这些局限性，我们提出了\\model{}，一种基于强化学习的框架，使LLM能够执行多轮且自适应的图-文混合RAG。\\model{}通过强化学习联合优化整个生成过程，使模型能够学习何时进行推理、从文本或图谱中检索何种信息，以及何时生成最终答案，所有决策均统一由一个生成策略完成。为了引导这一学习过程，我们设计了一种两阶段训练框架，兼顾任务结果与检索效率，使模型能够在利用混合证据的同时避免不必要的检索开销。在五个问答基准上的实验结果表明，\\model{}显著优于现有的RAG基线方法，充分展示了端到端强化学习在支持复杂推理中自适应、高效检索方面的优势。"
  },
  {
    "date": "2025-12-10",
    "title": "Architectures for Building Agentic AI",
    "authors": "Sławomir Nowaczyk",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09458v1",
    "source": "arXiv",
    "abstract": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",
    "title_zh": "构建代理型人工智能的架构",
    "abstract_zh": "本章认为，代理型与生成式人工智能的可靠性主要是一种架构属性。我们将代理系统定义为在闭环中运行、以目标为导向并使用工具进行决策的实体，并展示可靠性如何通过原则性的组件化（目标管理器、规划器、工具路由模块、执行器、记忆模块、验证器、安全监控器、遥测系统）、规范化的接口（基于模式约束、经过验证、权限最小化的工具调用），以及明确的控制与保障回路而产生。在继承经典理论的基础上，我们提出一个实用的分类体系：工具使用型代理、记忆增强型代理、规划与自我改进型代理、多代理系统，以及具身或网络代理，并分析每种模式如何重塑可靠性边界与故障模式。我们进一步提炼出设计指导原则，包括类型化模式、幂等性、权限管理、事务语义、记忆溯源与清洁性、运行时治理（预算控制、终止条件），以及“先模拟后执行”的防护机制。"
  },
  {
    "date": "2025-12-10",
    "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
    "authors": "Justin W. Lin, Eliot Krzysztof Jones, Donovan Julian Jasper, Ethan Jun-shen Ho, Anna Wu, Arnold Tianyi Yang, Neil Perry, Andy Zou, Matt Fredrikson, J. Zico Kolter, Percy Liang, Dan Boneh, Daniel E. Ho",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09882v1",
    "source": "arXiv",
    "abstract": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
    "title_zh": "将AI代理与网络安全专业人员在现实世界渗透测试中的表现进行比较",
    "abstract_zh": "我们首次在真实的企业环境中，对人工智能（AI）代理与人类网络安全专业人员进行了全面评估。本次评估中，我们对比了十名网络安全专业人士、六款现有的AI代理以及我们新提出的代理框架ARTEMIS，在一个包含约8,000台主机、分布在12个子网的大型大学网络上展开测试。ARTEMIS是一个多代理框架，具备动态提示生成、任意子代理调用以及自动漏洞优先级排序等功能。在对比研究中，ARTEMIS总体排名第二，共发现9个有效漏洞，有效提交率达82%，表现优于10名人类参与者中的9人。相比之下，现有代理框架如Codex和CyAgent的表现普遍低于大多数人类参与者，而ARTEMIS则展现出与最强人类参与者相当的技术复杂度和提交质量。我们观察到，AI代理在系统性枚举、并行攻击执行以及成本方面具有显著优势——某些ARTEMIS变体每小时仅需18美元，而专业渗透测试人员的费用为每小时60美元。同时，我们也识别出关键的能力短板：AI代理存在较高的误报率，并且在处理基于图形界面（GUI）的任务时表现不佳。"
  },
  {
    "date": "2025-12-10",
    "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning",
    "authors": "Khurram Khalil, Muhammad Mahad Khaliq, Khaza Anuarul Hoque",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09829v1",
    "source": "arXiv",
    "abstract": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.",
    "title_zh": "RIFT：一种基于强化学习的可扩展LLM加速器故障评估方法",
    "abstract_zh": "现代AI加速器的庞大规模给传统的故障评估方法带来了严峻挑战，这些方法不仅计算成本高昂，而且对关键故障模式的覆盖能力较差。本文提出了一种可扩展的框架——RIFT（基于强化学习的智能故障定位），该框架能够自动化发现最小且影响重大的故障场景，从而实现高效的设计阶段故障评估。RIFT将寻找最坏情况故障这一复杂问题转化为一个序列决策问题，结合混合敏感性分析进行搜索空间剪枝，并利用强化学习智能生成最小化、高影响力的测试用例集。在使用NVIDIA A100 GPU对数十亿参数的大语言模型（LLM）工作负载进行评估时，RIFT相较于进化算法实现了**2.2倍**的故障评估速度提升，同时相比随机故障注入，测试向量数量减少超过**99%**，且实现了**更优的故障覆盖率**。此外，所提出的框架还提供了可操作的数据，支持智能硬件保护策略的制定；实验表明，基于RIFT指导的有选择性错误校正码方案，在单位面积上的故障覆盖效率比均匀的三模冗余（TMR）保护提高了**12.8倍**。RIFT还能自动生成符合UVM标准的验证文档，确保其发现结果可直接应用并无缝集成到商业RTL验证流程中。"
  },
  {
    "date": "2025-12-10",
    "title": "Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning",
    "authors": "Kaichen He, Zihao Wang, Muyao Li, Anji Liu, Yitao Liang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09706v1",
    "source": "arXiv",
    "abstract": "The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA",
    "title_zh": "通过强化学习训练一个模型以掌握跨层级代理行为",
    "abstract_zh": "代理型人工智能的范式正从精心设计的复杂工作流，转向训练后原生的模型。然而，现有的智能体通常局限于静态、预定义的动作空间——例如仅使用API、GUI事件或机器人指令。这种僵化限制了它们在动态环境中根据上下文灵活调整交互粒度的能力。为弥合这一差距，我们提出CrossAgent，一种统一的代理模型，能够掌握多种异构动作空间，并自主选择每一步轨迹中最有效的交互接口。我们引入了一个全面的训练流程，结合冷启动监督微调与多轮组相对策略优化（Multi-Turn Group Relative Policy Optimization, GRPO）算法。该方法使智能体能够学习自适应的动作切换能力，在高层次效率与低层次精度之间实现平衡，而无需依赖人工设定的规则。在开放世界Minecraft环境中的800多个任务上进行的大量实验表明，CrossAgent达到了当前最先进的性能水平。通过动态利用不同动作空间的优势，我们的模型显著优于固定动作基线，展现出更强的泛化能力和长时程推理效率。所有代码和模型均可在 https://github.com/CraftJarvis/OpenHA 获取。"
  },
  {
    "date": "2025-12-10",
    "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis",
    "authors": "Naizhu Jin, Zhong Li, Guang Yang, Tian Zhang, Qingkai Zeng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09679v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.",
    "title_zh": "理解思维链在代码生成中的有效性：一项实证与信息论分析",
    "abstract_zh": "大型语言模型（LLMs）在代码生成任务中表现出色，但链式思维（Chain-of-Thought, CoT）提示机制如何提升性能的内在机理仍不清晰。本文通过系统性的实证研究与信息论分析，深入探讨了神经代码生成中CoT的有效性，评估了五种范式（零样本、零样本CoT、自规划、结构化CoT、推理型CoT）在六个Python基准测试、包含12种编程语言的多语言基准测试，以及六种参数规模从7B到480B不等的模型上的表现，并以条件互信息 $I(Y;C|X)$ 作为理论视角。研究结果表明，外部引导的CoT始终优于直接生成，其中结构化方法平均可使 Pass@1 提升5%–12%，且使用的token数量远少于反思式推理；同时发现，CoT的效果受编程语言类型系统和模型容量的影响显著。进一步研究表明，推理的“质量”至关重要：由高性能生成器产生的高质量结构化CoT，其准确率显著高于使用相同模板的轻量级替代方案，而简单的零样本CoT甚至可能降低性能。这些发现为根据模型能力、语言特性及任务复杂度选择合适的CoT策略提供了切实可行的指导。"
  },
  {
    "date": "2025-12-10",
    "title": "Chasing Shadows: Pitfalls in LLM Security Research",
    "authors": "Jonathan Evertz, Niklas Risse, Nicolai Neuer, Andreas Müller, Philipp Normann, Gaetano Sapia, Srishti Gupta, David Pape, Soumya Shaw, Devansh Srivastav, Christian Wressnegger, Erwin Quiring, Thorsten Eisenhofer, Daniel Arp, Lea Schönherr",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09549v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly prevalent in security research. Their unique characteristics, however, introduce challenges that undermine established paradigms of reproducibility, rigor, and evaluation. Prior work has identified common pitfalls in traditional machine learning research, but these studies predate the advent of LLMs. In this paper, we identify \\emph{nine} common pitfalls that have become (more) relevant with the emergence of LLMs and that can compromise the validity of research involving them. These pitfalls span the entire computation process, from data collection, pre-training, and fine-tuning to prompting and evaluation. We assess the prevalence of these pitfalls across all 72 peer-reviewed papers published at leading Security and Software Engineering venues between 2023 and 2024. We find that every paper contains at least one pitfall, and each pitfall appears in multiple papers. Yet only 15.7\\% of the present pitfalls were explicitly discussed, suggesting that the majority remain unrecognized. To understand their practical impact, we conduct four empirical case studies showing how individual pitfalls can mislead evaluation, inflate performance, or impair reproducibility. Based on our findings, we offer actionable guidelines to support the community in future work.",
    "title_zh": "追影逐迹：大语言模型安全研究中的陷阱",
    "abstract_zh": "大型语言模型（LLMs）在安全研究中的应用日益广泛。然而，其独特的特性引入了诸多挑战，这些挑战正在削弱传统研究中可复现性、严谨性和评估标准的根基。以往的研究已识别出传统机器学习研究中常见的陷阱，但这些研究均出现在大语言模型出现之前。本文我们识别出**九个**随着LLM的兴起而变得（更加）重要的常见陷阱，这些陷阱可能损害涉及LLM的研究的有效性。这些陷阱贯穿整个计算流程，从数据收集、预训练、微调，到提示工程和评估环节均有体现。我们对2023至2024年间发表于顶级安全与软件工程会议上的全部72篇同行评审论文进行了分析，发现每篇论文都至少包含一个陷阱，且每个陷阱均在多篇论文中出现。然而，仅有15.7%的陷阱被作者明确讨论，表明绝大多数问题仍未被充分认识。为理解这些陷阱的实际影响，我们开展了四项实证案例研究，揭示个别陷阱如何导致评估误导、虚增性能表现或破坏研究可复现性。基于上述发现，我们提出了切实可行的指导建议，以支持学术界在未来研究中规避这些问题。"
  },
  {
    "date": "2025-12-10",
    "title": "Transpiling quantum circuits by a transformers-based algorithm",
    "authors": "Michele Banfi, Paolo Zentilini, Sebastiano Corli, Enrico Prati",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09834v1",
    "source": "arXiv",
    "abstract": "Transformers have gained popularity in machine learning due to their application in the field of natural language processing. They manipulate and process text efficiently, capturing long-range dependencies among data and performing the next word prediction. On the other hand, gate-based quantum computing is based on controlling the register of qubits in the quantum hardware by applying a sequence of gates, a process which can be interpreted as a low level text programming language. We develop a transformer model capable of transpiling quantum circuits from the qasm standard to other sets of gates native suited for a specific target quantum hardware, in our case the set for the trapped-ion quantum computers of IonQ. The feasibility of a translation up to five qubits is demonstrated with a percentage of correctly transpiled target circuits equal or superior to 99.98%. Regardless the depth of the register and the number of gates applied, we prove that the complexity of the transformer model scales, in the worst case scenario, with a polynomial trend by increasing the depth of the register and the length of the circuit, allowing models with a higher number of parameters to be efficiently trained on HPC infrastructures.",
    "title_zh": "基于Transformer算法的量子电路编译",
    "abstract_zh": "变压器在机器学习领域因其在自然语言处理中的应用而广受欢迎。它们能够高效地处理和操作文本，捕捉数据之间的长距离依赖关系，并实现下一个词的预测。另一方面，基于门的量子计算是通过在量子硬件中对量子比特寄存器施加一系列量子门来实现控制，这一过程可被理解为一种低层级的文本编程语言。我们开发了一种变压器模型，能够将符合qasm标准的量子电路转换为适用于特定目标量子硬件的门集，本研究中针对的是IonQ公司囚禁离子量子计算机所使用的门集。实验验证了该模型在最多五量子比特情况下的可行性，正确转换的目标电路比例达到或超过99.98%。无论寄存器深度和所应用的门数如何，我们证明该变压器模型的复杂度在最坏情况下随寄存器深度和电路长度呈多项式增长趋势，从而使得具有更多参数的模型能够在高性能计算（HPC）基础设施上高效训练。"
  },
  {
    "date": "2025-12-10",
    "title": "Towards Language Model Guided TLA+ Proof Automation",
    "authors": "Yuhao Zhou, Stavros Tripakis",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09758v1",
    "source": "arXiv",
    "abstract": "Formal theorem proving with TLA+ provides rigorous guarantees for system specifications, but constructing proofs requires substantial expertise and effort. While large language models have shown promise in automating proofs for tactic-based theorem provers like Lean, applying these approaches directly to TLA+ faces significant challenges due to the unique hierarchical proof structure of the TLA+ proof system. We present a prompt-based approach that leverages LLMs to guide hierarchical decomposition of complex proof obligations into simpler sub-claims, while relying on symbolic provers for verification. Our key insight is to constrain LLMs to generate normalized claim decompositions rather than complete proofs, significantly reducing syntax errors. We also introduce a benchmark suite of 119 theorems adapted from (1) established mathematical collections and (2) inductive proofs of distributed protocols. Our approach consistently outperforms baseline methods across the benchmark suite.",
    "title_zh": "面向语言模型引导的TLA+证明自动化",
    "abstract_zh": "使用TLA+进行形式化定理证明能够为系统规范提供严格的保证，但构建证明需要大量的专业知识和精力。尽管大型语言模型在自动化基于策略的定理证明器（如Lean）的证明方面展现出巨大潜力，但将这些方法直接应用于TLA+却面临重大挑战，原因在于TLA+证明系统具有独特的分层证明结构。本文提出一种基于提示（prompt-based）的方法，利用大语言模型（LLM）指导将复杂的证明目标逐步分解为更简单的子命题，同时依赖符号证明工具完成验证。我们的核心思想是限制LLM仅生成标准化的命题分解，而非完整的证明，从而显著减少语法错误。此外，我们还构建了一个包含119个定理的基准测试集，这些定理来源于（1）已确立的数学文献以及（2）分布式协议的归纳证明。实验结果表明，该方法在基准测试集上始终优于基线方法。"
  },
  {
    "date": "2025-12-10",
    "title": "Recoverable Lock-Free Locks",
    "authors": "Hagit Attiya, Panagiota Fatourou, Eleftherios Kosmas, Yuanhao Wei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09710v1",
    "source": "arXiv",
    "abstract": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.",
    "title_zh": "可恢复的无锁锁",
    "abstract_zh": "本文提出了首个同时引入无锁性（lock-freedom）和可恢复性（recoverability）的转换方法。该转换从基于锁的实现出发，为锁获取和锁释放操作提供了一种可恢复的无锁替代方案。为了增强通用性，该转换支持嵌套锁，并且在确保对所应用的基于锁实现的正确性不造成损害的前提下，保证了系统的可恢复性。"
  },
  {
    "date": "2025-12-10",
    "title": "FLARE v2: A Recursive Framework for Program Comprehension Across Languages and Levels of Abstraction",
    "authors": "Justin Heath",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09261v1",
    "source": "arXiv",
    "abstract": "Building on the classroom framework reported in Heath et al. (2025), this paper proposes FLARE v2 as a recursive, semiotically informed account of how program meaning is constructed. It reinterprets the descriptive tiers of FLARE v1 as instances of a single generative operation: identify elements (characterised by the four properties Receives, Sends, Effects, Shares); analyse their bindings along two dimensions (Causal-Temporal and Communicative); and recognise the new element that emerges. The Causal-Temporal dimension encompasses three subtypes - Sequential, Branch, and Event - that together account for control flow in both procedural and event-driven environments. A Compositional Ladder provides a visual parallel between literacy progressions and programming structures, illustrating how recursive composition operates from blocks and statements through segments, systems, and services. The framework aims to address conceptual and cognitive-load limitations reported in FLARE v1 and is situated within semiotic and program-comprehension theory. FLARE v2 is presented as a conceptual lens with potential implications for pedagogy and curriculum design; implementation and empirical evaluation are left for future work.",
    "title_zh": "FLARE v2：一种跨语言及多抽象层次的程序理解递归框架",
    "abstract_zh": "基于Heath等人（2025）报告的课堂框架，本文提出FLARE v2，这是一种递归的、受符号学启发的程序意义建构机制。它将FLARE v1中的描述层级重新诠释为单一生成操作的实例：识别元素（由“接收”“发送”“影响”“共享”四个属性表征）；沿两个维度分析其绑定关系（因果-时间维度与交际维度）；并识别由此产生的新元素。其中，因果-时间维度包含三种子类型——顺序型、分支型和事件型，共同解释了在过程式与事件驱动环境中的控制流。一个“组合阶梯”提供了读写能力发展路径与编程结构之间的视觉类比，展示了从代码块与语句到段落、系统乃至服务的递归组合过程。该框架旨在解决FLARE v1中所报告的概念性与认知负荷局限，并植根于符号学与程序理解理论。FLARE v2被呈现为一种概念性视角，具有对教学法与课程设计的潜在启示意义；其具体实施与实证评估则留待未来研究。"
  },
  {
    "date": "2025-12-10",
    "title": "GEARS - A Fully Run-Time Configurable Geant4 Application",
    "authors": "Jing Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09246v1",
    "source": "arXiv",
    "abstract": "The Geant4 toolkit is the standard for simulating the passage of particles through matter, but its conventional architecture often requires users to modify and recompile C++ code to alter fundamental simulation parameters such as geometry, physics list, and primary particle source. This architectural constraint introduces significant friction for new users and slows down the experimental iteration cycle. This paper introduces GEARS (Geant4 Example Application with Rich features yet Small footprint), a universally applicable Geant4 application that fundamentally addresses this issue. GEARS achieves complete simulation configurability without C++ recompilation by strictly utilizing external configuration methods: Geometry is defined via simple text-based configuration, the Physics List is selected via the standard PHYSLIST environment variable, and the Primary Source is defined through the General Particle Source (GPS) macro commands. Furthermore, regarding GEARS as an application instead of a framework, key features include a flat ntuple structure with short variable names for highly efficient analysis and a solution for capturing vital step zero data. Output creation is also fully managed via run-time macro commands and volume properties. The project is distributed as a ready-to-use Docker container to eliminate compilation barriers. Through these design considerations, GEARS transforms Geant4 into a practical, ready-to-use tool, enabling users to rapidly prototype and execute simulations for diverse experiments solely through simple text configuration files, without ever needing to modify or compile the underlying C++ source code.",
    "title_zh": "GEARS - 一个完全可在运行时配置的 Geant4 应用程序",
    "abstract_zh": "Geant4 工具包是模拟粒子穿过物质过程的标准工具，但其传统架构通常要求用户修改并重新编译 C++ 代码，才能更改几何结构、物理列表和初级粒子源等基本仿真参数。这种架构限制给新手带来了显著的学习障碍，并拖慢了实验迭代周期。本文介绍了 GEARS（具有丰富功能 yet 小体积的 Geant4 示例应用），这是一种通用适用的 Geant4 应用程序，从根本上解决了上述问题。GEARS 通过严格采用外部配置方式，实现了无需重新编译 C++ 代码即可完全配置整个仿真过程：几何结构通过简单的文本配置文件定义，物理列表通过标准的 PHYSLIST 环境变量选择，初级粒子源则通过通用粒子源（GPS）宏命令进行定义。此外，将 GEARS 视为一个独立应用程序而非框架，其关键特性包括：采用扁平化的 ntuple 结构，使用简短的变量名以实现高效分析；以及对至关重要的“第一步”数据的完整捕获方案。输出文件的生成也完全通过运行时宏命令和体积属性来管理。该项目以现成可用的 Docker 容器形式发布，彻底消除了编译障碍。通过这些设计考量，GEARS 成功将 Geant4 转变为一个实用且开箱即用的工具，使用户仅通过简单的文本配置文件即可快速原型设计并执行各种实验的仿真，而无需修改或编译底层 C++ 源代码。"
  },
  {
    "date": "2025-12-10",
    "title": "Analysis of the Security Design, Engineering, and Implementation of the SecureDNA System",
    "authors": "Alan T. Sherman, Jeremy J. Romanik Romano, Edward Zieglar, Enis Golaszewski, Jonathan D. Fuchs, William E. Byrd",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09233v1",
    "source": "arXiv",
    "abstract": "We analyze security aspects of the SecureDNA system regarding its system design, engineering, and implementation. This system enables DNA synthesizers to screen order requests against a database of hazards. By applying novel cryptography, the system aims to keep order requests and the database of hazards secret. Discerning the detailed operation of the system in part from source code (Version 1.0.8), our analysis examines key management, certificate infrastructure, authentication, and rate-limiting mechanisms. We also perform the first formal-methods analysis of the mutual authentication, basic request, and exemption-handling protocols. Without breaking the cryptography, our main finding is that SecureDNA's custom mutual authentication protocol SCEP achieves only one-way authentication: the hazards database and keyservers never learn with whom they communicate. This structural weakness violates the principle of defense in depth and enables an adversary to circumvent rate limits that protect the secrecy of the hazards database, if the synthesizer connects with a malicious or corrupted keyserver or hashed database. We point out an additional structural weakness that also violates the principle of defense in depth: inadequate cryptographic bindings prevent the system from detecting if responses, within a TLS channel, from the hazards database were modified. Consequently, if a synthesizer were to reconnect with the database over the same TLS session, an adversary could replay and swap responses from the database without breaking TLS. Although the SecureDNA implementation does not allow such reconnections, it would be stronger security engineering to avoid the underlying structural weakness. We identify these vulnerabilities and suggest and verify mitigations, including adding strong bindings. Software Version 1.1.0 fixes SCEP with our proposed SCEP+ protocol.",
    "title_zh": "SecureDNA 系统的安全设计、工程与实现分析",
    "abstract_zh": "我们对SecureDNA系统的安全性进行了分析，涵盖其系统设计、工程实现与具体实施。该系统使DNA合成设备能够根据一个危险数据库筛查订单请求。通过应用新型密码学技术，系统旨在保护订单请求及危险数据库的机密性。基于源代码（版本1.0.8）的部分细节，我们的分析重点考察了密钥管理、证书基础设施、身份认证以及速率限制机制。此外，我们首次对相互认证、基本请求处理和豁免处理协议进行了形式化方法分析。\n\n在不破解密码学的前提下，我们的主要发现是：SecureDNA自定义的相互认证协议SCEP仅实现了单向认证——即危险数据库和密钥服务器始终无法知晓它们正在与谁通信。这一结构性缺陷违背了纵深防御原则，使得攻击者在合成设备连接到恶意或被篡改的密钥服务器或哈希数据库时，可绕过保护危险数据库机密性的速率限制机制。\n\n我们还指出另一项同样违反纵深防御原则的结构性弱点：加密绑定不足，导致系统无法检测TLS通道中来自危险数据库的响应是否被篡改。因此，若合成设备在同一TLS会话中重新连接数据库，攻击者可在不破坏TLS安全性的前提下，重放并替换数据库的响应。尽管当前SecureDNA实现禁止此类重连，但更优的安全工程实践应消除其根本的结构性缺陷。\n\n我们识别出这些漏洞，并提出并验证了相应的缓解措施，包括引入强加密绑定。软件版本1.1.0已根据我们提出的SCEP+协议修复了SCEP协议。"
  },
  {
    "date": "2025-12-10",
    "title": "Baseline: Operation-Based Evolution and Versioning of Data",
    "authors": "Jonathan Edwards, Tomas Petricek",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09762v1",
    "source": "arXiv",
    "abstract": "Baseline is a platform for richly structured data supporting change in multiple dimensions: mutation over time, collaboration across space, and evolution through design changes. It is built upon Operational Differencing, a new technique for managing data in terms of high-level operations that include refactorings and schema changes. We use operational differencing to construct an operation-based form of version control on data structures used in programming languages and relational databases. This approach to data version control does fine-grained diffing and merging despite intervening structural transformations like schema changes. It offers users a simplified conceptual model of version control for ad hoc usage: There is no repo; Branching is just copying. The informaton maintained in a repo can be synthesized more precisely from the append-only histories of branches. Branches can be flexibly shared as is commonly done with document files, except with the added benefit of diffing and merging. We conjecture that queries can be operationalized into a sequence of schema and data operations. We develop that idea on a query language fragment containing selects and joins. Operationalized queries are represented as a future timeline that is speculatively executed as a branch off of the present state, returning a value from its hypothetical future. Operationalized queries get rewritten to accommodate schema change \"for free\" by the machinery of operational differencing. Altogether we develop solutions to four of the eight challenge problems of schema evolution identified in a recent paper.",
    "title_zh": "基线：基于操作的数据演化与版本控制",
    "abstract_zh": "Baseline 是一个支持多维度变化的丰富结构化数据平台，涵盖时间上的变异、空间上的协作以及通过设计变更的演化。它基于一种名为“操作差异（Operational Differencing）”的新技术，该技术以高层次操作（包括重构和模式变更）来管理数据。我们利用操作差异，在编程语言和关系数据库所使用的数据结构上构建了一种基于操作的版本控制形式。这种数据版本控制方法即使在经历诸如模式变更等中间结构性转换的情况下，仍能实现细粒度的差异比对与合并。\n\n该方法为用户提供了简化直观的版本控制概念模型，适用于临时性使用：无需仓库（repo），分支仅需复制即可创建。原本存储在仓库中的信息，可以通过各分支的追加式历史记录更精确地还原。分支可以像文档文件一样灵活共享，同时额外具备差异比对和合并的能力。\n\n我们推测，查询可被转化为一系列模式与数据操作的序列。我们在一个包含选择（selects）和连接（joins）的查询语言片段中发展了这一思想。经过操作化的查询被表示为一条未来时间线，作为从当前状态分叉出的一个假设性分支进行推测性执行，并返回其假想未来的结果。借助操作差异机制，这些操作化查询能够“免费”适应模式变更。总体而言，我们解决了近期一篇论文中提出的八个模式演化挑战问题中的四个。"
  },
  {
    "date": "2025-12-10",
    "title": "Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers",
    "authors": "Zhaolan Huang, Kaspar Schleiser, Gyungmin Myung, Emmanuel Baccelli",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09800v1",
    "source": "arXiv",
    "abstract": "Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.",
    "title_zh": "Ariel-ML：基于嵌入式 Rust 在异构多核微控制器上实现神经网络的并行计算",
    "abstract_zh": "低功耗微控制器（MCU）硬件正从传统的单核架构逐步演变为以多核架构为主。与此同时，新的嵌入式软件组件越来越多地使用 Rust 语言编写，而 C/C++ 在该领域的主导地位则逐渐减弱。另一方面，各种小型人工神经网络（ANN）在边缘人工智能（Edge AI）应用场景中日益普及，因此可直接部署并运行在低功耗 MCU 上。在此背景下，无论是渐进式的改进还是全新的创新服务，都必须持续通过在已部署于现场的传感/执行系统中嵌入的软件实现 ANN 推理来完成更新与适配。然而，迄今为止，尚缺乏一个基于 Rust 的嵌入式软件平台，能够自动实现针对多核 MCU 上任意 TinyML 模型的推理计算并行化。本文正是为填补这一空白，提出了一种名为 Ariel-ML 的新型工具包。Ariel-ML 结合了通用的 TinyML 处理流程与一个嵌入式 Rust 软件平台，能够充分利用多种 32 位微控制器家族（如 Arm Cortex-M、RISC-V 和 ESP-32）的多核处理能力。我们已将其实现的完整开源代码发布，并通过一系列不同类型的 TinyML 模型对其性能进行了基准测试。结果表明，正如预期，Ariel-ML 在推理延迟方面优于现有技术；同时，与现有的基于嵌入式 C/C++ 的工具包相比，其内存占用也达到了相当水平。因此，Ariel-ML 为 TinyML 实践者以及资源受限的嵌入式 Rust 开发者提供了一个极具价值的技术基础。"
  },
  {
    "date": "2025-12-10",
    "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
    "authors": "Emanuele La Malfa, Ping Zhu, Samuele Marro, Sara Bernardini, Michael Wooldridge",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09629v1",
    "source": "arXiv",
    "abstract": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",
    "title_zh": "一种基于智能体大模型与PDDL的端到端规划框架",
    "abstract_zh": "我们提出了一种由验证器支持的端到端规划框架。一个协调器接收以自然语言编写的用户需求，并将其转换为PDDL（规划领域定义语言）模型，其中领域和问题通过子模块（代理）进行迭代优化，以解决常见的规划需求，如时间约束、最优性，以及人类描述中可能存在的模糊性和矛盾之处。经过验证的领域和问题随后被传递给外部规划引擎以生成规划方案。协调器与各代理均由大型语言模型（LLMs）驱动，在整个过程中无需任何人工干预。最后，一个模块将最终的规划结果重新翻译回自然语言，以提升人类可读性，同时确保每一步的正确性。我们在多个领域和任务中展示了该框架的灵活性与有效性，包括Google NaturalPlan基准测试、PlanBench，以及Blocksworld和汉诺塔等规划问题（这些任务对LLM而言即使在小规模实例上也常存在困难）。我们的框架可与任意PDDL规划引擎和验证工具（如Fast Downward、LPG、POPF、VAL和uVAL，我们已进行了测试）集成，标志着基于LLM的端到端规划迈出了重要一步。"
  },
  {
    "date": "2025-12-10",
    "title": "Supporting Dynamic Agentic Workloads: How Data and Agents Interact",
    "authors": "Ioana Giurgiu, Michael E. Nidd",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09548v1",
    "source": "arXiv",
    "abstract": "The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.",
    "title_zh": "支持动态智能体工作负载：数据与智能体的交互方式",
    "abstract_zh": "由大型语言模型（LLMs）和专用推理代理驱动的多智能体系统兴起，暴露出当前数据管理架构的根本性局限。传统的数据库与数据编织（data fabrics）是为静态、定义明确的工作负载而设计的，而智能体系统则表现出动态、上下文驱动且协作性强的行为特征。智能体持续分解任务，在不同模态间切换注意力，并与同伴共享中间结果——生成非确定性、多模态的工作负载，对传统查询优化器和缓存机制构成巨大压力。我们提出一种以智能体为中心的数据编织（Agent-Centric Data Fabric），这是一种统一的架构范式，重新思考数据系统如何服务于、优化、协调并从智能体工作负载中学习。为此，我们引入了注意力引导的数据检索、面向上下文驱动智能体联邦的语义微缓存、预测性数据预取以及基于多数共识的数据服务等概念。这些机制共同作用，使智能体能够更快、更高效地获取代表性数据，同时减少冗余查询、数据移动以及系统间的推理负担。通过将数据系统视为动态的协作者而非静态执行者，我们勾勒出面向行为响应型数据基础设施的新研究方向：缓存、探测与编排协同运作，实现动态、推理驱动的智能体之间高效且富含上下文的数据交换。"
  },
  {
    "date": "2025-12-10",
    "title": "CORE: A Conceptual Reasoning Layer for Large Language Models",
    "authors": "Vishwas Hegde, Vindhya Shigehalli",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09222v1",
    "source": "arXiv",
    "abstract": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.",
    "title_zh": "CORE：大语言模型的概念推理层",
    "abstract_zh": "大型语言模型在单轮生成任务中表现良好，但在多轮交互中仍需从不断扩展的token历史中重新构建用户意图和任务状态，因为模型内部表示无法跨轮次持续保留。这种以token为中心的范式导致了语义漂移、推理模式不一致以及随着对话深入而不断增长的提示（prompt）长度。我们提出CORE——一种概念优先的交互层，可在不修改模型权重的情况下提升多轮交互的稳定性。CORE结合了一个小型通用认知操作符库与一个持久化的局部概念（Local Concept），该概念是一种紧凑的语义状态，能够捕捉任务目标、约束条件、用户偏好以及中间结果。每次模型调用仅需接收这一概念状态、用户的最新指令以及所选的操作符，无需重复回放完整的历史记录。初步原型模拟CORE的行为显示，累计提示token数量减少了约42%；但此数据反映的是原型环境下的情况，不应被解读为实际应用中的性能指标。CORE提供了一种与模型无关的机制，将概念性推理与语言生成相分离，为构建更稳定、可扩展的多轮交互系统指明了可行方向。"
  },
  {
    "date": "2025-12-10",
    "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks",
    "authors": "Xinye Cao, Yihan Lin, Guoshun Nan, Qinchuan Zhou, Yuhang Luo, Yurui Gao, Zeliang Zhang, Haolang Lu, Qimei Cui, Yanzhao Hou, Xiaofeng Tao, Tony Q. S. Quek",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09485v1",
    "source": "arXiv",
    "abstract": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.",
    "title_zh": "基于大语言模型的零接触网络安全性自动化：面向定制化组相对策略优化",
    "abstract_zh": "零接触网络（Zero-Touch Networks, ZTNs）代表了一种向全自动化与智能化网络管理转型的变革性范式，能够为第六代（6G）网络所面临的复杂性提供所需的可扩展性和适应性。然而，6G网络所具有的分布式架构、高度开放性以及深度异构性，显著扩大了攻击面，带来了前所未有的安全挑战。为应对这一问题，安全自动化旨在实现动态复杂环境下的智能安全管理，成为保障6G ZTN安全的关键能力。尽管前景广阔，但在6G ZTN中实施安全自动化仍面临两大核心挑战：1）在真实世界中并行且对抗性的条件下，实现从安全策略生成、验证到更新的全生命周期自动化；2）使安全策略能够适应不断演化的威胁和动态变化的环境。为此，我们提出SecLoop与SA-GRPO。\n\nSecLoop是首个完全自动化的框架，首次将大语言模型（LLMs）贯穿于安全策略生成、编排、响应与反馈的整个生命周期，实现了在动态网络环境中智能且自适应的安全防御，有效解决了第一个挑战。此外，我们提出了SA-GRPO——一种新颖的安全感知组相对策略优化算法，通过对比来自多个并行SecLoop执行实例所收集的群体反馈，迭代优化安全策略，从而应对第二个挑战。\n\n在五个基准测试平台上的大量真实世界实验表明，包括11个MITRE ATT&CK攻击流程及超过20类攻击在内的综合评估中，所提出的SecLoop与SA-GRPO展现出显著优越性。我们计划将该平台开源发布至社区，以推动安全自动化向下一代通信技术的持续演进。"
  },
  {
    "date": "2025-12-10",
    "title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks",
    "authors": "Uisang Lee, Changhoon Chung, Junmo Lee, Soo-Mook Moon",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09385v1",
    "source": "arXiv",
    "abstract": "The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.",
    "title_zh": "BugSweeper：基于图神经网络的智能合约漏洞函数级检测",
    "abstract_zh": "以太坊的迅速发展使得快速且准确地检测智能合约漏洞变得愈发重要。尽管基于机器学习的方法已展现出一定潜力，但许多方法仍依赖于领域专家设计的基于规则的预处理步骤。这些基于规则的预处理方法常常会丢弃源代码中的关键上下文信息，可能导致某些漏洞被遗漏，并限制了对新出现威胁的适应能力。我们提出了 BugSweeper，一种端到端的深度学习框架，能够直接从源代码中检测漏洞，无需人工特征工程。BugSweeper 将每个 Solidity 函数表示为函数级抽象语法图（FLAG），这是一种新颖的图结构，它将抽象语法树（AST）与增强的控制流和数据流语义相结合。随后，我们的两阶段图神经网络（GNN）对这些图进行分析：第一阶段 GNN 用于过滤语法图中的噪声，第二阶段 GNN 则执行高层次推理，以检测多种类型的漏洞。在真实世界合约上的大量实验表明，BugSweeper 显著优于所有现有的先进检测方法。通过摒弃手工设计规则的需求，我们的方法提供了一种强大、自动化且可扩展的解决方案，能够在不依赖安全专家的情况下有效保障智能合约的安全性。"
  },
  {
    "date": "2025-12-10",
    "title": "From Forecast to Action: Uncertainty-Aware UAV Deployment for Ocean Drifter Recovery",
    "authors": "Jingeun Kim, Yong-Hyuk Kim, Yourim Yoon",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09260v1",
    "source": "arXiv",
    "abstract": "We present a novel predict-then-optimize framework for maritime search operations that integrates trajectory forecasting with UAV deployment optimization-an end-to-end approach not addressed in prior work. A large language model predicts the drifter's trajectory, and spatial uncertainty is modeled using Gaussian-based particle sampling. Unlike traditional static deployment methods, we dynamically adapt UAV detection radii based on distance and optimize their placement using meta-heuristic algorithms. Experiments on real-world data from the Korean coastline demonstrate that our method, particularly the repair mechanism designed for this problem, significantly outperforms the random search baselines. This work introduces a practical and robust integration of trajectory prediction and spatial optimization for intelligent maritime rescue.",
    "title_zh": "从预测到行动：面向海洋漂流器回收的不确定性感知无人机部署",
    "abstract_zh": "我们提出了一种新颖的“先预测后优化”框架，用于海上搜救任务，该框架将轨迹预测与无人机部署优化相结合，是一种端到端的方法，此前的研究尚未涉及。通过大型语言模型预测漂流器的运动轨迹，并采用基于高斯分布的粒子采样方法对空间不确定性进行建模。与传统的静态部署方法不同，我们的方法根据距离动态调整无人机的探测半径，并利用元启发式算法优化其部署位置。在韩国海岸线真实数据上的实验表明，本方法——尤其是针对该问题设计的修复机制——显著优于随机搜索基线方法。本研究为智能海上救援引入了实用且稳健的轨迹预测与空间优化融合方案。"
  },
  {
    "date": "2025-12-10",
    "title": "Latent-Autoregressive GP-VAE Language Model",
    "authors": "Yves Ruffenach",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09535v1",
    "source": "arXiv",
    "abstract": "We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.",
    "title_zh": "潜在自回归GP-VAE语言模型",
    "abstract_zh": "我们研究了一种基于高斯过程（GP）的完全隐式自回归方案，该方案被整合进变分自编码器（VAE）中。在此框架下，序列动态从观测空间转移到连续的隐含空间，而语言生成则通过非自回归解码器保持并行性。本文提出了一个完整的方法论框架，包括因果GP先验、结构化的近似后验以及基于正则化ELBO的训练策略。在刻意受限的原型验证（POC）框架内进行的实证评估表明，该模型能够稳定训练，且序列采样与并行采样两种方式表现出一致的行为。总体而言，结果表明，语言模型中的部分时序结构可由隐含空间的概率几何特性来支持，而非依赖于显式的神经运算。"
  },
  {
    "date": "2025-12-10",
    "title": "ZeroOS: A Universal Modular Library OS for zkVMs",
    "authors": "Guangxian Zou, Isaac Zhang, Ryan Zarick, Kelvin Wong, Thomas Kim, Daniel L. -K. Wong, Saeid Yazdinejad, Dan Boneh",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.09300v1",
    "source": "arXiv",
    "abstract": "zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.",
    "title_zh": "ZeroOS：一种适用于zkVM的通用模块化库操作系统",
    "abstract_zh": "zkVMs 通过与现代程序和工具链在指令集架构（ISA）层面的兼容性，承诺实现通用的可验证计算。然而，兼容性远不止于 ISA 层面；现代程序通常无法在没有操作系统和 libc 的情况下运行甚至编译。为解决这一问题，zkVMs 通常会维护特定语言运行时的分支，并将其静态链接到应用程序中，以构建自包含的unikernel。但这种临时性的方法导致了版本混乱，使可验证应用（vApps）承担了不必要的庞大可信计算基（TCB）。我们通过 ZeroOS 解决了这一难题：ZeroOS 是专为 vApp unikernel 设计的模块化库操作系统（libOS），vApp 开发者可以使用现成的工具链，仅编译和链接其应用所需的 Linux ABI 的精确子集。任何 zkVM 团队只需为其平台编写一个 ZeroOS 启动加载器，即可轻松接入 ZeroOS 生态系统，从而显著降低维护负担，并通过整合开发与审计资源，统一整个 zkVM 生态。ZeroOS 已开源，免费获取，地址为 https://github.com/LayerZero-Labs/ZeroOS。"
  },
  {
    "date": "2025-12-10",
    "title": "The Trillion-Dollar Cost of IT's Willful Ignorance: Software Disasters are Predictable and Avoidable",
    "authors": "Robert N. Charette",
    "publish": "IEEE Spectrum",
    "url": "https://doi.org/10.1109/mspec.2025.11297124",
    "source": "IEEE",
    "abstract": "KGB Chairman Charkov's question to inorganic chemist Valery Legasov in HBO's “Chernobyl” miniseries makes a good epitaph for the hundreds of software development, modernization, and operational failures I have covered for IEEE Spectrum since my first contribution, to its September 2005 special issue on learning—or rather, not learning—from software failures. I noted then, and it's still true two decades later: Software failures are universally unbiased. They happen in every country, to large companies and small. They happen in commercial, nonprofit, and governmental organizations, regardless of status or reputation.",
    "title_zh": "IT的故意忽视所造成的万亿美元代价：软件灾难是可预见且可避免的",
    "abstract_zh": "HBO电视剧《切尔诺贝利》中，克格勃主席查尔科夫向无机化学家瓦列里·列加索夫提出的问题，恰如其分地成为我多年来为IEEE Spectrum撰文记录数百起软件开发、现代化及运维失败事件的墓志铭。从我首次投稿于2005年9月特刊——关于从软件故障中学习（或更确切地说，未能学习）——以来，我一直强调的观点至今依然成立：软件故障是普遍存在的，毫无偏见。它们发生在每个国家，影响着大公司和小企业；无论组织是商业性质、非营利性还是政府机构，无论其地位或声誉如何，都可能遭遇此类问题。"
  },
  {
    "date": "2025-12-11",
    "title": "Design and Application of a C++ Compiler Error Solution Query Platform Integrating Large Language Models and Human-in-the-Loop Support",
    "authors": "Xinzheng Dong, Gang Lin",
    "publish": "Proceedings of the 2025 2nd International Symposium on Artificial Intelligence for Education",
    "url": "https://doi.org/10.1145/3775073.3775185",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "结合大语言模型与人机协同支持的C++编译器错误解决方案查询平台的设计与应用",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-10",
    "title": "Low-Latency Implementation of Bitsliced SPN-Cipher on IoT Processors",
    "authors": "Jiahao Xiang, Lang Li",
    "publish": "IEEE Transactions on Computers",
    "url": "https://doi.org/10.1109/tc.2025.3642221",
    "source": "IEEE",
    "abstract": "Bitsliced cipher implementations demonstrate enhanced performance and security on high-end processors through specialized instruction utilization. However, IoT processors, particularly 32-bit architectures, present significant implementation challenges due to limited register sizes and instruction sets, hindering efficient parallelism in bitsliced SPN ciphers. This study presents optimization strategies for implementing bitsliced SPN ciphers on 32-bit processors using common instruction sets. The linear layer optimization employs a decomposition algorithm that transforms complex permutation operations into minimal instruction sequences. This approach recursively identifies optimal instruction combinations while maintaining computational efficiency. For the non-linear layer, operations are constrained to basic logic instructions (NOT, AND, OR, XOR). A novel encoding method for the Bit-slice Gate Complexity (BGC) model is proposed to optimize S-box transformations within these constraints using Boolean satisfiability solvers. Additionally, a comprehensive benchmarking framework facilitates standardized performance evaluation across implementations. Experimental evaluation of the optimized implementations on ARM Cortex-M and Xtensa LX processors demonstrates significant performance improvements. The proposed techniques achieve reductions of 9.7% and 67.6% in Cycles Per Byte for AES and QARMAv2 implementations, respectively.",
    "title_zh": "物联网处理器上比特切片SPN密码的低延迟实现",
    "abstract_zh": "比特切片密码实现通过利用高端处理器中的专用指令，展现出更高的性能与安全性。然而，物联网（IoT）处理器，尤其是32位架构，在实现上面临显著挑战，主要受限于寄存器尺寸较小和指令集有限，难以在比特切片SPN（替代-置换网络）密码中实现高效的并行运算。本研究提出了一种针对32位处理器的比特切片SPN密码优化策略，基于通用指令集进行实现。在线性层优化方面，采用一种分解算法，将复杂的置换操作转化为最少的指令序列，该方法通过递归方式识别最优的指令组合，同时保持计算效率。对于非线性层，所有操作均限制在基本逻辑指令（NOT、AND、OR、XOR）范围内。为此，本文提出一种新型的比特切片门复杂度（BGC）模型编码方法，结合布尔可满足性求解器（SAT Solver），在上述约束条件下优化S盒变换。此外，构建了一个全面的基准测试框架，以实现不同实现方案之间的标准化性能评估。对ARM Cortex-M和Xtensa LX处理器上的优化实现进行实验评估，结果表明性能显著提升：AES实现的每字节周期数（Cycles Per Byte, CPB）降低9.7%，QARMAv2实现则降低67.6%。"
  },
  {
    "date": "2025-12-10",
    "title": "Dependency-Aware Data Parallelism on Spatial CGRA via Constraint Satisfaction and Graph Coloring",
    "authors": "Yuan Dai, Xuchen Gao, Huan Lin, Wenbo Yin, Wai-Shing Luk, Lingli Wang",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2025.3642732",
    "source": "IEEE",
    "abstract": "Coarse-grained Reconfigurable Architecture (CGRA) is a competitive accelerator architecture for computation-intensive loop kernels. Spatial CGRA is a typical CGRA that performs all the operations spatially to reduce reconfiguration costs within a single iteration, demanding high data parallelism. To achieve this goal, one of the main challenges is the loop-carried dependency between memory accesses. Many existing CGRA compilers struggle to precisely analyze the dependency distance, especially when accesses involve complex address patterns. Consequently, these compilers often default to setting the distance to one, based on a worst-case assumption, leading to degraded performance. However, we observe that a precise distance can improve performance significantly, raising the requirement for an efficient distance calculation approach. Another challenge is the performance constraints of single-bank memory, which necessitate the designer partitioning the original data into a multi-bank memory. However, we observe that the mapping result can cause the inter-iteration conflict, thereby invalidating the memory partition scheme. Therefore, an efficient post-mapping conflict detection is required. In this paper, we develop a constraint satisfaction problem (CSP)-based approach for calculating dependency distance and detecting conflicts, which determines the maximum available dependency distance and identifies conflicts within both intra- and inter-iterations. Besides, we formulate access scheduling as a graph coloring problem, which can minimize conflicts and improve performance. Overall, we develop a comprehensive end-to-end framework with architectural and compiler support for efficient data parallelism on spatial CGRA. We conduct extensive experiments to systematically evaluate the impact of different approaches on performance and compilation. Evaluation results show that our architecture can achieve 13.16× and 1.19× (up to 1.68×) average performance improvements compared to a RISC-V CPU and a state-of-the-art CGRA SoC, respectively. Besides, our architecture has 7.38× and 1.18× (up to 1.65×) average energy efficiency gains compared to these two architectures.",
    "title_zh": "基于约束满足与图着色的时空CGRA上依赖感知的数据并行性",
    "abstract_zh": "粗粒度可重构架构（CGRA）是一种针对计算密集型循环内核具有竞争力的加速器架构。空间型CGRA是一种典型的CGRA，它通过在单次迭代内进行全部操作的空间化处理，以降低重配置开销，从而要求较高的数据并行性。为了实现这一目标，主要挑战之一是内存访问之间的循环依赖关系。现有的许多CGRA编译器难以精确分析依赖距离，尤其是在涉及复杂地址模式的访问情况下。因此，这些编译器通常默认将依赖距离设为1，基于最坏情况假设，导致性能下降。然而，我们观察到，若能精确计算依赖距离，则可显著提升性能，这提出了对高效距离计算方法的需求。另一个挑战是单bank内存的性能瓶颈，这要求设计者将原始数据划分为多bank内存结构。但我们也发现，映射结果可能导致跨迭代冲突，从而使内存分区方案失效。因此，亟需一种高效的后映射冲突检测机制。\n\n本文提出了一种基于约束满足问题（CSP）的方法，用于计算依赖距离并检测冲突，该方法能够确定最大可用的依赖距离，并识别出存在于迭代内部及迭代之间的冲突。此外，我们将访问调度建模为图着色问题，以最小化冲突并提升性能。总体而言，我们构建了一个完整的端到端框架，结合了硬件架构与编译器支持，实现了空间型CGRA上的高效数据并行。我们进行了大量实验，系统地评估了不同方法对性能和编译效率的影响。评估结果表明，与RISC-V CPU相比，我们的架构平均性能提升了13.16倍，与当前最先进的CGRA SoC相比，平均性能提升了1.19倍（最高达1.68倍）。此外，在能效方面，相较于这两种架构，我们的设计分别实现了7.38倍和1.18倍（最高达1.65倍）的平均能效提升。"
  },
  {
    "date": "2025-12-10",
    "title": "NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation",
    "authors": "Md. Kamrul Hossain, Walid Aljoby",
    "publish": "IEEE Open Journal of the Communications Society",
    "url": "https://doi.org/10.1109/ojcoms.2025.3642642",
    "source": "IEEE",
    "abstract": "Intent-Based Networking (IBN) often leverages the programmability of Software-Defined Networking (SDN) to simplify network management. However, significant challenges remain in automating the entire pipeline, from user-specified high-level intents to device-specific low-level configurations. Existing solutions often rely on rigid, rule-based translators and fixed APIs, limiting extensibility and adaptability. By contrast, recent advances in large language models (LLMs) offer a promising pathway that leverages natural language understanding and flexible reasoning. However, it is unclear to what extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a first-of-its-kind benchmarking suite comprising eight datasets: Intent2Flow-ODL, Intent2Flow-ONOS, Intent2Flow-Ryu, Intent2Flow-Floodlight, FlowConflict-ODL, FlowConflict-ONOS, FlowConflict-Ryu, and FlowConflict-Floodlight. These datasets are specifically designed for evaluating LLMs performance in intent translation and conflict detection tasks within the industry-grade and research-focused SDN controllers such as ODL, ONOS, Ryu, and Floodlight. Our results provide the first comprehensive comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a wide range of performance outcomes. However, while these results demonstrate the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully autonomous IBN pipeline remains unexplored. Thus, our second contribution is NetIntent, a unified and adaptable framework that leverages LLMs to automate the full IBN lifecycle, including translation, activation, and assurance within SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting dynamic re-prompting and contextual feedback to robustly execute user-defined intents with minimal human intervention. Our implementation of NetIntent across ODL, ONOS, Ryu, and Floodlight achieves a consistent and adaptive end-to-end IBN realization.",
    "title_zh": "NetIntent：利用大型语言模型实现端到端基于意图的SDN自动化",
    "abstract_zh": "基于意图的网络（Intent-Based Networking, IBN）通常利用软件定义网络（Software-Defined Networking, SDN）的可编程性来简化网络管理。然而，从用户指定的高层意图到设备特定的低层配置，实现整个自动化流程仍面临重大挑战。现有解决方案多依赖于僵化的规则驱动翻译器和固定的API接口，限制了系统的可扩展性和适应性。相比之下，大型语言模型（Large Language Models, LLMs）近年来的发展为解决这些问题提供了新路径，其具备自然语言理解与灵活推理能力。但目前尚不清楚LLMs在IBN任务中的实际表现程度。为此，我们提出了IBNBench——首个针对该领域的基准测试套件，包含八个数据集：Intent2Flow-ODL、Intent2Flow-ONOS、Intent2Flow-Ryu、Intent2Flow-Floodlight、FlowConflict-ODL、FlowConflict-ONOS、FlowConflict-Ryu 和 FlowConflict-Floodlight。这些数据集专为评估LLMs在工业级与研究型SDN控制器（如ODL、ONOS、Ryu和Floodlight）中执行意图翻译与冲突检测任务的表现而设计。\n\n我们的实验结果首次对33个开源LLMs在IBNBench及相关数据集上的性能进行了全面比较，揭示了显著的性能差异。尽管这些结果展示了LLMs在独立IBN任务中的潜力，但将LLMs集成到完全自主的IBN全流程中仍处于探索阶段。因此，我们的第二项贡献是NetIntent——一个统一且可适应的框架，利用LLMs实现SDN系统中IBN全生命周期的自动化，涵盖意图翻译、配置激活与服务保障等环节。NetIntent协调LLM与非LLM代理协同工作，支持动态重提示（re-prompting）和上下文反馈机制，能够在极少人工干预的情况下稳健地执行用户定义的意图。我们在ODL、ONOS、Ryu和Floodlight等多个控制器上实现了NetIntent的部署，成功实现了稳定且自适应的端到端IBN功能。"
  },
  {
    "date": "2025-12-10",
    "title": "An XSS Attack Detection Model Based on Two-Stage AST Analysis",
    "authors": "Qiuhua Wang, Chuangchuang Li, Lifeng Yuan, Dong Wang, Yeru Wang, Yizhi Ren, Weizhi Meng",
    "publish": "IEEE Transactions on Dependable and Secure Computing",
    "url": "https://doi.org/10.1109/tdsc.2025.3642307",
    "source": "IEEE",
    "abstract": "Cross-site scripting (XSS) attacks pose a significant threat to web applications and user privacy, with the number of such attacks rapidly increasing. Although existing machine learning and deep learning-based XSS attack detection models are effective against common XSS attacks, these models all overlook their own security and often fail to defend against adversarial samples that exploit model vulnerabilities, allowing attackers to successfully bypass these models by using XSS adversarial samples. To address this challenge, in this paper, we propose a novel XSS attack detection model based on two-stage Abstract Syntax Tree (AST) analysis and Long Short-Term Memory (LSTM) neural networks, effectively mitigating the impact of adversarial samples. Our model leverages the ability of AST parsing and analysis of HTML and JavaScript code to effectively eliminate redundant information and adversarial perturbations introduced by adversarial samples. The two-stage process first extracts JavaScript code from the HTML AST, then identifies malicious code fragments from the JavaScript AST. Finally, the LSTM neural network is trained to classify samples as malicious or benign. By analyzing the HTML and JavaScript components of web pages, our model identifies and eliminates adversarial perturbations that interfere with detection, significantly enhancing the security and reliability of the detection process. Extensive experiments on real datasets demonstrate our model's superior performance, achieving an accuracy rate of 0.991 and an F1 score of 0.998 against standard XSS samples, outperforming existing models. More importantly, when facing adversarial XSS samples, most existing detection models exhibit severe robustness degradation with the detection rate (DR) below 0.880, whereas our model maintains a detection rate of over 0.982, significantly higher than state-of-the-art models and demonstrating its significant effectiveness in defending against XSS adversarial attacks.",
    "title_zh": "基于两阶段抽象语法树分析的XSS攻击检测模型",
    "abstract_zh": "跨站脚本（XSS）攻击对Web应用和用户隐私构成重大威胁，且此类攻击的数量正在迅速增加。尽管现有的基于机器学习和深度学习的XSS攻击检测模型在应对常见XSS攻击方面表现良好，但这些模型普遍忽视了自身的安全性，往往无法有效防御利用模型漏洞的对抗样本，导致攻击者能够通过使用XSS对抗样本成功绕过这些检测系统。为应对这一挑战，本文提出一种基于两阶段抽象语法树（AST）分析与长短期记忆（LSTM）神经网络相结合的新型XSS攻击检测模型，有效缓解对抗样本带来的影响。该模型充分利用AST对HTML和JavaScript代码进行解析与分析的能力，能够有效消除冗余信息以及对抗样本引入的扰动。整个两阶段流程首先从HTML的抽象语法树中提取JavaScript代码，随后在JavaScript的抽象语法树中识别恶意代码片段。最后，使用LSTM神经网络对样本进行分类，判断其是否为恶意。通过分析网页的HTML与JavaScript组件，本模型能够识别并剔除干扰检测的对抗性扰动，显著提升了检测过程的安全性与可靠性。在真实数据集上的大量实验表明，本模型性能优越，对标准XSS样本的准确率高达0.991，F1分数达到0.998，优于现有各类模型。更重要的是，在面对对抗性XSS样本时，大多数现有检测模型表现出严重的鲁棒性下降，检测率（DR）低于0.880；而本模型的检测率仍保持在0.982以上，显著高于当前最先进的模型，充分证明了其在防御XSS对抗攻击方面的强大有效性。"
  },
  {
    "date": "2025-12-10",
    "title": "DAO-ML To Solidity: A Scalable Code Generation Approach for Decentralized Autonomous Organization Development",
    "authors": "Sowelu Avanzo, Marco Ottina, Daniele Pautasso, Irene Domenicale, Alex Norta, Claudio Schifanella",
    "publish": "Proceedings of the 2025 International Conference on Information Technology for Social Good",
    "url": "https://doi.org/10.1145/3748699.3749812",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "DAO-ML 转 Solidity：一种可扩展的去中心化自治组织开发代码生成方法",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-10",
    "title": "Configurable Hardware Acceleration for Hyperdimensional Computing Extension on RISC-V",
    "authors": "Rocco Martino, Marco Angioli, Antonello Rosato, Marcello Barbirotta, Abdallah Cheikh, Mauro Olivieri",
    "publish": "IEEE Transactions on Computers",
    "url": "https://doi.org/10.1109/tc.2025.3642250",
    "source": "IEEE",
    "abstract": "Hyperdimensional Computing (HDC) is a neuro-inspired computational model that represents and manipulates information using high-dimensional distributed representations that are combined and compared using simple and highly parallel vector operations. In this work, we present a highly flexible hardware acceleration unit for HDC learning tasks based on the binary spatter-code model. Integrated into the execution stage of the Klessydra-T03 RISC-V core, the unit accelerates the core arithmetic operations of HDC and can be configured at synthesis time in terms of hardware parallelism, supported operations, and size of the local memories, trading off execution time with hardware resources to match application needs. A custom RISC-V Instruction Set Extension efficiently controls the accelerator, with instructions fully integrated into the GNU Compiler Collection toolchain and exposed to the programmer as intrinsics. Dedicated Control and Status Registers allow specifying the characteristics of the high-dimensional space and the target learning tasks at runtime, controlling the hardware loops of the accelerator and enabling the same hardware architecture to be used for various tasks. The dual flexibility coming from hardware configuration and software programmability sets this work apart from application-specific solutions in the literature, offering a unique, versatile accelerator adaptable to a wide range of applications and learning tasks.",
    "title_zh": "RISC-V上用于超维度计算扩展的可配置硬件加速",
    "abstract_zh": "超维计算（Hyperdimensional Computing, HDC）是一种受神经科学启发的计算模型，通过高维分布式表示来表征和操作信息，并利用简单且高度并行的向量运算实现信息的组合与比较。本文提出了一种基于二进制散射码（binary spatter-code）模型的、高度灵活的HDC学习任务硬件加速单元。该单元集成于Klessydra-T03 RISC-V核心的执行阶段，能够加速HDC的核心算术操作。其硬件并行度、支持的操作类型以及本地存储器大小可在综合时进行配置，从而在执行时间与硬件资源之间实现权衡，以满足不同应用的需求。通过自定义的RISC-V指令集扩展，该加速器可被高效控制，相关指令已完全集成至GNU编译工具链中，并对程序员以内联函数（intrinsics）的形式暴露。专用的控制与状态寄存器允许在运行时动态指定高维空间特征及目标学习任务，控制加速器的硬件循环，使得同一硬件架构可适用于多种不同的任务。这种由硬件配置灵活性与软件可编程性共同带来的双重灵活性，使本工作区别于文献中现有的专用解决方案，提供了一种独特且通用的加速器，能够适应广泛的应用场景与学习任务。"
  },
  {
    "date": "2025-12-10",
    "title": "Training Task Reasoning LLM Agents for Multi-Turn Task Planning via Single-Turn Reinforcement Learning",
    "authors": "Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang",
    "publish": "IEEE Control Systems Letters",
    "url": "https://doi.org/10.1109/lcsys.2025.3642767",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.",
    "title_zh": "通过单轮强化学习训练LLM代理以实现多轮任务规划的推理",
    "abstract_zh": "大型语言模型（LLMs）在知识获取、推理和工具使用方面展现出卓越的能力，使其成为自主智能体应用的有力候选者。然而，在复杂多轮任务规划中训练LLM智能体仍面临诸多挑战，包括稀疏的回合级奖励、长时程中的信用分配难题，以及在多轮交互设置下强化学习带来的计算开销。为此，本文提出一种新方法，将多轮任务规划问题转化为单轮任务推理问题，通过专家轨迹提供的密集且可验证的奖励信号，实现基于组相对策略优化（GRPO）的高效策略优化。理论分析表明，GRPO在单轮任务推理上的改进能够为最少轮次下的多轮任务成功率提供下界，并具备向更短时程子任务泛化的能力。在复杂任务规划基准上的实验评估显示，我们仅使用15亿参数的模型，通过单轮GRPO训练，其性能优于高达140亿参数的基线模型，在长时程规划任务中取得了70%的成功率。"
  },
  {
    "date": "2025-12-10",
    "title": "Efficient Vulnerability Assessment of Multi-Bit Faults in Embedded Healthcare Software",
    "authors": "Jinting Ren, Yu Wu, Hengyi Ren, Xing Li, Zhaoyang Han",
    "publish": "IEEE Journal of Biomedical and Health Informatics",
    "url": "https://doi.org/10.1109/jbhi.2025.3631266",
    "source": "IEEE",
    "abstract": "With the growing use of wearable and implantable medical devices, ensuring the reliability of embedded software is crucial for patient safety. These devices are susceptible to soft errors, and traditional single-bit fault models often fail to account for multi-bit faults, leading to either insufficient protection or excessive overhead. We introduce <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">V-FAME</b>, a lightweight and efficient vulnerability analysis framework for embedded healthcare systems. V-FAME uses machine learning and a multi-bit fault model to identify fault-sensitive code regions quickly and accurately. Unlike existing tools that focus on instruction-level analysis, V-FAME uses a basic block-level classification approach to significantly prune the fault space. Our experiments show V-FAME achieves a speedup of over 6.16x while maintaining an accuracy of over 80%. This framework supports reliable and cost-effective fault mitigation in medical applications.",
    "title_zh": "嵌入式医疗软件中多比特故障的高效漏洞评估",
    "abstract_zh": "随着可穿戴和植入式医疗设备的广泛应用，确保嵌入式软件的可靠性对于患者安全至关重要。这些设备容易受到软错误的影响，而传统的单比特故障模型往往无法充分考虑多比特故障，导致保护不足或开销过大。我们提出了V-FAME，一种轻量且高效的嵌入式医疗系统漏洞分析框架。V-FAME采用机器学习与多比特故障模型，能够快速准确地识别出对故障敏感的代码区域。与现有工具侧重于指令级分析不同，V-FAME采用基本块级别的分类方法，显著缩小了故障分析范围。实验结果表明，V-FAME在保持超过80%准确率的同时，实现了超过6.16倍的加速效果。该框架为医疗应用中的可靠且低成本的故障缓解提供了有力支持。"
  },
  {
    "date": "2025-12-10",
    "title": "Large-Scale Intranet Security Assessment Based on Bayesian Attack Graphs Using System Audit Logs",
    "authors": "Chengliang Gao, Jing Qiu, Jiaxu Xing, Ximing Chen, Du Cheng, Lejun Zhang, Tiejun Wu",
    "publish": "IEEE Transactions on Dependable and Secure Computing",
    "url": "https://doi.org/10.1109/tdsc.2025.3642153",
    "source": "IEEE",
    "abstract": "Large-scale dynamic intranet environments are characterized by constantly changing configurations, evolving user behaviors, and diverse assets that increase vulnerability pathways. These factors undermine the effectiveness of Bayesian attack graphs and reveal the limitations of traditional security methods that rely on static assumptions. To address these challenges, this paper proposes a novel Bayesian attack graph method designed for large-scale, active intranet security assessments. It captures real-time intranet changes by extracting system audit logs and generates attack graphs with MulVAL, ultimately resulting in a time-spanning understanding of potential security risks. Furthermore, it identifies direct-risk paths by eliminating weak dependencies between actions and estimates the likelihood of action execution based on expectations, thereby substantially reducing the computational complexity of Bayesian security analysis. To validate the proposed method, this paper conducts dynamic threat modeling and quantitative security analysis on an enterprise intranet using logs from over 1,000 hosts. The results demonstrate that the proposed method not only provides internal network security risk values at any given time but also identifies specific and observable potential attack paths. Furthermore, this study provides a reference framework for prioritizing vulnerability remediation based on changes in internal network security conditions",
    "title_zh": "基于系统审计日志的贝叶斯攻击图的大规模内网安全评估",
    "abstract_zh": "大规模动态内网环境具有配置不断变化、用户行为持续演进以及资产类型多样等特点，这些因素增加了潜在的攻击路径，削弱了贝叶斯攻击图的有效性，并暴露出传统安全方法在依赖静态假设方面的局限性。为应对这些挑战，本文提出一种新型的贝叶斯攻击图方法，专用于大规模活跃内网的安全评估。该方法通过提取系统审计日志实时捕捉内网变化，利用MulVAL生成攻击图，从而实现对潜在安全风险的时间跨度分析。此外，该方法通过消除动作之间的弱依赖关系，识别出直接风险路径，并基于期望值估算动作执行的可能性，显著降低了贝叶斯安全分析的计算复杂度。为验证所提方法的有效性，本文基于来自1000多台主机的日志，对某企业内网进行了动态威胁建模与定量安全分析。结果表明，该方法不仅能够在任意时刻提供内网安全风险值，还能识别出具体且可观察的潜在攻击路径。同时，本研究还构建了一个参考框架，可根据内网安全状况的变化，为漏洞修复工作提供优先级建议。"
  },
  {
    "date": "2025-12-10",
    "title": "2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI",
    "authors": "Soumyadeep Chandra, Sayeed Shafayet Chowdhury, Kaushik Roy",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2025.3642715",
    "source": "IEEE",
    "abstract": "Thermal analysis is increasingly critical in modern integrated circuits, where non-uniform power dissipation and high transistor densities can cause rapid temperature spikes and reliability concerns. Traditional methods, such as FEM-based simulations, offer high accuracy but are computationally prohibitive for early-stage design, often requiring multiple iterative redesign cycles to resolve late-stage thermal failures. To address these challenges, we propose ‘<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2D-ThermAl</i>’, a physics-informed generative AI framework that effectively identifies heat sources and estimates full-chip transient and steady-state thermal distributions directly from input activity profiles. ThermAl employs a hybrid U-Net architecture enhanced with positional encoding and a Boltzmann regularizer to maintain physical fidelity. Our model is trained on an extensive dataset of heat dissipation maps for over 200 circuit configurations, ranging from simple logic gates (e.g., inverters, NAND, XOR) to complex designs, generated using COMSOL and Cadence EDA flows. The dataset captures diverse activity patterns, and we note that material-dependent thermal properties may require targeted fine-tuning to ensure accuracy across different fabrication contexts. Experimental results demonstrate that 2D-ThermAl provides precise temperature mappings for large circuits, with a root mean squared error (RMSE) of only 0.71°C and outperforms conventional FEM tools by running up to ∼ 200× faster. We analyze performance across diverse layouts and workloads and discuss its applicability to large-scale EDA workflows. Although thermal reliability assessments often extend beyond 85°C for post-layout signoff, our focus here is on early-stage hotspot detection and thermal pattern learning. To ensure generalization beyond the nominal operating range (25−55°C), we additionally performed cross-validation on an extended dataset that spans 25−95°C, maintaining a high accuracy (< 2.2% full-scale RMSE) even under elevated temperature conditions representative of the peak power and stress scenarios. Limitations such as 2D-only modeling and real-world validation are addressed with concrete future directions, including 3D extension, generalization across technology nodes, and transfer learning strategies. The code and dataset are publicly available at: https://github.com/soumyadeepchandra/2D-ThermAl.",
    "title_zh": "2D-ThermAl：基于生成式人工智能的电路热分析物理信息框架",
    "abstract_zh": "热分析在现代集成电路中日益重要，因为非均匀的功耗分布和高密度的晶体管布局可能导致快速的温度飙升以及可靠性问题。传统的有限元法（FEM）仿真虽然精度较高，但在早期设计阶段计算成本过高，往往需要多次迭代修改才能解决后期出现的热失效问题。为应对这些挑战，我们提出了“<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2D-ThermAl</i>”——一种基于物理信息的生成式人工智能框架，能够直接从输入活动模式中有效识别热源，并估算全芯片的瞬态与稳态热分布。ThermAl采用融合位置编码和玻尔兹曼正则化项的混合U-Net架构，以保持物理一致性。该模型在超过200种电路配置的大量热耗散图谱数据集上进行训练，涵盖从简单逻辑门（如反相器、NAND、XOR）到复杂设计的各种结构，数据通过COMSOL和Cadence EDA流程生成。该数据集包含了多样的活动模式，我们注意到材料相关的热学特性可能需针对不同制造工艺进行针对性微调，以确保跨工艺场景下的准确性。实验结果表明，2D-ThermAl可对大规模电路实现精确的温度映射，均方根误差（RMSE）仅为0.71°C，且运行速度比传统FEM工具快约200倍。我们在多种版图结构和工作负载下评估了其性能，并讨论了其在大规模EDA工作流中的适用性。尽管热可靠性评估通常需覆盖高于85°C的范围以用于布局后签核，但本文重点在于早期热点检测与热模式学习。为确保模型在标称工作范围（25–55°C）之外仍具备良好泛化能力，我们进一步在覆盖25–95°C的扩展数据集上进行了交叉验证，在代表峰值功耗与应力场景的高温条件下依然保持高精度（全量程RMSE低于2.2%）。文中也指出了当前方法的局限性，如仅限二维建模及缺乏真实世界验证，并提出了明确的未来方向：包括向三维建模拓展、跨工艺节点的泛化能力提升，以及迁移学习策略的应用。代码与数据集已公开，获取地址为：https://github.com/soumyadeepchandra/2D-ThermAl。"
  }
]