[
  {
    "date": "2026-01-23",
    "title": "Preventing the Collapse of Peer Review Requires Verification-First AI",
    "authors": "Lei You, Lele Cao, Iryna Gurevych",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16909v1",
    "source": "arXiv",
    "abstract": "This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal shrinkage, when real improvements become hard to separate from noise. In a minimal model that mixes occasional high-fidelity checks with frequent proxy judgment, we derive an explicit coupling law and an incentive-collapse condition under which rational effort shifts from truth-seeking to proxy optimization, even when current decisions still appear reliable. These results motivate actions for tool builders and program chairs: deploy AI as an adversarial auditor that generates auditable verification artifacts and expands effective verification bandwidth, rather than as a score predictor that amplifies claim inflation."
  },
  {
    "date": "2026-01-23",
    "title": "Quantum Position Verification with Remote Untrusted Devices",
    "authors": "Gautam A. Kavuri, Yanbao Zhang, Abigail R. Gookin, Soumyadip Patra, Joshua C. Bienfang, Honghao Fu, Yusuf Alnawakhtha, Dileep V. Reddy, Michael D. Mazurek, Carlos Abellán, Waldimar Amaya, Morgan W. Mitchell, Sae Woo Nam, Carl A. Miller, Richard P. Mirin, Martin J. Stevens, Scott Glancy, Emanuel Knill, Lynden K. Shalm",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16892v1",
    "source": "arXiv",
    "abstract": "Many applications require or benefit from being able to securely localize remote parties. In classical physics, adversaries can in principle have complete knowledge of such a party's devices, and secure localization is fundamentally impossible. This limitation can be overcome with quantum technologies, but proposals to date require trusting vulnerable hardware. Here we develop and experimentally demonstrate a protocol for device-independent quantum position verification that guarantees security with only observed correlations from a loophole-free Bell test across a quantum network. The protocol certifies the position of a remote party against adversaries who, before each instance of the test, are weakly entangled, but otherwise have unlimited quantum computation and communication capabilities. Our demonstration achieves a one-dimensional localization that is 2.47(2) times smaller than the best, necessarily non-remote, classical localization protocol. Compared to such a classical protocol having identical latencies, the localization is 4.53(5) times smaller. This work anchors digital security in the physical world."
  },
  {
    "date": "2026-01-23",
    "title": "LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems",
    "authors": "João A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, Carolina Scarton",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16890v1",
    "source": "arXiv",
    "abstract": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the effects of persuasion on both claim verification and evidence retrieval using a decoupled evaluation strategy. Experiments on the FEVER and FEVEROUS benchmarks show that persuasion attacks can substantially degrade both verification performance and evidence retrieval. Our analysis identifies persuasion techniques as a potent class of adversarial attacks, highlighting the need for more robust AFC systems."
  },
  {
    "date": "2026-01-23",
    "title": "Universal classical and quantum fluctuations in the large deviations of current of noisy quantum systems: The case of QSSEP and QSSIP",
    "authors": "Mathias Albert, Denis Bernard, Tony Jin, Stefano Scopa, Shiyi Wei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16883v1",
    "source": "arXiv",
    "abstract": "We study the fluctuation statistics of integrated currents in noisy quantum diffusive systems, focusing on the Quantum Symmetric Simple Exclusion and Inclusion Processes (QSSEP/QSSIP). These one-dimensional fermionic (QSSEP) and bosonic (QSSIP) models feature stochastic nearest-neighbor hopping driven by Brownian noise, together with boundary injection and removal processes. They provide solvable microscopic settings in which quantum coherence coexists with diffusion. Upon noise averaging, their dynamics reduce to those of the classical SSEP/SSIP. We show that the cumulant generating function of the integrated current, at large scales, obeys a large deviation principle. To leading order in system size and for each noise realization, it converges to that of the corresponding classical process, establishing a classical typicality of current fluctuations in these noisy quantum systems. We further demonstrate a direct connection with Macroscopic Fluctuation Theory (MFT), showing that the large-scale equations satisfied by biased quantum densities coincide with the steady-state Hamilton equations of MFT, thereby providing a microscopic quantum justification of the MFT framework in these models. Finally, we identify the leading finite-size corrections to the current statistics. We show the existence of subleading contributions of purely quantum origin, which are absent in the corresponding classical setting, and provide their explicit expressions for the second and third current cumulants. These quantum corrections are amenable to direct experimental or numerical verification, provided sufficient control over the noise realizations can be achieved. Their presence points toward the necessity of a quantum extension of Macroscopic Fluctuation Theory."
  },
  {
    "date": "2026-01-23",
    "title": "CER-HV: A CER-Based Human-in-the-Loop Framework for Cleaning Datasets Applied to Arabic-Script HTR",
    "authors": "Sana Al-azzawi, Elisa Barney, Marcus Liwicki",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16713v1",
    "source": "arXiv",
    "abstract": "Handwritten text recognition (HTR) for Arabic-script languages still lags behind Latin-script HTR, despite recent advances in model architectures, datasets, and benchmarks. We show that data quality is a significant limiting factor in many published datasets and propose CER-HV (CER-based Ranking with Human Verification) as a framework to detect and clean label errors. CER-HV combines a CER-based noise detector, built on a carefully configured Convolutional Recurrent Neural Network (CRNN) with early stopping to avoid overfitting noisy samples, and a human-in-the-loop (HITL) step that verifies high-ranking samples. The framework reveals that several existing datasets contain previously underreported problems, including transcription, segmentation, orientation, and non-text content errors. These have been identified with up to 90 percent precision in the Muharaf and 80-86 percent in the PHTI datasets. We also show that our CRNN achieves state-of-the-art performance across five of the six evaluated datasets, reaching 8.45 percent Character Error Rate (CER) on KHATT (Arabic), 8.26 percent on PHTI (Pashto), 10.66 percent on Ajami, and 10.11 percent on Muharaf (Arabic), all without any data cleaning. We establish a new baseline of 11.3 percent CER on the PHTD (Persian) dataset. Applying CER-HV improves the evaluation CER by 0.3-0.6 percent on the cleaner datasets and 1.0-1.8 percent on the noisier ones. Although our experiments focus on documents written in an Arabic-script language, including Arabic, Persian, Urdu, Ajami, and Pashto, the framework is general and can be applied to other text recognition datasets."
  },
  {
    "date": "2026-01-23",
    "title": "SCHIGAND: A Synthetic Facial Generation Mode Pipeline",
    "authors": "Ananya Kadali, Sunnie Jehan-Morrison, Orasiki Wellington, Barney Evans, Precious Durojaiye, Richard Guest",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16627v1",
    "source": "arXiv",
    "abstract": "The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation."
  },
  {
    "date": "2026-01-23",
    "title": "Challenges in the Proper Metrological Verification of Smart Energy Meters",
    "authors": "Antonio Bracale, Jakub Janowicz, Piotr Kuwałek, Grzegorz Wiczyński",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16612v1",
    "source": "arXiv",
    "abstract": "The most common instruments currently measuring active/reactive energy and power quality indicators are smart energy meters. Unfortunately, the verification of such meters is currently performed under ideal conditions or with simple signal models, which do not recreate actual states occurring in the power grid and do not ensure the verification of the properties of their signal chains. This paper presents challenges in the proper metrological verification of smart energy meters. It presents existing legal and normative requirements and scientific research directions regarding these meters. Selected test results are presented, which show that although the tested meters meet the normative and legal requirements because they have been approved for sale, numerous imperfections in the signal and measurement chains of the analyzed instruments are revealed for the selected test signal. On the basis of the presented research results, further directions of research in the field of smart energy meters have been determined."
  },
  {
    "date": "2026-01-23",
    "title": "Numerical investigation of unsteady flow in a reversible pump-turbine",
    "authors": "Chirag Trivedi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16584v1",
    "source": "arXiv",
    "abstract": "Hydropower is an important source of renewable energy that provides clean energy. Pump-turbine type hydraulic turbine is widely used to mitigate the intermittent energy demand and store a large-scale energy. Pump-turbine operates in reverse mode in pump mode to store energy. Flow conditions in turbine mode and pump mode operations is substantially different. This study investigates the unsteady flow field in the model pump-turbine. A computational model of the pump-turbine was created, and the model included hexahedral mesh of 58.19 million nodes. Total verification and validation error was 7.7%. Three operating conditions in turbine mode and four in pump mode were simulated. Flow characteristics, such as blade loading, time-dependent pressure fluctuations, frequency spectra, radial and tangential velocity were investigated. The frequency spectra revealed amplitude of frequencies up to tenth harmonics of the blade passing frequency in pump mode. The higher harmonic frequencies can potentially reach the high mode eigen frequencies and increase the risk of resonance. Flow field analysis in the draft tube indicted the strong presence of Dean vortices causing highly asymmetric flow at the runner inlet in pump mode operation. This study provides essential insights into the complex flow phenomena and advances the understanding of unsteady flow behaviour in pump-turbines."
  },
  {
    "date": "2026-01-23",
    "title": "Retrieve-Refine-Calibrate: A Framework for Complex Claim Fact-Checking",
    "authors": "Mingwei Sun, Qianlong Wang, Ruifeng Xu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16555v1",
    "source": "arXiv",
    "abstract": "Fact-checking aims to verify the truthfulness of a claim based on the retrieved evidence. Existing methods typically follow a decomposition paradigm, in which a claim is broken down into sub-claims that are individually verified. However, the decomposition paradigm may introduce noise to the verification process due to irrelevant entities or evidence, ultimately degrading verification accuracy. To address this problem, we propose a Retrieve-Refine-Calibrate (RRC) framework based on large language models (LLMs). Specifically, the framework first identifies the entities mentioned in the claim and retrieves evidence relevant to them. Then, it refines the retrieved evidence based on the claim to reduce irrelevant information. Finally, it calibrates the verification process by re-evaluating low-confidence predictions. Experiments on two popular fact-checking datasets (HOVER and FEVEROUS-S) demonstrate that our framework achieves superior performance compared with competitive baselines."
  },
  {
    "date": "2026-01-23",
    "title": "Learning to Optimize by Differentiable Programming",
    "authors": "Liping Tao, Xindi Tong, Chee Wei Tan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16510v1",
    "source": "arXiv",
    "abstract": "Solving massive-scale optimization problems requires scalable first-order methods with low per-iteration cost. This tutorial highlights a shift in optimization: using differentiable programming not only to execute algorithms but to learn how to design them. Modern frameworks such as PyTorch, TensorFlow, and JAX enable this paradigm through efficient automatic differentiation. Embedding first-order methods within these systems allows end-to-end training that improves convergence and solution quality. Guided by Fenchel-Rockafellar duality, the tutorial demonstrates how duality-informed iterative schemes such as ADMM and PDHG can be learned and adapted. Case studies across LP, OPF, Laplacian regularization, and neural network verification illustrate these gains."
  },
  {
    "date": "2026-01-22",
    "title": "TidyVoice: A Curated Multilingual Dataset for Speaker Verification Derived from Common Voice",
    "authors": "Aref Farhadipour, Jan Marquenie, Srikanth Madikeri, Eleanor Chodroff",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16358v1",
    "source": "arXiv",
    "abstract": "The development of robust, multilingual speaker recognition systems is hindered by a lack of large-scale, publicly available and multilingual datasets, particularly for the read-speech style crucial for applications like anti-spoofing. To address this gap, we introduce the TidyVoice dataset derived from the Mozilla Common Voice corpus after mitigating its inherent speaker heterogeneity within the provided client IDs. TidyVoice currently contains training and test data from over 212,000 monolingual speakers (Tidy-M) and around 4,500 multilingual speakers (Tidy-X) from which we derive two distinct conditions. The Tidy-M condition contains target and non-target trials from monolingual speakers across 81 languages. The Tidy-X condition contains target and non-target trials from multilingual speakers in both same- and cross-language trials. We employ two architectures of ResNet models, achieving a 0.35% EER by fine-tuning on our comprehensive Tidy-M partition. Moreover, we show that this fine-tuning enhances the model's generalization, improving performance on unseen conversational interview data from the CANDOR corpus. The complete dataset, evaluation trials, and our models are publicly released to provide a new resource for the community."
  },
  {
    "date": "2026-01-22",
    "title": "Magnon equilibrium spin current in collinear antiferromagnets",
    "authors": "Vladimir A. Zyuzin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16184v1",
    "source": "arXiv",
    "abstract": "We theoretically predict that Dzyaloshinskii-Moriya interaction can induce magnon equilibrium spin current in collinear antiferromagnets. Such a current, being a response to the effective magnon vector potential, can be considered as magnon analog of the superconducting supercurrent or the persistent current. Large amplitude of the predicted effect may compensate for the smallness of the Dzyaloshinskii-Moriya interaction, making the equilibrium spin currents to be experimentally observed. We suggest that external electric field can play the role of effective flux magnons interact with and propose an experiment based on the interference of magnons in the ring geometry as a verification of the concept."
  },
  {
    "date": "2026-01-22",
    "title": "Practical applications of Set Shaping Theory to Non-Uniform Sequences",
    "authors": "A. Schmidt, A. Vdberg, A. Petit",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15853v1",
    "source": "arXiv",
    "abstract": "Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work"
  },
  {
    "date": "2026-01-22",
    "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "authors": "Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang, Haitao Mi, Dong Yu, Michael R. Lyu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15808v1",
    "source": "arXiv",
    "abstract": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities."
  },
  {
    "date": "2026-01-22",
    "title": "U3-xi: Pushing the Boundaries of Speaker Recognition via Incorporating Uncertainty",
    "authors": "Junjie Li, Kong Aik Lee",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15719v1",
    "source": "arXiv",
    "abstract": "An utterance-level speaker embedding is typically obtained by aggregating a sequence of frame-level representations. However, in real-world scenarios, individual frames encode not only speaker-relevant information but also various nuisance factors. As a result, different frames contribute unequally to the final utterance-level speaker representation for Automatic Speaker Verification systems. To address this issue, we propose to estimate the inherent uncertainty of each frame and assign adaptive weights accordingly, where frames with higher uncertainty receive lower attention. Based on this idea, we present U3-xi, a comprehensive framework designed to produce more reliable and interpretable uncertainty estimates for speaker embeddings. Specifically, we introduce several strategies for uncertainty supervision. First, we propose speaker-level uncertainty supervision via a Stochastic Variance Loss, where the distance between an utterance embedding and its corresponding speaker centroid serves as a pseudo ground truth for uncertainty learning. Second, we incorporate global-level uncertainty supervision by injecting the predicted uncertainty into the sof tmax scale during training. This adaptive scaling mechanism adjusts the sharpness of the decision boundary according to sample difficulty, providing global guidance. Third, we redesign the uncertainty estimation module by integrating a Transformer encoder with multi-view self-attention, enabling the model to capture rich local and long-range temporal dependencies. Comprehensive experiments demonstrate that U3-xi is model-agnostic and can be seamlessly applied to various speaker encoders. In particular, when applied to ECAPA-TDNN, it achieves 21.1% and 15.57% relative improvements on the VoxCeleb1 test sets in terms of EER and minDCF, respectively."
  },
  {
    "date": "2026-01-22",
    "title": "zkFinGPT: Zero-Knowledge Proofs for Financial Generative Pre-trained Transformers",
    "authors": "Xiao-Yang Liu, Ningjie Li, Keyi Wang, Xiaoli Zhi, Weiqin Tong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15716v1",
    "source": "arXiv",
    "abstract": "Financial Generative Pre-trained Transformers (FinGPT) with multimodal capabilities are now being increasingly adopted in various financial applications. However, due to the intellectual property of model weights and the copyright of training corpus and benchmarking questions, verifying the legitimacy of GPT's model weights and the credibility of model outputs is a pressing challenge. In this paper, we introduce a novel zkFinGPT scheme that applies zero-knowledge proofs (ZKPs) to high-value financial use cases, enabling verification while protecting data privacy. We describe how zkFinGPT will be applied to three financial use cases. Our experiments on two existing packages reveal that zkFinGPT introduces substantial computational overhead that hinders its real-world adoption. E.g., for LLama3-8B model, it generates a commitment file of $7.97$MB using $531$ seconds, and takes $620$ seconds to prove and $2.36$ seconds to verify."
  },
  {
    "date": "2026-01-22",
    "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation",
    "authors": "Khusrav Badalov, Young Yoon",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15687v1",
    "source": "arXiv",
    "abstract": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations."
  },
  {
    "date": "2026-01-22",
    "title": "Tensor-based phase difference estimation on time series analysis",
    "authors": "Shu Kanno, Kenji Sugisaki, Rei Sakuma, Jumpei Kato, Hajime Nakamura, Naoki Yamamoto",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15616v1",
    "source": "arXiv",
    "abstract": "We propose a phase-difference estimation algorithm based on the tensor-network circuit compression, leveraging time-evolution data to pursue scalability and higher accuracy on a quantum phase estimation (QPE)-type algorithm. Using tensor networks, we construct circuits composed solely of nearest-neighbor gates and extract time-evolution data by four-type circuit measurements. In addition, to enhance the accuracy of time-evolution and state-preparation circuits, we propose techniques based on algorithmic error mitigation and on iterative circuit optimization combined with merging into matrix product states, respectively. Verifications using a noiseless simulator for the 8-qubit one-dimensional Hubbard model using an ancilla qubit show that the proposed algorithm achieves accuracies with 0.4--4.7\\% error from a true energy gap on an appropriate time-step size, and that accuracy improvements due to the algorithmic error mitigation are observed. We also confirm the enhancement of the overlap with matrix product states through iterative optimization. Finally, the proposed algorithm is demonstrated on IBM Heron devices with Q-CTRL error suppression for 8-, 36-, and 52-qubit models using more than 5,000 2-qubit gates. These largest-scale demonstrations for the QPE-type algorithm represent significant progress not only toward practical applications of near-term quantum computing but also toward preparation for the era of error-corrected quantum devices."
  },
  {
    "date": "2026-01-22",
    "title": "Swelling-Induced Stress-Assisted Transfer of Nanodiamond Arrays with a PVA Carrier Tape for Conformal Bio-Integrated Sensing and Labelling",
    "authors": "Luyao Zhang, Lingzhi Wang, Xinhao Hu, Yip Tai Nam, Mingzhe Sun, Jixiang Jing, Lizhi Xu, Yuan Lin, Yong Hou, Zhiqin Chu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.15587v1",
    "source": "arXiv",
    "abstract": "The conformal integration of nitrogen-vacancy (NV) center nanodiamond arrays onto soft, hydrated, curvilinear biological interfaces remain a fundamental challenge for in vivo quantum sensing and imaging. Conventional transfer techniques often fail due to reliance on high temperature, corrosive chemicals, or mechanical peeling, leading to pattern damage, low fidelity, or poor biocompatibility. Here, we report a transfer strategy utilizing polyvinyl alcohol (PVA) carrier soluble tape, enabling rapid, residue-free, high-fidelity transfer of nanodiamond patterns onto diverse biointerfaces. The success of this method is rooted in a unique \"hydrate-soften-expand-self-peel\" mechanism of the soluble tape with PVA backing. In situ mechanical tracking reveals non-uniform PVA swelling upon hydration generates transient local normal and shear stresses at the interface. These stresses delaminate the tape within 3 minutes at room temperature while promoting adhesion of the nanodiamond array to the substrate. In contrast, conventional water-soluble tapes with composite structures undergo passive dissolution and collapse, causing residue contamination and reduced efficiency. Leveraging this mechanism, we achieve conformal patterning on ultra-soft hydrogels (~0.6 kPa) and highly curved bio-surfaces (hair, 100 μm^-1). Additionally, we demonstrate a dual-identity verification system integrating data storage and physical unclonable functions on a hydrogel contact lens. This work provides a versatile tool for bio-interface engineering and a general framework for gentle, efficient transfer of functional nanomaterials."
  },
  {
    "date": "2026-01-21",
    "title": "Combining Tests and Proofs for Better Software Verification",
    "authors": "Li Huang, Bertrand Meyer, Manuel Oriol",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.16239v1",
    "source": "arXiv",
    "abstract": "Test or prove? These two approaches to software verification have long been presented as opposites. One is dynamic, the other static: a test executes the program, a proof only analyzes the program text. A different perspective is emerging, in which testing and proving are complementary rather than competing techniques for producing software of verified quality. Work performed over the past few years and reviewed here develops this complementarity by taking advantage of Design by Contract, as available in Eiffel, and exploiting a feature of modern program-proving tools based on ``Satisfiability Modulo Theories'' (SMT): counterexample generation. A counterexample is an input combination that makes the program fail. If we are trying to prove a program correct, we hope not to find any. One can, however, apply counterexample generation to incorrect programs, as a tool for automatic test generation. We can also introduce faults into a correct program and turn the counterexamples into an automatically generated regression test suite with full coverage. Additionally, we can use these mechanisms to help produce program fixes for incorrect programs, with a guarantee that the fixes are correct. All three applications, leveraging on the mechanisms of Eiffel and Design by Contract, hold significant promise to address some of the challenges of program testing, software maintenance and Automatic Program Repair."
  }
]