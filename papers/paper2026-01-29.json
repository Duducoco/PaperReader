[
  {
    "date": "2026-01-29",
    "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
    "authors": "Grzegorz Stefanski, Alberto Presta, Michal Byra",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22141v1",
    "source": "arXiv",
    "abstract": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning."
  },
  {
    "date": "2026-01-29",
    "title": "PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters",
    "authors": "Jian Gao, Yiwei Zou, Abhishek Pradhan, Wenhao Huang, Yumin Su, Kaiyuan Yang, Xuan Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21984v1",
    "source": "arXiv",
    "abstract": "Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at https://github.com/xz-group/PowerGenie."
  },
  {
    "date": "2026-01-29",
    "title": "VERSA: Verified Event Data Format for Reliable Soccer Analytics",
    "authors": "Geonhee Jo, Mingu Kang, Kangmin Lee, Minho Lee, Pascal Bauer, Sang-Ki Ko",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21981v1",
    "source": "arXiv",
    "abstract": "Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis."
  },
  {
    "date": "2026-01-29",
    "title": "Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding",
    "authors": "Yifan Zhu, Huiqiang Rong, Haoran Luo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21969v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available."
  },
  {
    "date": "2026-01-29",
    "title": "TidyVoice 2026 Challenge Evaluation Plan",
    "authors": "Aref Farhadipour, Jan Marquenie, Srikanth Madikeri, Teodora Vukovic, Volker Dellwo, Kathy Reid, Francis M. Tyers, Ingo Siegert, Eleanor Chodroff",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21960v1",
    "source": "arXiv",
    "abstract": "The performance of speaker verification systems degrades significantly under language mismatch, a critical challenge exacerbated by the field's reliance on English-centric data. To address this, we propose the TidyVoice Challenge for cross-lingual speaker verification. The challenge leverages the TidyVoiceX dataset from the novel TidyVoice benchmark, a large-scale, multilingual corpus derived from Mozilla Common Voice, and specifically curated to isolate the effect of language switching across approximately 40 languages. Participants will be tasked with building systems robust to this mismatch, with performance primarily evaluated using the Equal Error Rate on cross-language trials. By providing standardized data, open-source baselines, and a rigorous evaluation protocol, this challenge aims to drive research towards fairer, more inclusive, and language-independent speaker recognition technologies, directly aligning with the Interspeech 2026 theme, \"Speaking Together.\""
  },
  {
    "date": "2026-01-29",
    "title": "Late-time X-ray afterglows of GRBs: Implications for particle acceleration at relativistic shocks",
    "authors": "Zhi-Qiu Huang, Om Sharan Salafia, Lara Nava, Annalisa Celotti, Giancarlo Ghirlanda",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21827v1",
    "source": "arXiv",
    "abstract": "Particle-in-cell (PIC) numerical simulations are currently among the most advanced tools to investigate particle acceleration at relativistic shocks. Still, they come with limitations imposed by finite computing power, whose impact is not straightforward to evaluate a priori. Observational features are hence required as verification. energy electrons accelerated at external shocks, provides a testbed for such predictions. Current numerical studies suggest that in GRB afterglows the maximum synchrotron photon energy, which corresponds to the limit of electron acceleration, may fall within the $\\sim$ 0.1--10 keV X-ray energy band at late times, $t\\gtrsim 10^6 - 10^7$ s. To test this prediction, we analyzed the X-ray spectra of six GRBs with \\emph{Swift}/XRT detections beyond $10^7$ s: our analysis reveals no clear evidence of a spectral cutoff. Using a model that accounts for the effect of the finite opening angle of the shock on the observed maximum synchrotron photon energy, we show that these observations are incompatible with PIC simulation predictions, unless one or more physical afterglow parameters attain values at odds with those typically inferred from afterglow modeling (small radiative efficiency, low ambient density, large equipartition fraction $ε_{\\rm B}$ of the magnetic field). These findings challenge existing numerical simulation results and imply a more efficient acceleration of electrons to high-energies than seen in PIC simulations, with important implications for our understanding of particle acceleration in relativistic shocks."
  },
  {
    "date": "2026-01-29",
    "title": "On Diagonalizable Systems with Random Structure",
    "authors": "Yuan Zhang, Yutong Han, Yuanqing Xia, Aming Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21710v1",
    "source": "arXiv",
    "abstract": "Diagonalizability plays an important role in the analysis and design of multivariable systems. A structured matrix is called structurally diagonalizable if almost all of its numerical realizations, obtained by assigning real values to its free entries, are diagonalizable. Structural diagonalizability is useful for the verification and optimization of various structural system properties. In this paper, we study the asymptotic probability distribution of structural diagonalizability for structured systems whose system matrices are represented by directed Erdős-Rényi random graphs. Leveraging a recently established graph-theoretic characterization of structural diagonalizability, we analyze the distribution of structurally diagonalizable graphs under different edge-density regimes. For dense graphs, we prove that the system is almost always structurally diagonalizable. For graphs of medium density, we derive tight upper and lower bounds on the asymptotic probability of structural diagonalizability. For extremely sparse graphs, we show that this probability approaches 0. The theoretical results are validated through extensive numerical simulations with varying numbers of vertices and connection probabilities."
  },
  {
    "date": "2026-01-29",
    "title": "StarSD: One-for-Many Speculative Decoding",
    "authors": "Junhao He, Feiran You, Hongyang Du",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21622v1",
    "source": "arXiv",
    "abstract": "Speculative decoding accelerates autoregressive generation by separating token proposal from verification, but most existing approaches are designed for single-node execution and do not scale well to multi-accelerator clusters used for serving modern Large Language Models (LLMs). We present StarSD, a one-for-many speculative decoding framework that uses a single draft model to serve multiple target models across distributed nodes via a star topology. StarSD decouples drafting and verification, enabling effective sharing of draft computation, and preventing distributed accelerators from remaining idle under bursty workloads. We provide a system-level analysis that characterizes when and why a single draft model can remain fully utilized by multiple verifiers, yielding predictable latency and utilization gains. Extensive experiments in real-world distributed inference settings demonstrate that StarSD simplifies deployment and supports flexible resource allocation across heterogeneous accelerators, while maintaining output quality. These results indicate that StarSD is a practical and scalable framework for bringing speculative decoding to modern cloud and edge inference infrastructures."
  },
  {
    "date": "2026-01-29",
    "title": "DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis",
    "authors": "Ruyi Qi, Zhou Liu, Wentao Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21403v1",
    "source": "arXiv",
    "abstract": "In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and \"zombie data\" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the \"divide-and-conquer\" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful, cross-modal analysis."
  },
  {
    "date": "2026-01-29",
    "title": "Bivariate Postprocessing of Wind Vectors",
    "authors": "Ferdinand Buchner, David Jobst, Annette Möller, Claudia Czado",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21401v1",
    "source": "arXiv",
    "abstract": "To quantify the uncertainty in numerical weather prediction (NWP) forecasts, ensemble prediction systems are utilized. Although NWP forecasts continuously improve, they suffer from systematic bias and dispersion errors. To obtain well calibrated and sharp predictive probability distributions, statistical postprocessing methods are applied to NWP output. Recent developments focus on multivariate postprocessing models incorporating dependencies directly into the model. We introduce three novel bivariate postprocessing approaches, and analyze their performance for joint postprocessing of bivariate wind vector components for 60 stations in Germany. Bivariate vine copula based models, a bivariate gradient boosted version of ensemble model output statistics (EMOS), and a bivariate distributional regression network (DRN) are compared to bivariate EMOS. The case study indicates that the novel bivariate methods improve over the bivariate EMOS approaches. The bivariate DRN and the most flexible version of the bivariate vine copula approach exhibit the best performance in terms of verification scores and calibration."
  },
  {
    "date": "2026-01-29",
    "title": "User-Centric Evidence Ranking for Attribution and Fact Verification",
    "authors": "Guy Alt, Eran Hirsch, Serwar Basch, Ido Dagan, Oren Glickman",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21387v1",
    "source": "arXiv",
    "abstract": "Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, they often present users with either insufficient or overly redundant information, leading to inefficient and error-prone verification. To address this, we propose Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We compare two approaches for the new ranking task: one-shot ranking and incremental ranking. We introduce a new evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. Compared to evidence selection, we conduct a controlled user study and demonstrate that evidence ranking both reduces reading effort and improves verification. This work provides a foundational step toward more interpretable, efficient, and user-aligned information verification systems."
  },
  {
    "date": "2026-01-29",
    "title": "SecIC3: Customizing IC3 for Hardware Security Verification",
    "authors": "Qinhan Tan, Akash Gaonkar, Yu-Wei Fan, Aarti Gupta, Sharad Malik",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21353v1",
    "source": "arXiv",
    "abstract": "Recent years have seen significant advances in using formal verification to check hardware security properties. Of particular practical interest are checking confidentiality and integrity of secrets, by checking that there is no information flow between the secrets and observable outputs. A standard method for checking information flow is to translate the corresponding non-interference hyperproperty into a safety property on a self-composition of the design, which has two copies of the design composed together. Although prior efforts have aimed to reduce the size of the self-composed design, there are no state-of-the-art model checkers that exploit their special structure for hardware security verification. In this paper, we propose SecIC3, a hardware model checking algorithm based on IC3 that is customized to exploit this self-composition structure. SecIC3 utilizes this structure in two complementary techniques: symmetric state exploration and adding equivalence predicates. We implement SecIC3 on top of two open-source IC3 implementations and evaluate it on a non-interference checking benchmark consisting of 10 designs. The experiment results show that SecIC3 significantly reduces the time for finding security proofs, with up to 49.3x proof speedup compared to baseline implementations."
  },
  {
    "date": "2026-01-29",
    "title": "White-Box Op-Amp Design via Human-Mimicking Reasoning",
    "authors": "Zihao Chen, Jiayin Wang, Ziyi Sun, Ji Zhuang, Jinyi Shen, Xiaoyue Ke, Li Shang, Xuan Zeng, Fan Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21321v1",
    "source": "arXiv",
    "abstract": "This brief proposes \\emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \\emph{\\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \\emph{\\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \\textcolor{blue}{https://github.com/zhchenfdu/whiteop}."
  },
  {
    "date": "2026-01-29",
    "title": "Lossless Copyright Protection via Intrinsic Model Fingerprinting",
    "authors": "Lingxiao Chen, Liqin Wang, Wei Lu, Xiangyang Luo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21252v1",
    "source": "arXiv",
    "abstract": "The exceptional performance of diffusion models establishes them as high-value intellectual property but exposes them to unauthorized replication. Existing protection methods either modify the model to embed watermarks, which impairs performance, or extract model fingerprints by manipulating the denoising process, rendering them incompatible with black-box APIs. In this paper, we propose TrajPrint, a completely lossless and training-free framework that verifies model copyright by extracting unique manifold fingerprints formed during deterministic generation. Specifically, we first utilize a watermarked image as an anchor and exactly trace the path back to its trajectory origin, effectively locking the model fingerprint mapped by this path. Subsequently, we implement a joint optimization strategy that employs dual-end anchoring to synthesize a specific fingerprint noise, which strictly adheres to the target manifold for robust watermark recovery. As input, it enables the protected target model to recover the watermarked image, while failing on non-target models. Finally, we achieved verification via atomic inference and statistical hypothesis testing. Extensive experiments demonstrate that TrajPrint achieves lossless verification in black-box API scenarios with superior robustness against model modifications."
  },
  {
    "date": "2026-01-29",
    "title": "Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox",
    "authors": "Enzo Nicolás Spotorno, Antônio Augusto Medeiros Fröhlich",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21249v1",
    "source": "arXiv",
    "abstract": "The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term \"HYDRA\" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle."
  },
  {
    "date": "2026-01-29",
    "title": "Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification",
    "authors": "Paul He, Yinya Huang, Mrinmaya Sachan, Zhijing Jin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21210v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning."
  },
  {
    "date": "2026-01-29",
    "title": "Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks",
    "authors": "Arther Tian, Alex Ding, Frank Chen, Simon Wu, Aaron Chan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21189v1",
    "source": "arXiv",
    "abstract": "Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings. This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints."
  },
  {
    "date": "2026-01-28",
    "title": "A Tolerance-Based Framework for Spatio-Temporal Forecast Validation Using the gamma-Index",
    "authors": "Cyril Voyant",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21004v1",
    "source": "arXiv",
    "abstract": "Classical field forecast evaluation relies mainly on local scores such as RMSE or MAE. These metrics severely over-penalize small spatial or temporal displacements of coherent structures, a limitation known as the double-penalty issue and common to many forecasting domains. The present paper introduces a tolerance-based framework built on the three-dimensional gamma index, initially designed for medical dose verification, as a unified acceptance criterion for gridded forecasts. The method embeds explicit margins in space (DTA), time (TTA), and intensity (IDT), and evaluates whether predictions agree with observations within predefined physical bounds rather than through pixel-wise differences only. A synthetic illustration is first used to show why conventional metrics can misrepresent usable forecasts. The approach is then applied to satellite-derived SSI fields to demonstrate operational behaviour on a real dataset. Results confirm that the gamma criterion preserves structural consistency under minor positional noise while isolating physically significant discrepancies. The formulation is generic and can be implemented for any gridded variable provided meaningful tolerances are defined, offering a pragmatic complement to existing spatial verification tools in general forecasting workflows."
  },
  {
    "date": "2026-01-28",
    "title": "Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles",
    "authors": "Lutz Oettershagen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.20989v1",
    "source": "arXiv",
    "abstract": "Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\\varepsilon_{\\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\\varepsilon_{\\max}$, where $m(\\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $Ω(m(\\varepsilon_{\\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\\varepsilon_{\\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs."
  },
  {
    "date": "2026-01-28",
    "title": "Infusion of Blockchain to Establish Trustworthiness in AI Supported Software Evolution: A Systematic Literature Review",
    "authors": "Mohammad Naserameri, Juergen Rilling",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.20918v1",
    "source": "arXiv",
    "abstract": "Context: Blockchain and AI are increasingly explored to enhance trustworthiness in software engineering (SE), particularly in supporting software evolution tasks. Method: We conducted a systematic literature review (SLR) using a predefined protocol with clear eligibility criteria to ensure transparency, reproducibility, and minimized bias, synthesizing research on blockchain-enabled trust in AI-driven SE tools and processes. Results: Most studies focus on integrating AI in SE, with only 31% explicitly addressing trustworthiness. Our review highlights six recent studies exploring blockchain-based approaches to reinforce reliability, transparency, and accountability in AI-assisted SE tasks. Conclusion: Blockchain enhances trust by ensuring data immutability, model transparency, and lifecycle accountability, including federated learning with blockchain consensus and private data verification. However, inconsistent definitions of trust and limited real-world testing remain major challenges. Future work must develop measurable, reproducible trust frameworks to enable reliable, secure, and compliant AI-driven SE ecosystems, including applications involving large language models."
  },
  {
    "date": "2026-1-29",
    "title": "LLM-powered Document Analysis and Application in Natural Resources",
    "authors": "Fanrong Meng, Xiaowei Liu, Fan Yang, Yike Guo, Fei Xiao",
    "publish": "Proceedings of the 2025 7th International Conference on Big-data Service and Intelligent Computation",
    "url": "https://doi.org/10.1145/3778265.3778275",
    "source": "ACM",
    "abstract": "None"
  }
]