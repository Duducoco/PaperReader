[
  {
    "date": "2026-01-06",
    "title": "SLIM: Stealthy Low-Coverage Black-Box Watermarking via Latent-Space Confusion Zones",
    "authors": "Hengyu Wu, Yang Cao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03242v1",
    "source": "arXiv",
    "abstract": "Training data is a critical and often proprietary asset in Large Language Model (LLM) development, motivating the use of data watermarking to embed model-transferable signals for usage verification. We identify low coverage as a vital yet largely overlooked requirement for practicality, as individual data owners typically contribute only a minute fraction of massive training corpora. Prior methods fail to maintain stealthiness, verification feasibility, or robustness when only one or a few sequences can be modified. To address these limitations, we introduce SLIM, a framework enabling per-user data provenance verification under strict black-box access. SLIM leverages intrinsic LLM properties to induce a Latent-Space Confusion Zone by training the model to map semantically similar prefixes to divergent continuations. This manifests as localized generation instability, which can be reliably detected via hypothesis testing. Experiments demonstrate that SLIM achieves ultra-low coverage capability, strong black-box verification performance, and great scalability while preserving both stealthiness and model utility, offering a robust solution for protecting training data in modern LLM pipelines."
  },
  {
    "date": "2026-01-06",
    "title": "Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models",
    "authors": "Kartik Bose, Abhinandan Kumar, Raghuraman Soundararajan, Priya Mudgil, Samonee Ralmilay, Niharika Dutta, Manphool Singhal, Arun Kumar, Saugata Sen, Anurima Patra, Priya Ghosh, Abanti Das, Amit Gupta, Ashish Verma, Dipin Sudhakaran, Ekta Dhamija, Himangi Unde, Ishan Kumar, Krithika Rangarajan, Prerna Garg, Rachel Sequeira, Sudhin Shylendran, Taruna Yadav, Tej Pal, Pankaj Gupta",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03232v1",
    "source": "arXiv",
    "abstract": "Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between <1B and >=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes."
  },
  {
    "date": "2026-01-06",
    "title": "UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward",
    "authors": "Yile Liu, Yixian Liu, Zongwei Li, Yufei Huang, Xinhua Feng, Zhichao Hu, Jinglu Hu, Jianfeng Yan, Fengzong Lian, Yuhong Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03205v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima."
  },
  {
    "date": "2026-01-06",
    "title": "Recursive querying of neural networks via weighted structures",
    "authors": "Martin Grohe, Christoph Standke, Juno Steegmans, Jan Van den Bussche",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03201v1",
    "source": "arXiv",
    "abstract": "Expressive querying of machine learning models - viewed as a form of intentional data - enables their verification and interpretation using declarative languages, thereby making learned representations of data more accessible. Motivated by the querying of feedforward neural networks, we investigate logics for weighted structures. In the absence of a bound on neural network depth, such logics must incorporate recursion; thereto we revisit the functional fixpoint mechanism proposed by Grädel and Gurevich. We adopt it in a Datalog-like syntax; we extend normal forms for fixpoint logics to weighted structures; and show an equivalent \"loose\" fixpoint mechanism that allows values of inductively defined weight functions to be overwritten. We propose a \"scalar\" restriction of functional fixpoint logic, of polynomial-time data complexity, and show it can express all PTIME model-agnostic queries over reduced networks with polynomially bounded weights. In contrast, we show that very simple model-agnostic queries are already NP-complete. Finally, we consider transformations of weighted structures by iterated transductions."
  },
  {
    "date": "2026-01-06",
    "title": "Eco-WakeLoc: An Energy-Neutral and Cooperative UWB Real-Time Locating System",
    "authors": "Silvano Cortesi, Lukas Schulthess, Davide Plozza, Christian Vogt, Michele Magno",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03171v1",
    "source": "arXiv",
    "abstract": "Indoor localization systems face a fundamental trade-off between efficiency and responsiveness, which is especially important for emerging use cases such as mobile robots operating in GPS-denied environments. Traditional RTLS either require continuously powered infrastructure, limiting their scalability, or are limited by their responsiveness. This work presents Eco-WakeLoc, designed to achieve centimeter-level UWB localization while remaining energy-neutral by combining ultra-low power wake-up radios (WuRs) with solar energy harvesting. By activating anchor nodes only on demand, the proposed system eliminates constant energy consumption while achieving centimeter-level positioning accuracy. To reduce coordination overhead and improve scalability, Eco-WakeLoc employs cooperative localization where active tags initiate ranging exchanges (trilateration), while passive tags opportunistically reuse these messages for TDOA positioning. An additive-increase/multiplicative-decrease (AIMD)-based energy-aware scheduler adapts localization rates according to the harvested energy, thereby maximizing the overall performance of the sensor network while ensuring long-term energy neutrality. The measured energy consumption is only 3.22mJ per localization for active tags, 951uJ for passive tags, and 353uJ for anchors. Real-world deployment on a quadruped robot with nine anchors confirms the practical feasibility, achieving an average accuracy of 43cm in dynamic indoor environments. Year-long simulations show that tags achieve an average of 2031 localizations per day, retaining over 7% battery capacity after one year -- demonstrating that the RTLS achieves sustained energy-neutral operation. Eco-WakeLoc demonstrates that high-accuracy indoor localization can be achieved at scale without continuous infrastructure operation, combining energy neutrality, cooperative positioning, and adaptive scheduling."
  },
  {
    "date": "2026-01-06",
    "title": "Self-Verification is All You Need To Pass The Japanese Bar Examination",
    "authors": "Andrew Shin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03144v1",
    "source": "arXiv",
    "abstract": "Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available."
  },
  {
    "date": "2026-01-06",
    "title": "The Anatomy of Conversational Scams: A Topic-Based Red Teaming Analysis of Multi-Turn Interactions in LLMs",
    "authors": "Xiangzhe Yuan, Zhenhao Zhang, Haoming Tang, Siying Hu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03134v1",
    "source": "arXiv",
    "abstract": "As LLMs gain persuasive agentic capabilities through extended dialogues, they introduce novel risks in multi-turn conversational scams that single-turn safety evaluations fail to capture. We systematically study these risks using a controlled LLM-to-LLM simulation framework across multi-turn scam scenarios. Evaluating eight state-of-the-art models in English and Chinese, we analyze dialogue outcomes and qualitatively annotate attacker strategies, defensive responses, and failure modes. Results reveal that scam interactions follow recurrent escalation patterns, while defenses employ verification and delay mechanisms. Furthermore, interactional failures frequently stem from safety guardrail activation and role instability. Our findings highlight multi-turn interactional safety as a critical, distinct dimension of LLM behavior."
  },
  {
    "date": "2026-01-06",
    "title": "ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning",
    "authors": "Tuc Nguyen, Thai Le",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.03093v1",
    "source": "arXiv",
    "abstract": "Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task-specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available."
  },
  {
    "date": "2026-01-06",
    "title": "Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning",
    "authors": "Nathanaël Carraz Rakotonirina, Ren Pang, Neha Anna John, Michael Bohlke-Schneider, Momchil Hardalov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02972v1",
    "source": "arXiv",
    "abstract": "The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\\% for 8B models and 40\\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\\text{AUC}_{\\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach."
  },
  {
    "date": "2026-01-06",
    "title": "Quality Degradation Attack in Synthetic Data",
    "authors": "Qinyi Liu, Dong Liu, Farhad Vadiee, Mohammad Khalil, Pedro P. Vergara Barrios",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02947v1",
    "source": "arXiv",
    "abstract": "Synthetic Data Generation (SDG) can be used to facilitate privacy-preserving data sharing. However, most existing research focuses on privacy attacks where the adversary is the recipient of the released synthetic data and attempts to infer sensitive information from it. This study investigates quality degradation attacks initiated by adversaries who possess access to the real dataset or control over the generation process, such as the data owner, the synthetic data provider, or potential intruders. We formalize a corresponding threat model and empirically evaluate the effectiveness of targeted manipulations of real data (e.g., label flipping and feature-importance-based interventions) on the quality of generated synthetic data. The results show that even small perturbations can substantially reduce downstream predictive performance and increase statistical divergence, exposing vulnerabilities within SDG pipelines. This study highlights the need to integrate integrity verification and robustness mechanisms, alongside privacy protection, to ensure the reliability and trustworthiness of synthetic data sharing frameworks."
  },
  {
    "date": "2026-01-06",
    "title": "Intersection patterns of set systems on manifolds with slowly growing homological shatter functions",
    "authors": "Sergey Avvakumov, Marguerite Bin, Xavier Goaoc",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02920v1",
    "source": "arXiv",
    "abstract": "A theorem of Matoušek asserts that for any $k \\ge 2$, any set system whose shatter function is $o(n^k)$ enjoys a fractional Helly theorem: in the $k$-wise intersection hypergraph, positive density implies a linear-size clique. Kalai and Meshulam conjectured a generalization of that phenomenon to homological shatter functions. It was verified for set systems with bounded homological shatter functions and ground set with a forbidden homological minor (which includes $\\mathbb{R}^d$ by a homological analogue of the van Kampen-Flores theorem). We present two contributions to this line of research: - We study homological minors in certain manifolds (possibly with boundary), for which we prove analogues of the van Kampen-Flores theorem and of the Hanani-Tutte theorem. - We introduce graded analogues of the Radon and Helly numbers of set systems and relate their growth rate to the original parameters. This allows to extend the verification of the Kalai-Meshulam conjecture for sufficiently slowly growing homological shatter functions."
  },
  {
    "date": "2026-01-06",
    "title": "Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis",
    "authors": "Mengze Hong, Di Jiang, Zeying Xie, Weiwei Zhao, Guan Wang, Chen Jason Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02914v1",
    "source": "arXiv",
    "abstract": "As audio deepfakes transition from research artifacts to widely available commercial tools, robust biometric authentication faces pressing security threats in high-stakes industries. This paper presents a systematic empirical evaluation of state-of-the-art speaker authentication systems based on a large-scale speech synthesis dataset, revealing two major security vulnerabilities: 1) modern voice cloning models trained on very small samples can easily bypass commercial speaker verification systems; and 2) anti-spoofing detectors struggle to generalize across different methods of audio synthesis, leading to a significant gap between in-domain performance and real-world robustness. These findings call for a reconsideration of security measures and stress the need for architectural innovations, adaptive defenses, and the transition towards multi-factor authentication."
  },
  {
    "date": "2026-01-06",
    "title": "ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning",
    "authors": "Abhishek HS, Pavan C Shekar, Arpit Jain, Ashwanth Krishnan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02880v1",
    "source": "arXiv",
    "abstract": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer."
  },
  {
    "date": "2026-01-06",
    "title": "LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark",
    "authors": "Ziyang Chen, Xing Wu, Junlong Jia, Chaochen Gao, Qi Fu, Debing Zhang, Songlin Hu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02872v1",
    "source": "arXiv",
    "abstract": "The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the \"thinking\" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding."
  },
  {
    "date": "2026-01-06",
    "title": "Bounded Rewriting Induction for LCSTRSs",
    "authors": "Kasper Hagens, Cynthia Kop",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02803v1",
    "source": "arXiv",
    "abstract": "Rewriting Induction (RI) is a method to prove inductive theorems, originating from equational reasoning. By using Logically Constrained Simply-typed Term Rewriting Systems (LCSTRSs) as an intermediate language, rewriting induction becomes a tool for program verification, with inductive theorems taking the role of equivalence predicates. Soundness of RI depends on well-founded induction, and one of the core obstacles for obtaining a practically useful proof system is to find suitable well-founded orderings automatically. Using naive approaches, all induction hypotheses must be oriented within the well-founded ordering, which leads to very strong termination requirements. This, in turn, severely limits the proof capacity of RI. Here, we introduce Bounded RI: an adaption of RI for LCSTRSs where such termination requirements are minimized."
  },
  {
    "date": "2026-01-06",
    "title": "Textile IR: A Bidirectional Intermediate Representation for Physics-Aware Fashion CAD",
    "authors": "Petteri Teikari, Neliana Fuenmayor",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02792v1",
    "source": "arXiv",
    "abstract": "We introduce Textile IR, a bidirectional intermediate representation that connects manufacturing-valid CAD, physics-based simulation, and lifecycle assessment for fashion design. Unlike existing siloed tools where pattern software guarantees sewable outputs but understands nothing about drape, and physics simulation predicts behaviour but cannot automatically fix patterns, Textile IR provides the semantic glue for integration through a seven-layer Verification Ladder -- from cheap syntactic checks (pattern closure, seam compatibility) to expensive physics validation (drape simulation, stress analysis). The architecture enables bidirectional feedback: simulation failures suggest pattern modifications; material substitutions update sustainability estimates in real time; uncertainty propagates across the pipeline with explicit confidence bounds. We formalise fashion engineering as constraint satisfaction over three domains and demonstrate how Textile IR's scene-graph representation enables AI systems to manipulate garments as structured programs rather than pixel arrays. The framework addresses the compound uncertainty problem: when measurement errors in material testing, simulation approximations, and LCA database gaps combine, sustainability claims become unreliable without explicit uncertainty tracking. We propose six research priorities and discuss deployment considerations for fashion SMEs where integrated workflows reduce specialised engineering requirements. Key contribution: a formal representation that makes engineering constraints perceptible, manipulable, and immediately consequential -- enabling designers to navigate sustainability, manufacturability, and aesthetic tradeoffs simultaneously rather than discovering conflicts after costly physical prototyping."
  },
  {
    "date": "2026-01-06",
    "title": "Finite Plasma Beta Three-dimensional Magnetic Field Extrapolation Based on MHD Relaxation Method",
    "authors": "Daiki Yamasaki, Takahiro Miyoshi, Satoshi Inoue",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02772v1",
    "source": "arXiv",
    "abstract": "Three-dimensional (3D) magnetic field in the solar atmosphere provides crucial information to understand the explosive phenomenon such as solar flares and coronal mass ejections. Since it is still hard that we determine the 3D magnetic field from direct observation, a nonlinear force-free field (NLFFF) extrapolation is one of the best modeling methods that provides 3D magnetic field. However, the method is based on zero-beta assumption, i.e., the model ignores the gas pressure gradient and gravitational force. The magnetic field based on an NLFFF is not well reconstructed in high-beta region, such as in chromospheric or lower height layer and in weak field region. To overcome this problem, we need to consider the magnetohydrostatic equilibrium. In this study, we developed a finite plasma beta magnetic field extrapolation method based on magnetohydrodynamic relaxation method. In our method, we consider a force balance of the Lorentz force and the gas pressure. We tested three different schemes and extrapolated 3D magnetic field using an observational photospheric vector magnetic field of one solar active region, which is a quadrupole complex sunspot group and well studied with an NLFFF. The verification of three schemes is performed by comparing the residual force, and we concluded that our methods reduce ~4% of residual force of the previous NLFFF. We also examined the plasma beta profile along the height and found that, in the core of the active region, plasma beta reaches a local minimum of ~0.01 in the lower corona with beta ~1 at the photosphere."
  },
  {
    "date": "2026-01-06",
    "title": "Vclip: Face-based Speaker Generation by Face-voice Association Learning",
    "authors": "Yao Shi, Yunfei Xu, Hongbin Suo, Yulong Wan, Haifeng Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02753v1",
    "source": "arXiv",
    "abstract": "This paper discusses the task of face-based speech synthesis, a kind of personalized speech synthesis where the synthesized voices are constrained to perceptually match with a reference face image. Due to the lack of TTS-quality audio-visual corpora, previous approaches suffer from either low synthesis quality or domain mismatch induced by a knowledge transfer scheme. This paper proposes a new approach called Vclip that utilizes the facial-semantic knowledge of the CLIP encoder on noisy audio-visual data to learn the association between face and voice efficiently, achieving 89.63% cross-modal verification AUC score on Voxceleb testset. The proposed method then uses a retrieval-based strategy, combined with GMM-based speaker generation module for a downstream TTS system, to produce probable target speakers given reference images. Experimental results demonstrate that the proposed Vclip system in conjunction with the retrieval step can bridge the gap between face and voice features for face-based speech synthesis. And using the feedback information distilled from downstream TTS helps to synthesize voices that match closely with reference faces. Demos available at sos1sos2sixteen.github.io/vclip."
  },
  {
    "date": "2026-01-06",
    "title": "Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System",
    "authors": "Yuqiao Xu, Mina Namazi, Sahith Reddy Jalapally, Osama Zafar, Youngjin Yoo, Erman Ayday",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02720v1",
    "source": "arXiv",
    "abstract": "Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework."
  },
  {
    "date": "2026-01-06",
    "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "authors": "Hongzhan Lin, Zixin Chen, Zhiqi Shen, Ziyang Luo, Zhen Ye, Jing Ma, Tat-Seng Chua, Guandong Xu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.02669v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications."
  },
  {
    "date": "2026-1-6",
    "title": "Post-Silicon Functional Verification and Validation: Chip to RTL Analysis",
    "authors": "Tim McDonley, Josh Delozier, Noah Taylor, Tamara Juntiff, Christian Eakins, Adam Kimura",
    "publish": "2025 IEEE Physical Assurance and Inspection of Electronics (PAINE)",
    "url": "https://doi.org/10.1109/paine66113.2025.11320228",
    "source": "IEEE",
    "abstract": "This paper presents a comprehensive workflow for performing post-silicon verification and validation from a layout file recovered through a destructive delayering and imaging process of the integrated circuit (IC). Standard cells are reintroduced into the GDS hierarchy through a tracing process allowing for the recovery of the behavior and boundary of each cell. The netlist is then recovered and formally verified against the RTL HDL code in a scalable manner to ensure alignment with the original design specifications."
  },
  {
    "date": "2026-1-6",
    "title": "Towards Few-shot LLM-based Vulnerability Reproduction and Verification for Industrial Web Applications",
    "authors": "Gang Yang, Lin Ni, Shaofeng Lin, Tao Xia",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279514",
    "source": "IEEE",
    "abstract": "The inherent vulnerability of Web system poses a severe security threat to modern industrial systems. Nevertheless, automatically obtaining accurate and reproducible vulnerability intelligence in a time-acceptable manner remains a challenge. To address this issue, we propose VulRV, an end-to-end vulnerability reproduction and verification system for industrial Web applications. VulRV employs large language models (LLMs) with few-shot Chain-of-Thought (CoT) prompting to generate the environment configuration. And then VulRV utilizes tool-calling method to establish corresponding vulnerability reproduction and validation environment. To evaluate the effectiveness of the proposed method, we collected reports of 22 vulnerabilities of mainstream industrial Web application spanning four different categories. Experimental results demonstrate that our method can effectively extract information from the reports to construct Docker containers-based environments to automatically reproduce and validate all these 22 vulnerabilities. Thereby the results show VulRV are capable of supporting subsequent red-team testing and risk assessment of industrial Web applications."
  },
  {
    "date": "2026-1-6",
    "title": "LLM-based Iterative Requirements Refinement in FSM with IEC 61499 Code Generation",
    "authors": "Valeriy Vyatkin, Sandeep Patil, Dmitrii Drozdov, Anatoly Shalyto",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279575",
    "source": "IEEE",
    "abstract": "This paper presents the Function Block Assistant (fbAssistant), an LLM-backed tool prototype for developing control logic in industrial automation. fbAssistant interprets natural language requirements and automatically generates state machines and their function blocks implementation. The study demonstrates iterative refinement, simulation validation, and deployment using EcoStruxure Automation Expert. The proposed approach aims at improved efficiency and accuracy in the development of automation software."
  },
  {
    "date": "2026-1-6",
    "title": "Enabling Software-Defined Tiered LLM Inference Continuum on Passive Optical Network",
    "authors": "Andrew Fernando Pakpahan, I-Shyan Hwang",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3651558",
    "source": "IEEE",
    "abstract": "The rapid expansion of large language models (LLMs) has shifted inference workloads from centralized cloud servers toward distributed execution across fog and edge environments. As this trend accelerates, existing broadband infrastructures, particularly passive optical networks (PONs), must evolve to meet the stringent latency, bandwidth, and coordination demands of real-time LLM inference. Therefore, it is essential to enhance the access network with intelligence and programmability to support efficient, scalable, and low-latency inference delivery across multiple network layers. This paper presents a software-defined architecture for enabling tiered LLM inference over Time and Wavelength Division Multiplexed Passive Optical Networks (TWDM-PON). The proposed system incorporates LLM-integrated Optical Line Terminals (OLTs) and Optical Network Units (ONUs), each equipped with processing and queuing capabilities to execute portions of LLM inference tasks locally. These intelligent nodes collaborate under centralized Software-Defined Networking (SDN) control to manage inference-related traffic and optimize resource allocation across the network. Through inference-aware dynamic bandwidth allocation, time and wavelength slicing, and queue-level traffic isolation, the architecture prioritizes LLM traffic to provide low and consistent latency for LLM inference tasks for prompt and token streams without degrading conventional broadband services. Simulation results show that the proposed architecture reduces average inference latency by up to 50%, improves throughput by up to 7%, and lowers jitter and packet drop probability by up to 32% and 25.01%, respectively, while maintaining end-to-end Quality of Service (QoS) under mixed broadband and LLM traffic."
  },
  {
    "date": "2026-1-6",
    "title": "Quantize-Sample-and-Verify: LLM Acceleration via Adaptive Edge-Cloud Speculative Decoding",
    "authors": "Guangyi Zhang, Yunlong Cai, Guanding Yu, Petar Popovski, Osvaldo Simeone",
    "publish": "IEEE Communications Letters",
    "url": "https://doi.org/10.1109/lcomm.2026.3651580",
    "source": "IEEE",
    "abstract": "In edge-cloud speculative decoding (SD), edge devices equipped with small language models (SLMs) generate draft tokens that are verified by large language models (LLMs) in the cloud. A key bottleneck in such systems is the limited communication bandwidth between edge and cloud, which necessitates quantization of the information transmitted about generated tokens. In this work, we introduce a novel quantize-sample (Q-S) strategy that provably preserves the output distribution of the cloud-based model, ensuring that the verified tokens match the distribution of those that would have been generated directly by the LLM. We develop a throughput model for edge-cloud SD that explicitly accounts for communication latency. Leveraging this model, we propose an adaptive mechanism that optimizes token throughput by dynamically adjusting the draft length and quantization precision in response to both semantic uncertainty and channel conditions. Simulations demonstrate that the proposed Q-S approach significantly improves decoding efficiency in realistic edge-cloud deployment scenarios."
  },
  {
    "date": "2026-1-6",
    "title": "An LLM-Aided System Information Modelling Methodology Applied in Tennessee Eastman Case",
    "authors": "Qianhang Lyu, Siqi Li, Martin G. Skjæveland, Yunqing Rao, Arild Waaler, Baifan Zhou",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279579",
    "source": "IEEE",
    "abstract": "System Information Modelling (SIM) employs formal semantic representations, such as ontologies and knowledge graphs, to digitally represent large industrial systems. Traditional SIM methods rely heavily on expert knowledge and manual effort, making them time-consuming, labour-intensive, and thus less scalable, which can hinder efficient system design and management. Amid the Industry 4.0 era and increasing industrial digitalisation, organisations are actively adopting and investigating SIM for diverse data-driven applications. In this work, we explore the potential of Large Language Models (LLMs) to partially automate SIM, with users inspect and control the process. Our approach aims to automatically extract system information from large amounts of technical documents, providing both formal semantic and visual graphical representations for domain engineers and IT professionals. We present preliminary results from a case study based on the Tennessee Eastman Process, an established benchmark in process control and industrial systems research, demonstrating the effectiveness of our approach in terms of modelling efficiency and scalability. This study paves the way for scalable and efficient industrial system design, offering significant benefits for complex industrial applications."
  },
  {
    "date": "2026-1-6",
    "title": "LLM En-powered UAV Networks Control for Efficient Wireless Communication",
    "authors": "Yue Zhang, Haibo Mei, Junyu Lai, Shuang Du",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310529",
    "source": "IEEE",
    "abstract": "Currently, Large Language Model (LLM), as a typical Generative AI (GAI) implementation, has been deployed to lead to artificial general intelligence for numerous applications. Numbers of LLM releases, like ChatGPT, Llama, Gemini, ChatGLM, can online interact with users to give instructions against different kinds of queries. Moreover, LLMs can process multi-model data, like text, image, voice, video, to support the workings of robots, expert systems, and operation control systems. Due to its merits, LLM in theory can help the operation of Unmanned Aerial Vehicle (UAV) communication networks, where each UAV is constrained by its size, weight and power (SWAP) to provide limited data collection functions in the air. In this paper, we apply LLM to online control the operation of UAVs by optimizing the trajectory of UAV , to lead to UAV energy efficiency. We firstly provide well established air-ground wireless communication model within UAV and ground terminals (GTs) to provide API library to LLM to help its closed-loop reasoning. Further, we establish Chain-of-Thought (CoT) and generate optimizing instructions to support UAV communication via prompt engineering. In the end, such LLM en-powered UAV communication network will be validated via experiments. The numerical results validate that LLM can enable UAV communication networks to more adaptive to complicated environments to work with less energy consumption and higher energy efficiency."
  },
  {
    "date": "2026-1-6",
    "title": "Enhanced UAV GPS Geolocation Verification with Novel Identification Metrics",
    "authors": "Arupa Sarkar, Fendy Santoso, Jun Shen, Bo Du, Jun Yan",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310706",
    "source": "IEEE",
    "abstract": "A significant threat to GPS users, including Unmanned Aerial Vehicles (UAVs), is the location spoofing attack, which can mislead systems with false GPS signals, jeopardising their operations and safety. To address this challenge, this study presents a method for verifying GPS spoofing attacks on UAV systems. The proposed solution develops a robust methodology by analysing the reported position of the UAV, along with various features of the received signal, such as the signal-to-noise ratio (SNR), azimuth, and pitch angles, at multiple base stations. Additionally, we consider Nakagami fading channels to model the properties of the received signal, which are relevant to real-world scenarios. We developed a smart verification algorithm using the Recurrent Neural Network (RNN) to authenticate the position reported by UAVs based on SNR, azimuth, and pitch angle data at various base station antennas. We have utilised both simple recurrent neural network (SRNN) and long short-term memory (LSTM) algorithms to evaluate and compare the performance of the model by varying the number of base stations. The performance of the algorithm was evaluated using confusion metrics, including accuracy, precision, and the F1 score. In addition, we have compared the performance of the proposed model with the models proposed in the previous study, which were built based on the received signal strength (RSS). The results show that the effectiveness of the models improves as the number of base stations increases."
  },
  {
    "date": "2026-1-6",
    "title": "A Scalable End-to-End IoT Data Pipeline with Dynamic Bucketing and Blockchain Verification",
    "authors": "Ishwak Sharda, Kshitij Goyal, Samuel D. Okegbile, Jun Cai",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310553",
    "source": "IEEE",
    "abstract": "The rapid advancement of wireless and cyber-physical systems has driven a growing demand for real-time sensor data in cyber environments. While existing solutions attempt to meet these demands, achieving both high-throughput processing and robust data integrity remains a significant challenge. This paper proposes an end-to-end distributed data pipeline and blockchain-enabled framework that integrates online anomaly detection, scalable data aggregation, and secure verification to ensure reliable cyber evolution. An Apache Kafka-based streaming pipeline ingests high-velocity sensor data and employs a dynamic bucketing strategy that finalizes buckets based on data volume, elapsed time, and network gas costs. Once validated, each bucket’s canonical sensor data representation is hashed and committed on-chain for tamper-evident storage. To enhance efficiency and security, the framework supports both Merkle tree and Verkle tree cryptographic data structures for comparative analysis. Implemented on a private Ethereum-like blockchain, our system efficiently handles large-scale sensor ingestion while enabling per-record verification. By integrating real-time anomaly correction, cryptographic proof mechanisms, and on-chain commitments, our solution delivers trustworthy, verifiable sensor streams tailored to the low-latency and high-reliability needs of next-generation systems."
  },
  {
    "date": "2026-1-6",
    "title": "On the Limits of LLM Reasoning: Evidence from Contamination, Translation and Answer Modification in Multiple-Choice Benchmarks",
    "authors": "Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3651579",
    "source": "IEEE",
    "abstract": "Multiple-choice benchmarks are widely used to assess LLMs, yet their accuracy scores often conflate memorization—understood as pattern-based recall—with genuine reasoning, that is, inference beyond surface pattern transfer, especially when test sets are public and prone to contamination. To disentangle these effects, we evaluate models under three experimental conditions: (i) public (MMLU) vs. private (UNED-Access) data; (ii) original vs. professionally translated questions (English/Spanish; less likely to appear verbatim in training data); and (iii) an answer modification that replaces the correct option with ‘‘None of the other answers’’—which becomes the right choice and dissociates success from previously seen tokens or concepts, requiring implicit inference steps. Across 16 proprietary and open-weights models, accuracy drops under answer modification are substantial (10%–93%), with larger declines on the public dataset (56% on MMLU vs. 51% on UNED-Access) and minimal differences between originals and translations. Taken together, contamination and translation appear to be second-order factors compared to the ‘‘None of the other answers’’ condition, suggesting that current LLMs generalize well across datasets and languages but show marked limitations when inference is required. Model size and baseline accuracy prove insufficient to predict robustness—although in low-contamination settings, accuracy becomes a more reliable indicator of inference-based behavior. Instead, training strategies explicitly targeting reasoning emerge as the primary drivers of robustness, with reasoning-oriented models consistently showing greater stability under the NOTO substitution."
  },
  {
    "date": "2026-1-6",
    "title": "Integrated-Circuit Supply Chain Verification Using Mask Set Comparison in the Open Source Context",
    "authors": "Olivier Thomas, Florent Gomez, Julien Barbate, Clarisse Ginet, Stephane Gauri",
    "publish": "2025 IEEE Physical Assurance and Inspection of Electronics (PAINE)",
    "url": "https://doi.org/10.1109/paine66113.2025.11320144",
    "source": "IEEE",
    "abstract": "This paper presents research on supply chain verification of silicon chips within an open-source context, demonstrating that the availability of mask sets for a given integrated circuit enables comparisons with those recovered from actual produced chips to detect any added or removed circuitry, thereby revealing the presence or absence of silicon backdoors. The proposed method relies on a specific sample preparation process to minimize deprocessing artifacts and employs recovery of the construction grid from captured images to achieve a distance-accurate and precise layout reconstruction. Additionally, the solution can correct feature detection errors while enabling full coverage, as well as the analysis and simulation of any discovered backdoors. The paper outlines the entire workflow-from laboratory procedures to image processing and analysis-proving that this verification approach is practical and effective in real-world scenarios."
  },
  {
    "date": "2026-1-6",
    "title": "The X-in-the-Loop Approach in P-CAR for Verification and Validation of Automated Vehicles",
    "authors": "Yuri Stangherlin, Andrea Rofini, Vincenzo Sulli, Valeria Ioannucci, Francesco Valentini, Elena Cinque, Marco Pratesi",
    "publish": "2025 IEEE 102nd Vehicular Technology Conference (VTC2025-Fall)",
    "url": "https://doi.org/10.1109/vtc2025-fall65116.2025.11310205",
    "source": "IEEE",
    "abstract": "The roadmap toward autonomous mobility is progressing across the industrialized world and Europe is at the forefront, with the recent release of the regulation for the type-approval of the automated driving system of fully automated vehicles. At the same time, the need for more reliable validation methods to ensure safety has become paramount. Real-world road environments open the way to many edge cases, making validation through classic ground tests challenging. The Everything-in-the-Loop approach proposed in the P-CAR project aims to fill the gap. To prove its capabilities, we have selected the Intelligent Speed Assistance system as a representative use case among the possible functionalities of a connected and automated vehicle. The choice is motivated by its reliance on advanced technologies, including connectivity, digital maps, positioning, and on-board sensing. The paper describes the proposed approach and the main aspects of the implementation. Then, it focuses on the validation process implemented by the P-CAR laboratory, whose outcomes represent a successful compliance test."
  },
  {
    "date": "2026-1-6",
    "title": "Time-Varying Azimuth Beam Steering in Sliding Spotlight SAR for Appropriate Azimuth Resolution and Experimental Verification",
    "authors": "Shuyue Li, Wei Xu, Pingping Huang, Weixian Tan",
    "publish": "2025 International Conference on Electronic Information, Computer and Aerospace Remote Sensing (EICARS)",
    "url": "https://doi.org/10.1109/eicars68214.2025.11320211",
    "source": "IEEE",
    "abstract": "Sliding spotlight SAR suffers from fixed angular velocity, making it difficult to achieve adaptive, high-resolution imaging in complex terrain. This paper introduces an adaptive azimuth beam-steering method for non-uniform scenes, where adjustable parameters a1 and a2 enable dynamic control of angular velocity and variable-resolution imaging. The geometric configuration and Doppler behavior of the sliding spotlight mode are analyzed to clarify how the proposed steering law affects resolution. Simulation experiments with point targets verify that the proposed method flexibly adjusts azimuth resolution according to terrain requirements, improving imaging adaptability and overall performance."
  },
  {
    "date": "2026-1-6",
    "title": "Construction and Verification of an Intelligent Connected Vehicle Training Platform Based on Mixed Reality and Artificial Intelligence",
    "authors": "Zeting An, Peng Shao, Qianlin Xv, Xialin Sun, Yuhang Zuo, Lin Cheng",
    "publish": "Proceedings of the 2025 International Conference on Computer Technology, Digital Media and Communication",
    "url": "https://doi.org/10.1145/3783669.3783724",
    "source": "ACM",
    "abstract": "None"
  },
  {
    "date": "2026-1-6",
    "title": "Combined Trace Neural Network for Improved Design Extraction in Integrated Circuit Post-Silicon Verification and Validation Workflows",
    "authors": "Emily Haines, Preston Pozderac, J. Timothy Balint, Yash Patel, Ryan Mattei, James Schaffranek, Adam R. Waite, Tamara Juntiff, Matt Sale, Adam Kimura",
    "publish": "2025 IEEE Physical Assurance and Inspection of Electronics (PAINE)",
    "url": "https://doi.org/10.1109/paine66113.2025.11320171",
    "source": "IEEE",
    "abstract": "Global economic trends have driven modern microelectronic fabrication to offshore facilities, creating a need to perform post-silicon Verification and Validation (V&V) on fabricated integrated circuits (ICs). Due to increasing technological advances and shrinking node sizes, the time and effort to assure the validity of these ICs increases. We address this scalability challenge through a data reduction and combination method using a novel Combined Trace Neural Network (CTNN). Our CTNN model allows for the extraction of metal and via traces from the same prepared sample simultaneously, cutting the data and processing time by over half. We prove our CTNN model's efficacy through a comparative analysis against conventional image analysis methods on a single metal/via layer of a 14 nm FinFET SOC chip. This enhancement to the workflow marks a significant step in the ability to assure newer technologies for use in the US DoD mission."
  },
  {
    "date": "2026-1-6",
    "title": "Wearable Exoskeleton-Based Immersive Teleoperation for Industrial Manufacturing Systems: Hardware Design and Verification",
    "authors": "Guangwei Zhang, Ruohan Wang, Mengke Wang, Honghao Lyu, Dapeng Lan, Dashun Zhang, Geng Yang",
    "publish": "2025 IEEE 23rd International Conference on Industrial Informatics (INDIN)",
    "url": "https://doi.org/10.1109/indin64977.2025.11279561",
    "source": "IEEE",
    "abstract": "Currently, robots face significant challenges in independently completing tasks within dynamic and unstructured environments. Teleoperation systems that utilize exoskeletons as input devices present an effective solution to this issue. This paper introduces an ergonomic 7-degree-of-freedom (7-DOF) exoskeleton device and develops an immersive teleoperation system integrated with a virtual reality (VR) head-mounted display (HMD). In this system, the operator, serving as the master side, dons the exoskeleton to issue control commands to the slave-side robot while leveraging feedback from both the exoskeleton and the VR HMD for cognitive decision-making. This closed-loop teleoperation system provides a multi-sensory feedback experience that integrates visual and haptic sensations, significantly enhancing operational stability and accuracy. Furthermore, for force feedback control, we propose a strategy based on environmental parameter estimation in conjunction with Weber’s law, allowing for self-adaptive adjustments of force feedback mapping in response to varying environmental conditions. Experimental results indicate that operators experience a high level of immersion with this system and successfully complete tasks such as remote ultrasound detection. The system demonstrates superior performance in terms of stability, accuracy, and user adaptability, highlighting its potential for complex remote operations in dynamic and unstructured environments."
  }
]