[
  {
    "date": "2026-2-11",
    "title": "Fixbench-RTL: A Comprehensive Benchmark for Evaluating LLMs on RTL Debugging",
    "authors": "Shijie Li, Weimin Fu, Yifang Zhao, Xiaolong Guo, Yier Jin",
    "publish": "2025 Asian Hardware Oriented Security and Trust Symposium (AsianHOST)",
    "url": "https://doi.org/10.1109/asianhost68425.2025.11370376",
    "source": "IEEE",
    "abstract": "The rapid advancement of large language models (LLMs) has presented new avenues for automating complex tasks in hardware design and verification. Due to the time-consuming and labor-intensive nature of hardware code debugging, there has been a growing interest in leveraging LLMs for automating this process. However, benchmarks used in existing studies tend to be simplistic, limited in bug type, and proposed by the authors themselves. In this paper, we introduce a novel benchmark named Fixbench-RTL and its construction framework, specifically designed to assess the ability of LLMs to identify and correct bugs in HDL-based hardware designs. The benchmark covers a broad spectrum of error types, including syntactic errors, functional bugs, and security vulnerabilities. Experimental evaluations show that current popular LLMs still fall short of meeting the practical requirements for hardware debugging. The benchmark aims to provide a foundation for future research in LLM-assisted hardware debugging.",
    "title_zh": "Fixbench-RTL：评估大型语言模型在RTL调试中的综合基准",
    "abstract_zh": "大型语言模型（LLMs）的快速发展为硬件设计和验证中的复杂任务自动化提供了新的途径。由于硬件代码调试过程既耗时又费力，人们对利用LLMs自动化这一过程的兴趣日益增长。然而，现有研究中使用的基准往往过于简单，错误类型有限，并且由作者自行提出。在本文中，我们介绍了一种名为Fixbench-RTL的新基准及其构建框架，专门用于评估LLMs在识别和修正基于HDL的硬件设计中的错误的能力。该基准涵盖了广泛的错误类型，包括语法错误、功能性错误和安全漏洞。实验评估表明，目前流行的LLMs在硬件调试的实际需求上仍然存在不足。该基准旨在为未来LLM辅助硬件调试的研究提供基础。"
  },
  {
    "date": "2026-2-11",
    "title": "ArchTune: A Predictive Energy Estimation Framework for LLM Inference on Edge Accelerators",
    "authors": "Arghyajoy Mondal, Rajdeep Samanta, Ashwin Krishnan, Sparsh Mittal, Manoj Nambiar, Rekha Singhal",
    "publish": "2025 5th International Conference on AI-ML-Systems (AIMLSystems)",
    "url": "https://doi.org/10.1109/aimlsystems67835.2025.11373945",
    "source": "IEEE",
    "abstract": "Transformer-based Large Language Models (LLMs) power applications from virtual assistants and code generation to scientific discovery. As their capabilities grow, they are used for emerging use cases such as offline AI copilots, on-device personalization, and edge inference. This demands efficient deployment of fine-tuned LLMs or compact Small Language Models (SLMs) on resource-constrained edge devices. However, deploying LLMs on the edge presents a significant hardware design challenge. The diversity in model architectures, input/output token lengths, and batch sizes leads to widely varying compute and memory demands. Moreover, design parameters like systolic array sizes, vector lengths, data widths, and operating frequencies drastically affect energy consumption and latency. While hardware-aware quantization is often adopted for power-performance gains, determining the optimal hardware configuration that meets tight power and performance budgets remains a non-trivial task, especially when the design space spans millions of possible configurations. Exhaustive exploration is computationally prohibitive and delays time to market. We introduce Architecture-Tuner (ArchTune), a lightweight analytical framework that predicts power, latency and energy consumption for RISC-V-based accelerators featuring systolic arrays and vector processing units (VPUs). Given an LLM workload, ArchTune rapidly estimates energy across millions of configurations using calibrated analytical models, eliminating the need for exhaustive simulations. ArchTune achieves <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$R^{2}=99.42 \\%$</tex> with <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1 0. 4 1 \\%}$</tex> MAPE for systolic arrays and 8.2% MAPE for VPUs. By combining these models with systematic latency and memory analysis, ArchTune empowers early-stage design-space exploration, enabling designers to select energy-efficient hardware tailored for specific LLM workloads on edge platforms.",
    "title_zh": "ArchTune：用于边缘加速器上LLM推理的预测能量估计框架",
    "abstract_zh": "基于Transformer的大型语言模型（LLMs）为从虚拟助手和代码生成到科学发现的应用提供动力。随着其能力的增长，它们被用于新兴的用例，如离线AI助手、设备内个性化和边缘推理。这要求在资源受限的边缘设备上高效部署微调的LLMs或紧凑的小型语言模型（SLMs）。然而，在边缘设备上部署LLMs带来了显著的硬件设计挑战。模型架构、输入/输出令牌长度和批处理大小的多样性导致计算和内存需求的巨大差异。此外，设计参数如脉动阵列大小、向量长度、数据宽度和工作频率极大地影响能量消耗和延迟。虽然硬件感知量化通常用于功率性能增益，但确定满足严格功率和性能预算的最佳硬件配置仍然是一项复杂的任务，尤其是在设计空间涵盖数百万种可能配置的情况下。全面的探索在计算上是不可行的，并且会延迟上市时间。我们引入了Architecture-Tuner（ArchTune），这是一个轻量级的分析框架，可以预测基于RISC-V的加速器中具有脉动阵列和向量处理单元（VPUs）的功率、延迟和能量消耗。给定一个LLM工作负载，ArchTune使用校准的分析模型快速估计数百万种配置的能量，消除了对全面模拟的需求。ArchTune在脉动阵列上实现了<tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$R^{2}=99.42 \\%$</tex>和<tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1 0. 4 1 \\%}$</tex>的MAPE，在VPUs上实现了8.2%的MAPE。通过将这些模型与系统的延迟和内存分析相结合，ArchTune支持早期设计空间探索，使设计者能够选择针对特定LLM工作负载的边缘平台量身定制的节能硬件。"
  }
]